\documentclass[12pt, a4paper]{report}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[ngerman]{babel}
%\usepackage[pdftex]{graphicx}
\usepackage{a4wide}
\usepackage[onehalfspacing]{setspace}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[marginal]{footmisc}
\usepackage{url}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{longtable}
\usepackage{comment} 

\usepackage{rotating}



%für die Kopfzeile
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\nouppercase{\leftmark}}
\fancyhead[R]{\thepage} 

%für römische Zahlen
\newcommand{\RM}[1]{\MakeUppercase{\romannumeral #1}}

%für geschwungene Variablenbuchstaben
\usepackage{mathrsfs}

\usepackage{multirow}
\usepackage{tabularx}

%für Mathe
\usepackage{amsmath}
\usepackage{amsfonts}



\usepackage{apager}


\usepackage{float}
\restylefloat{figure}


% Für Lit.-Verzeichnis folgenden Befehl im Terminal ausführen: bibtex Masterarbeit_Moritz_Hanke
\bibliographystyle{apager_dgps}


% Fürs Abkürzungsverzeichnis folgenden Befehl im Terminal ausführen: makeindex Masterarbeit_Moritz_Hanke.nlo -s nomencl.ist -o Masterarbeit_Moritz_Hanke.nls
\usepackage{nomencl}
\let\abk\nomenclature
\renewcommand{\nomname}{Abkürzungsverzeichnis}
\setlength{\nomlabelwidth}{.20\hsize}
\renewcommand{\nomlabel}[1]{#1 \dotfill}
\setlength{\nomitemsep}{-\parsep}
\makenomenclature 


%\usepackage{Sweave}



\begin{document}

% Titelblatt
\thispagestyle{empty}

\begin{center}
\textbf{\LARGE{TITEL DER ARBEIT\\}} 
\end{center}

\begin{center}
\textbf{\Large{\\Masterarbeit}}
\end{center}
\begin{verbatim}

\end{verbatim}


\begin{figure}[htbp]
 \begin{minipage}{0.4\linewidth}
  \centering
  \setkeys{Gin}{width=0.4\textwidth}
  \includegraphics{BIPS_Logo_deutsch.png}
 \end{minipage}%
 \begin{minipage}{0.5\linewidth}
  \centering
  \setkeys{Gin}{width=0.99\textwidth}
  \includegraphics{Universitaet_Bremen.png}
 \end{minipage}
\end{figure} 


\begin{comment}
\begin{subfigure}[htbp]
\begin{center}
\setkeys{Gin}{width=0.2\textwidth}
\includegraphics{BIPS_Logo_deutsch.png}
\end{center}
\end{subfigure}


\begin{subfigure}[htbp]
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
\includegraphics{Universitaet_Bremen.png}
\end{center}
\end{subfigure}
\end{comment}
\begin{center}
\textbf{Fachbereich 3: Mathematik \\
Studiengang: Medical Biometry/Biostatistics (M.Sc.)\\}
\end{center}
\begin{verbatim}

\end{verbatim}

\begin{flushleft}
\begin{tabular}{lll}
& & \\
& & \\
\textbf{Eingereicht von:} & & Hanke, Moritz \\
%\textbf{Geboren am:} & & 07.08.1985 \\
\textbf{Matrikelnummer:} & & 2404575 \\
& & \\
& & \\
\textbf{Betreuung:} & & Prof. Dr. Iris Pigeot-Kübler\\
& & Dr. Ronja Foraita \\
& & \\
& & \\
\textbf{Eingereicht am:} & & \today\\
& & \\
& & \\
\end{tabular}
\end{flushleft}





%Verzeichnisse
\pagenumbering{Roman}
\tableofcontents

\listoffigures

\listoftables

\printnomenclature 

\abk{Lasso}{Least absolute shrinkage and selection operator}
\abk{DAG}{Directed acyclic graph}
\abk{OLS}{Ordinary least squares}
\abk{MSE}{Mean squared error}
\abk{RSS}{Residual sum of squares}
\abk{PRSS}{Penalized residual sum of squares}
\abk{KEGG}{Kyoto Encyclopedia of Genes and Genomes}


% Begin der Arbeit
\chapter{Einleitung}
\pagenumbering{arabic}
Im Zusammenhang mit linearen Regressionen bei \textit{hochdimensionalen Daten} stellt sich häufig das so genannte \textit{"`large p, small n"'}-Problem. Dabei handelt es sich um den Umstand, dass ein Datensatz weniger \textit{Beobachtungen n} als \textit{erklärende Variablen p} beinhaltet. Speziell in der Genetik gilt oftmals sogar $p \gg n$, d.h. es werden beispielsweise die Expressionslevel von mehreren Tausend Genen bei wenigen Hundert Probanden gemessen. Für das klassische lineare Regressionsmodell $\mathbf{y}=\textbf{X}\boldsymbol\beta+\boldsymbol\epsilon$ liefert in diesem Fall die Schätzung $\boldsymbol{\hat{\beta}}$ mittels der \textit{Methode der kleinsten Quadrate }(englisch: ordinary least squares; \textit{OLS}) keine eindeutige Lösung und das Modell wird an die Daten überangepasst \cite{hastie_efficient_2004}. Deshalb wird gefordert, dass $\boldsymbol{\beta}$ \textit{spärlich} besetzt ist, d.h. für die meisten Regressionskoeffizienten $\beta_i=0$ gilt \cite{buehlmann2011statistics}. Hierbei handelt es sich speziell in der Genetik um eine realistische Annahme, da in der Regel nur wenige Gene für ein spezifische Responsevariablenausprägung verantwortlich sind. Die Schwierigkeit besteht darin, den Einfluss der wenigen aktiven Kovariablen zu schätzen und gleichzeitig eine Variablenselektion vorzunehmen, sodass Kovariablen ohne Einfluss aus dem Modell entfernt werden.\\
Ein verbreiteter Ansatz ist in diesem Zusammenhang die Einführung einer \textit{Penalisierungen} bei der Schätzung der Regressionskoeffizienten, was eine Schrumpfung und Selektion der Regressionskoeffizienten zur Folge hat. Dazu wird das Kleinste-Quadrat-Kriterium $\arg\min\limits_{\boldsymbol{\beta}} \sum_{i=1}^{n}(y_i - \mathbf{x}_i' \boldsymbol{\beta})^2$ des OLS \cite{fahrmeir2009regression} um einen \textit{Strafterm} $P(\boldsymbol{\beta})$ erweitert, wodurch einzelne Regressionkoeffizienten $\beta_j$ als $\hat{\beta}_j=0$ geschätzt werden können. Die Folge ist eine eindeutige Lösung und der Vektor $\boldsymbol{\hat{\beta}}$ ist wie für $\boldsymbol{\beta}$ angenommen spärlich besetzt.\\ 
Für diese Strategie ist der \textit{Least absolute shrinkage and selection operator} (\textit{Lasso}) von \citeA{tibshirani96regression} eine der bekanntesten Methoden und wurde in den letzten Jahren im Hinblick auf die Einbeziehung von vorab bekannten Informationen im Strafterm erweitert. \citeA{li_network-constrained_2008} haben dazu einen Strafterm entwickelt, der die Netzwerkstruktur der erklärenden Variablen untereinander berücksichtigt. Dieser Ansatz ist besonders in der Genetik vielversprechend, da durch Datenbanken wie der \textit{Kyoto Encyclopedia of Genes and Genomes} (\textit{KEGG}; \citeNP{Kanehisa2000KEGG}) relativ umfangreiche Informationen über Genregulationsnetzwerke (sogenannte \textit{Pathways}) und damit über die Abhängigkeit von einzelnen Genen untereinander vorliegen.\\
In der vorliegenden Arbeit wird eine Weiterentwicklung der netzwerkbasierten Penalisierung durch \citeA{kim_network-based_2013} um verschiedene \textit{Zentralitätsmaße} bei der Schätzung der Parameter $\boldsymbol{\beta}$ vorgestellt. Die Ergebnisse werden untereinander sowie mit den "`klassischen"' Penalisierungsansätzen Lasso und \textit{Elastic net} \cite{zou_regularization_2005} verglichen. Ziel ist es dabei, den Einfluss der zusätzlichen Information über die erklärenden Variablen, die ein Netzwerk bilden, auf die Güte der Schätzung darzustellen und darauf basierende Empfehlungen für die Anwendung von netzwerkbasierten Penalisierungen abzugeben.






\chapter{Theorie}\label{Kap_Theorie}
\section{Graphen und Netzwerkstatistiken}\label{Kap_Graphen und Netzwerkstatistiken}
%
%
% Netzwerke <--> Graphen
%
%
In den nachfolgenden beiden Unterkapiteln wird eine Auswahl an grundlegenden Begriffen aus der Graphentheorie (Kapitel \ref{Kap_Grundlegende Begriffe der Graphentheorie}) sowie Statistiken für Netzwerkanalysen, insbesondere zur Zentralität und Verbundenheit von einzelnen Knoten (Kapitel \ref{Kap_Zentralitätsstatistiken}), eingeführt, die für diese Arbeit benötigt werden. Für eine ausführliche Erläuterung zur Graphentheorie bzw. zu Netzwerkstatistiken sei auf \citeA{diestel2006graph} sowie \citeA{kolaczyk2009statistical} und \citeA{newman2010networks} verwiesen.\\

\subsection{Grundlegende Begriffe der Graphentheorie}\label{Kap_Grundlegende Begriffe der Graphentheorie}
Ein Graph ist ein Paar $G=(V,E)$, das aus einer Menge $V=\{v_1,\dots,v_{N_V}\}$ an \textit{Knoten} und einer Menge $E=\{e_1,\dots,e_{N_E}\}$ an \textit{Kanten} besteht \cite{brandes2005graphfunda}. Die Kardinalitäten $N_V =|V|$ und $N_E=|E|$ geben die \textit{Ordnung} und die \textit{Größe} des Graphen an. Eine ungerichtete Kante $e \in E$, ist die Zuordnung zweier Knoten $u,v \in V$, für $u \neq v$, eines Graphen $G=(V,E)$, d.h. es gilt $\{u,v\} = e$. Die Knoten $u$ und $v$ werden in diesem Fall als \textit{inzident} zu der Kante $e$ und \textit{adjazent} zueinander bezeichnet. Für $e=\{u,v\}$ handelt es sich um ein \textit{ungeordnetes} Knotenpaar an Knoten, d.h. $\{u,v\}=\{(u,v),(v,u)\}$. Graphen, die nur ungeordnete Knotenpaare beinhalten, sind \textit{ungerichtet}. Dagegen bezeichnet $e=(u,v) \neq (v,u)$, dass es sich um ein \textit{geordnetes} Paar an Knoten handelt, wobei die Kante $e$ vom \textit{Anfangsknoten} $u$ auf den \textit{Endknoten} $v$ zeigt \cite{kolaczyk2009statistical}. Als \textit{gerichtete} Graphen werden Graphen bezeichnet, die nur geordnete Knotenpaare beinhalten. Der Graph $G_A=(V_A,E_A)$ ist ein \textit{Teilgraph} von $G=(V,E)$ wenn $V_A \subseteq V$ und $E_A \subseteq E$ gilt. Die in dieser Arbeit verwendeten Graphen sind \textit{schlichte} Graphen, d.h. es gibt weder \textit{Schlingen} noch mehrere Kanten zwischen zwei Knoten. Als Schlinge wird eine Kante bezeichnet, die nur einen Endknoten an beiden Kantenenden besitzen, d.h. $e=\{v,v\}$ bzw. $e=(v,v)$ \cite{tittmann2011graphen}. Darüber hinaus werden nur \textit{endliche} Graphen ($N_V < \infty$) berücksichtigt, die entweder gerichtet oder ungerichtet sind.\\

Der Knotengrad $d(v)$ für $v \in V$ gibt die Anzahl der Kanten an, die inzident auf den Knoten $v$ sind. Für einen Graphen $G=(V,E)$ sind der \textit{Minimumknotengrad} $\delta(G)$, der \textit{Maximumknotengrad} $\Delta(G)$ und der \textit{durchschnittliche Knotengrad} $\bar{d}(G)$ definiert \cite{diestel2006graph} als
\begin{align}
\delta(G)=\min\{d(v) \ | \ v \in V\}\label{Eq_Minimumknotengrad}
\end{align}
\begin{align}
\Delta(G)=\max\{d(v) \ | \ v \in V\}\label{Eq_Maximumknotengrad}
\end{align}
\begin{align}
\bar{d}(G)=\frac{1}{N_V}\sum_{v \in V}d(v).\label{Eq_durchschnittlicher Knotengrad}
\end{align}
Handelt es sich um einen gerichteten Graphen, kann für jeden Knoten $v \in V$ mittels $d^{in}(v)$ die Anzahl auf $v$ und mit $d^{out}(v)$ die Anzahl von $v$ gerichteten Kanten angegeben werden. Entsprechend können die Maße aus \eqref{Eq_Minimumknotengrad}, \eqref{Eq_Maximumknotengrad} und \eqref{Eq_durchschnittlicher Knotengrad} angepasst werden, indem nur $d^{in}(v)$ bzw. $d^{out}(v)$ berücksichtigt werden.\\

Die \textit{Adjazenzmatrix} $\textbf{A}=(a_{u,v})_{N_V \times N_V}$ kennzeichnet die Konnektivität, d.h. ob es eine Kante zwischen zwei Knoten gibt. Dafür sei 
\begin{align}\label{Eq_Adjazenzmatrix}
a_{u,v} = \begin{cases}
1, \qquad &\text{wenn} \ (u,v) \in E \ \textit{für} \ u \neq v \ ,\\
0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Handelt es sich bei $G=(V,E)$ um einen ungerichteten Graphen ist $\textbf{A}$ eine symmetrische Matrix \cite{kolaczyk2009statistical}.\\
Die \textit{Inzidenzmatrix} $\textbf{B}=(b_{v,e})_{N_V \times N_E}$ eines \textit{ungerichteten} Graphen $G=(V,E)$ gibt an, ob $e \in E$ inzident zu $v \in V$ ist \cite{kolaczyk2009statistical} und es sei
%nicht gut, dass da schon wieder "sei" ist
\begin{align}\label{Eq_Inzidenzmatrix_unger}
b_{v,e} = \begin{cases}
1, \qquad &\text{wenn} \ v \in e \ ,\\
0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Für einen \textit{gerichteten} Graphen $G=(V,E)$ kann unterschieden werden, ob $v \in e$ der Anfangs- oder Endknoten für einen beliebigen Knoten $u \in V$ ist. In diesem Fall ist die Inzidenzmatrix $\textbf{\~B}=(\tilde{b}_{v,e})_{N_V \times N_E}$ definiert \cite{brandes2005graphfunda} als
\begin{align}\label{Eq_Inzidenzmatrix_ger}
\tilde{b}_{v,e} = \begin{cases}
-1, \qquad &\text{wenn} \ (v,u) = e \  ,\\
\ 1, \qquad &\text{wenn} \ (u,v) = e \ ,\\
\ 0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Sei $\textbf{D}=\text{diag}(d(v), v \in V)$ eine \textit{Diagonalmatrix} und $\textbf{\~B}'$ die transponierte Matrix von $\textbf{\~B}$, dann gilt $\textbf{\~B\~B}'=\textbf{D}-\textbf{A}$. Dabei handelt es sich bei $\textbf{\~B\~B}'$ um eine $N_V \times N_V$ Matrix, die als \textit{Laplace-Matrix} $\textbf{L}$ bezeichnet wird. Deren zweitkleinster \textit{Eigenwert} $\lambda_2$ ist umso größer, je mehr Kanten es zwischen den einzelnen Knoten gibt und kann als ein Maß für die \textit{Verbundenheit} des Graphen interpretiert werden \cite{kolaczyk2009statistical}.\\ 
\begin{comment}
Für die \textit{normierte Laplace-Matrix} $\textbf{\~L}$ gilt $\textbf{\~L}=\textbf{D}^{1/2}\textbf{L}\textbf{D}^{1/2}$, wobei $\textbf{D}^{1/2}=\text{diag}\frac{1}{\sqrt{d(v)}}$ für $d(v) \neq 0$ und ansonsten $0$ ist \cite{brandes2005graphfunda}.
\end{comment}
% WIRD das gebraucht, wenn der Graph partioniert werden soll??? -> Spectrum

Ein \textit{Weg} $\mathcal{W}(v_1,v_m)$ zwischen den Knoten $v_1$ und $v_m$ in einem Graphen $G=(V,E)$ bezeichnet eine Abfolge $\{v_1, e_1, v_2,\dots,e_{m-1},v_m \}$ mit $v_1, \dots, v_m \in V$ und $e_1,\dots,e_{m-1} \in E$, die die beiden Knoten miteinander verbindet und die \textit{Länge} $m-1$ hat \cite{diestel2006graph}. Für ungerichtete Graphen gilt hierbei $e_k=\{v_{k-1}, v_k\}$ und für gerichtete Graphen $e_k=(v_{k-1}, v_k)$. Wird zusätzlich gefordert, dass $e_k \neq e_l$ für $k \neq l$ gilt, d.h. dass keine Kante $e_k$ mehrmals in der Abfolge vorkommt, handelt es sich um eine \textit{Spur} $\mathcal{S}(v_1,v_m)$. Erfüllt ein Weg die Bedingung $v_k \neq v_l$ für $k \neq l$, wird dieser \textit{Pfad} $\mathcal{P}(v_1,v_m)$ genannt \cite{brandes2005graphfunda}. Für $v_1 = v_m$ ist $\mathcal{W}(v_1,v_m)$ ein \textit{Zyklus} und für $\mathcal{P}(v_1,v_m)$ ein \textit{Kreis}. Der kürzeste Pfad zwischen zwei Knoten $v_1,v_m \in V$ heißt \textit{Distanz} $D(v_1,v_m)$ und seine Länge entspricht der Anzahl an zugehörigen Kanten. Mit der \textit{größten} Distanz aus allen Knotenkombinationen eines zusammenhängenden Graphen $G=(V,E)$ wird dessen Durchmesser $\phi(G)$ angegeben \cite{diestel2006graph}. Wege, Pfade, Spuren, Zyklen und Kreise sind gerichtet, wenn der zugrunde liegende Graph gerichtet ist. Dieser wird, wenn er keinen Zyklus beinhaltet, als \textit{gerichteter, azyklischer Graph}  (englisch: directed acyclic graph; \textit{DAG}) bezeichnet \cite{kolaczyk2009statistical}.\\

Ein ungerichteter Graph $G=(V,E)$ ist \textit{zusammenhängend}, wenn jeder Knoten $v \in V$ von jedem anderen Knoten $u \in V$ über einen Weg erreicht werden kann. Entsprechend ist ein Graph \textit{unzusammenhängend}, wenn mindestens ein Knoten nicht von allen übrigen Knoten erreicht werden kann. 
Als \textit{Komponente} eines Graphen wird ein \textit{maximal zusammenhängender} Subgraph $G_A=(V_A,E_A)$ mit $V_A \subseteq V$ und $E_A \subseteq E$ für $G=(V,E)$ bezeichnet, wobei "`maximal"' die Eigenschaft ist, dass es keinen zusammenhängenden Subgraphen  $G_B=(V_B, E_B)$ in $G=(V,E)$ gibt, für den $V_B \supseteq V_A$ und $E_B \supseteq E_A$ gilt. Eine \textit{Clique} ist ein Subgraph, in dem alle Knoten adjazent zueinander sind.
\begin{comment}
Ein maximal zusammenhängender Teilgraph $G_A=(V_A,E_A)$, für den $V_A \subseteq V$ und $E_A \subseteq E$ für $G=(V,E)$ gilt und es keinen zusammenhängenden Subgraphen  gibt, heißt \textit{Komponente}.
\end{comment} 
Ist $G=(V,E)$ gerichtet, so wird dieser als \textit{schwach verbunden} bezeichnet, wenn der zugrunde liegende ungerichtete Graph zusammenhängend ist, und als \textit{stark verbunden}, wenn jeder Knoten von jedem anderen Knoten über einen direkten Weg erreichbar ist \cite{brandes2005graphfunda}. Mit der $k$-\textit{Verbundeheit} $\kappa(G)$ wird angegeben, wie viele Knoten mindestens aus einem zusammenhängenden Graphen $G=(V,E)$ \textit{entfernt} werden müssen, damit dieser unzusammenhängend wird \cite{diestel2006graph}.\\
% Cluster?
% tree?
% Cliquen?

\subsection{Zentralitätsstatistiken}\label{Kap_Zentralitätsstatistiken}
Um die \textit{Bedeutung} eines Knotens $u \in V$ zu bestimmen, kann seine \textit{Zentralität} innerhalb des Graphen $G=(V,E)$ angegeben werden. In der Literatur finden sich diverse Maße, die Zentralität teilweise sehr unterschiedlich definieren und daher bei der Beurteilung, wie zentral ein Knoten ist, zu unterschiedlichen Ergebnissen kommen können (vgl. dazu bspw. \citeA{koschuetzki2005centralityindices}). Ein Beispiel für die Mehrdeutigkeit des Begriffs Zentralität sind die Knoten $1$ und $8$ im Graphen in Abbildung \ref{fig:Degree_vs_Path}. Wird die Zentralität schlicht als die Anzahl an benachbarten Knoten aufgefasst, hat Knoten $1$ (gelb) eine relativ zentrale Position ($d(1)=6$), Knoten $8$ (rot) hingegen ist nach dieser Definition weniger zentral gelegen ($d(8)=5$). Wird dagegen die Zentralität als die Anzahl an Pfaden, auf denen ein Knoten liegt aufgefasst, ist unmittelbar ersichtlich, dass der Knoten $8$ (rot) zentraler als der Knoten $1$ ist.\\


<<echo=FALSE>>=

#             1  2  3  4  5  6 6.5 7  8  9 10 11 12 13 14 15 16 17 18 19 
A <- matrix(c(0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #1 
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #2
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #3
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #4
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #5
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #6
              1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #6.5
              0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #7
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,  #8
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,  #9
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,  #10
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,  #11
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #12
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #13
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #14
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #15
              0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #16
              0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #17
              0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #18
              0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), ncol=20, byrow=T)

library(igraph)

g <- graph.adjacency(A, "undirected")
V(g)$color <- 5
V(g)[1]$color <- "yellow"
V(g)[8]$color <- "red"
#V(g)[7]$color <- "green"
@
<<echo=FALSE, Degree_vs_Path, fig.pos="H", out.width='.6\\linewidth', fig.align='center', fig.scap="Beispielgraph", fig.lp="fig:", fig.cap="Beispiel für einen Graphen, in dem in Abhängigkeit von der Definition von Zentralität, entweder der rote oder der gelbe Knoten die bedeutsamste Position einnimmt.">>=
plot(g, edge.width=3)
@

\paragraph*{Eigenvektor-Zentralität}\label{Kap_Eigenvektor-Zentralität}
\paragraph*{Closeness-Zentralität}\label{Kap_Closeness-Zentralität}
\paragraph*{Betweeness-Zentralität}\label{Kap_Betweeness-Zentralität}
% gibt es Statistik d(v_i)/\kappa(G)? würde die was taugen?
% statt d(v_i) sowas wie nachbran von nachbarn?


\section{Lineare Regression}\label{Kap_Lineare Regression}
In den drei folgenden Unterkapiteln werden die Grundlagen und Annahmen des klassischen linearen Regressionsmodells vorgestellt, die für die Methoden der penalisierten Regression in Kapitel \ref{Kap_Penalisierte Regression} Voraussetzung sind. Wenn nicht anders gekennzeichnet, bezieht sich die Darstellung auf \citeA{gross2003linear}, \citeA{fahrmeir2009regression} bzw. \citeA{seber2003linear}, die eine vertiefende und weiterführende Einführung in die lineare Regression geben. Kapitel \ref{Kap_Das klassische lineare Modell} beinhaltet eine kurze Übersicht über die Annahmen des klassischen linearen Modells. Die für die Schätzung der Regressionskoeffizienten am häufigsten verwendete Methode der kleinsten Quadrate wird in Kapitel \ref{Kap_Methode der Kleinsten Quadrate} erläutert. Im abschließenden Unterkapitel \ref{Kap_Multikollinearität und $p > n$} werden die Probleme Multikollinearität und "`$p > n$"' vorgestellt, die beide in der Genetik vorkommen und Penalisierungsansätze notwendig machen.
\begin{comment}
$\mathbf{X}$ sind ZVs\\
$\mathbf{y}$ ist eine kontinuierliche ZVs
\end{comment}
\subsection{Das klassische lineare Modell}\label{Kap_Das klassische lineare Modell}
Im klassischen linearen Regressionsmodell wird eine \textit{lineare} Beziehung zwischen der \textit{Responsevariable} $Y$ und den \textit{Kovariablen} $X_1, \dots, X_p$ angenommen:
\begin{align}\label{Eq_klassisches_modell}
Y=\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \varepsilon.
\end{align}
Dabei wird $Y$ als Zufallsvariable und $x_j$ für $j=1,\dots,p$ entweder als Realisierungen der Zufallsvariablen $X_1,\dots,X_p$ oder als deterministische Werte für $X_1,\dots,X_p$ aufgefasst. Da reale Datensätze, insbesondere in der Genetik, häufig Beobachtungsdaten beinhalten, wird für die folgende Arbeit angenommen, dass es sich bei $x_1,\dots,x_p$ um Realisierungen von Zufallsvariablen handelt. Mit der Zufallsvariable $\varepsilon$ in \eqref{Eq_klassisches_modell} wird ein additiv wirkender Fehler bezeichnet, der unabhängig von den Kovariablen bzw. der Responsevariable auftritt und identisch verteilt ist.\\
Ziel bei der linearen Regression ist die Schätzung der unbekannten Parameter $\beta_0, \beta_1, \dots, \beta_p$, die als \textit{Regressionskoeffizienten} den Einfluss der einzelnen Kovariablen auf die Responsevariable angeben. Bei $\beta_0$ handelt es sich um eine \textit{Konstante} (englisch: Intercept), die unabhängig von den Kovariablenausprägungen einen "`Grundwert"' für die Responsevariable angibt. Liegen $n$ beobachtbare $Y$ sowie $p$ Kovariablen vor und seien
\begin{align*}
\mathbf{Y} = (Y_1,\dots,Y_n)'
\qquad \text{und} \qquad
\boldsymbol{\beta} = (\beta_0,\beta_1,\dots,\beta_p
)'
\qquad \text{sowie} \qquad
\boldsymbol{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n
)'
\end{align*}
einspaltige Matrizen sowie
\begin{align*}
\mathbf{X} = \begin{pmatrix}
1 & x_{11} & \dots & x_{1p}\\ 
\vdots & \vdots & &\vdots\\
1 & x_{n1} & \dots & x_{np}
\end{pmatrix}
\end{align*}
die \textit{Designmatrix} mit den Realisierungen der Kovariablen, so kann \eqref{Eq_klassisches_modell} in Matrixnotation als
\begin{align}\label{Eq_klass_lin_matrix}
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
\end{align}
dargestellt werden. Dazu muss angenommen werden, dass $\mathbf{X}$ \textit{vollen Spaltenrang} $\text{rg}(\mathbf{X})=p+1$ besitzt und die erste Spalte aus dem Vektor $\mathbf{1}=(1,\dots, 1)'$ besteht, um den Intercept zu berücksichtigen. Für die Fehler $\boldsymbol{\varepsilon}$ wird angenommen, dass  $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$, $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$ und $\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}')=\sigma^2\mathbf{I}$ seien, wobei $\mathbf{I}$ die Einheitsmatrix bezeichnet. Für die Responsevariable gilt
\begin{align}
\mathbb{E}(\mathbf{Y})&=\mathbb{E}(\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon})=\mathbf{X} \boldsymbol{\beta} \notag \\ 
\mathbb{C}\text{ov}(\mathbf{Y})&=\mathbb{C}\text{ov}(\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon})=\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\sigma^2\mathbf{I} \label{Eq_Cov_epsilon}
\end{align}
und unter zusätzlicher Annahme von \textit{normalverteilten} Fehlern, dass
\begin{align*}
\mathbf{Y} \sim \text{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}).
\end{align*} 

\subsection{Methode der Kleinsten Quadrate}\label{Kap_Methode der Kleinsten Quadrate}
Die Schätzung der Regressionskoeffizienten $\boldsymbol{\beta}$ für $\boldsymbol{\beta} \in \mathbb{R}^p$ kann bei vollem Spaltenrang von $\mathbf{X}$ mittels der \textit{Methode der kleinsten Quadrate} (englisch: ordinary least squares; \textit{OLS}) erfolgen. Für die Schätzer $\boldsymbol{\hat{\beta}}^{OLS}$ wird die \textit{Residuenquadratsumme} (englisch: residual sum of squares; \textit{RSS}) 
\begin{align}\label{Eq_RSS_OLS}
\text{RSS}(\boldsymbol{\beta})&=\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}=\sum_{i=1}^{n}(Y_i - \mathbf{X}_i'\boldsymbol{\beta})^2=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}) \notag \\
&=\mathbf{Y}'\mathbf{Y} -2\mathbf{Y}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
\end{align}
bzgl. $\boldsymbol{\beta}$ minimiert, d.h. 
\begin{align*}
\boldsymbol{\beta}^{OLS}&=\arg \displaystyle\min_{\boldsymbol{\beta}} (\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}
Differenzieren von \eqref{Eq_RSS_OLS} nach $\boldsymbol{\beta}$
\begin{align}\label{Eq_Ableitung_OLS}
\frac{\partial \text{RSS}(\boldsymbol{\beta})}{\partial  \boldsymbol{\beta}} = -2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
\end{align}
und anschließendes Nullsetzen ergibt die so genannte \textit{Normalengleichung}
\begin{align}\label{Eq_Normalengleichung}
&& 0&=-2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} \notag \\
&\Leftrightarrow & \mathbf{X}'\mathbf{X}\boldsymbol{\beta}&= \mathbf{X}'\mathbf{Y}.
\end{align}
Sind die Spalten von $\mathbf{X}$ \textit{linear unabhängig} und gilt $p \le n$, sodass $rg(\mathbf{X})=p$, handelt es sich bei $(\mathbf{X}'\mathbf{X})$ um eine \textit{positiv definite} und damit invertierbare Matrix. Als Schätzer $\boldsymbol{\hat{\beta}}^{OLS}$ ergibt sich aus \eqref{Eq_Normalengleichung}
\begin{align}\label{Eq_OLS_Schätzer}
\boldsymbol{\hat{\beta}}^{OLS} &= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}.
\end{align}
Mittels $\boldsymbol{\hat{\beta}}^{OLS}$ können die Responsevariablen $\mathbf{Y}$ als
\begin{align}\label{Eq_y_Schaetzung}
\mathbf{\hat{Y}}=\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}=\mathbf{X}\boldsymbol{\hat{\beta}}^{OLS}
\end{align}
geschätzt werden, sodass sich die Residuen $\boldsymbol{\hat{\varepsilon}}$ als Abweichungen der geschätzten von den beobachteten Responsevariablenausprägungen ergeben
\begin{align}
\boldsymbol{\hat{\varepsilon}}=\mathbf{Y}-\mathbf{\hat{Y}}=\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{OLS}.
\end{align}
Unter Berücksichtigung der Modellannahme aus \eqref{Eq_klass_lin_matrix} und $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$ ist der OLS-Schätzer ein unverzerrter Schätzer für $\boldsymbol{\beta}$, da
\begin{align}\label{Eq_Erwartungswert_OLS}
\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS})&=\mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}\right]=\mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon})\right] \notag \\
&= \mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon} \right] \notag \\
&=\boldsymbol{\beta}
\end{align}
und für die Kovarianzmatrix von $\boldsymbol{\hat{\beta}}^{OLS}$ gilt wegen $\mathbb{C}\text{ov}(\mathbf{y})=\sigma^2 \mathbf{I}$, dass
\begin{align}\label{Eq_Cov_OLS}
\mathbb{C}\text{ov}(\boldsymbol{\hat{\beta}}^{OLS})&=\mathbb{C}\text{ov}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}\right]=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \mathbb{C}\text{ov}(\mathbf{Y})\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\right]' \notag \\
&=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \sigma^2 \mathbf{I}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\right]'=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1} \notag \\
&=\sigma^2(\mathbf{X}'\mathbf{X})^{-1}.
\end{align}
Mit dem Varianzschätzer $\hat{\sigma}^2=\frac{1}{n-p}\boldsymbol{\hat{\varepsilon}}'\boldsymbol{\hat{\varepsilon}}$ kann die Kovarianzmatrix von $\boldsymbol{\hat{\beta}}^{OLS}$ als
\begin{align}
\widehat{\mathbb{C}\text{ov}(\boldsymbol{\hat{\beta}}^{OLS})}=\frac{1}{n-p}\boldsymbol{\hat{\varepsilon}}'\boldsymbol{\hat{\varepsilon}}(\mathbf{X}'\mathbf{X})^{-1}
\end{align}
geschätzt werden.\\
Mittels des \textit{Gauß-Markov-Theorems} lässt sich zeigen, dass für den OLS-Schätzer und alle anderen \textit{unverzerrten linearen} Schätzern $\boldsymbol{\tilde{\beta}}$ für einen beliebigen Vektor $\mathbf{b} \in \mathbb{R}^{p+1}$ folgendes Verhältnis gilt:
\begin{align*}
\mathbb{V}\text{ar}(\mathbf{b}'\boldsymbol{\tilde{\beta}}) \ge \mathbb{V}\text{ar}(\mathbf{b}'\boldsymbol{\hat{\beta}}^{OLS}).
\end{align*}
Somit gilt $\mathbb{V}\text{ar}(\tilde{\beta}_j) \ge \mathbb{V}\text{ar}(\hat{\beta}_j^{OLS})$, wenn $\mathbf{b}$ als kanonischer Einheitsvektor mit der Eins an der j-ten Stelle definiert wird.\\

Wird die Genauigkeit des Schätzers $\hat{\beta}_j^{OLS}$ für $\beta_j$ betrachtet, bietet sich als Risikofunktion (siehe Anhang \ref{App_Risikofunktion}) der \textit{mittlere quadratische Fehler} (englisch: mean squared error; \textit{MSE}) an, der die \textit{erwartete mittlere Abweichung} eines geschätzten vom wahren Regressionskoeffizienten angibt:
\begin{align*}
\text{MSE}(\hat{\beta}_j)=\mathbb{E}[(\hat{\beta}_j - \beta_j)^2]=\mathbb{V}\text{ar}(\hat{\beta}_j)+(Bias(\hat{\beta}_j))^2.
\end{align*}
Motiviert wird dieses Maß durch die Überlegung, dass ein unverzerrter Schätzer im Hinblick auf die Nähe zum wahren Parameter durch eine große Varianz dennoch sehr ungenau sein kann. Umgekehrt kann ein Schätzer, der eine nicht zu große systematische Verzerrung (Bias) und eine geringe Varianz aufweist, im Bezug auf seine erwartete Lage relativ dicht am wahren Parameter liegen. In Abschnitt \ref{Kap_Multikollinearität und $p > n$} wird erläutert, wodurch die einzelnen OLS-Schätzer trotz ihrer Unverzerrtheit jeweils einen relativ großen MSE haben können.\\

Als eine Verallgemeinerung des MSE kann die Verlustfunktion $\mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) = (\boldsymbol{\hat{\beta}}^{OLS} - \boldsymbol{\beta} )'(\boldsymbol{\hat{\beta}}^{OLS}-\boldsymbol{\beta})$ aufgefasst werden, die den \textit{quadrierten Abstand} des OLS-Parametervektors $\boldsymbol{\hat{\beta}}$ zum wahren Parametervektor $\boldsymbol{\beta}$ wiedergibt. Die zugehörige Risikofunktion ist der \textit{erwartete quadrierte Abstand} (siehe Anhang \ref{App_Beweis_Erwarteter_quadrierter_Abstand}):
\begin{align}\label{Eq_Erwarteter_Quadrierter_Abstand}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace=\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}].
\end{align}
Da $\mathbf{X}'\mathbf{X}$ auf der Diagonalen nur Werte $\geq 0$ haben kann und $\sigma^2 \geq 0$ gilt, muss stets auch $\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}] \geq 0$ gelten. Unter Berücksichtigung, dass die Spur einer positiv definiten Matrix gleich der Summe der Eigenwerte dieser Matrix ist und für die Inverse der Matrix die Eigenwerte die Reziproke der ursprünglichen Eigenwerte sind \cite{strang09ointro_linalg}, kann \eqref{Eq_Erwarteter_Quadrierter_Abstand} auch als \begin{align}\label{Eq_Lambda_Laenge_OLS}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace=\sigma^2 \sum_{j=0}^{p} \frac{1}{\lambda_j}
\end{align}
dargestellt werden, wobei $\lambda_{max}=\lambda_{0},\lambda_{1},\dots,\lambda_p=\lambda_{min}$ gilt \cite{hoerl_ridge_1970}.
Für die \textit{erwartete quadrierte Länge} von $\boldsymbol{\hat{\beta}}^{OLS}$ ergibt sich
\begin{align}\label{Eq_Laenge_OLS}
\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS} {'}\boldsymbol{\hat{\beta}}^{OLS})
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}]'[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}] \right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})]'
[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})]
\right\rbrace \notag \\
&=\mathbb{E}\left\lbrace
[\boldsymbol{\beta}'\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]'[\boldsymbol{\beta}'\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]
\right\rbrace \notag \\
&=\boldsymbol{\beta}'\boldsymbol{\beta} +
\mathbb{E}\left\lbrace
[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]'[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]
\right\rbrace \notag \\
&=\boldsymbol{\beta}'\boldsymbol{\beta} +
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace \notag \\
&\stackrel{\eqref{Eq_Beweis_Erwarteter_Quadrierter_Abstand}}{=}\boldsymbol{\beta}'\boldsymbol{\beta} + 
\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}].
\end{align}
Anhand von \eqref{Eq_Laenge_OLS} ist erkennbar, dass die OLS-Schätzung trotz ihrer Erwartungstreue im Vergleich zu dem wahren Parametervektor "`zu lang"' ist. 


\subsection{Multikollinearität und "`$p > n$"'}\label{Kap_Multikollinearität und $p > n$}
Korrelieren zwei oder mehr Kovariablen der Matrix $\mathbf{X}$ hoch miteinander, ohne dass sich eine Variable als Linearkombination der anderen Kovariablen beschreiben lässt, liegt \textit{Multikollinearität} vor. In diesem Fall ist $\mathbf{X}'\mathbf{X}$ invertierbar, allerdings kommt es zu einer Inflation der Varianz von $\boldsymbol{\hat{\beta}}^{OLS}$. Sei $\hat{\beta}_j^{OLS}$ der Schätzer für den Regressionskoeffizienten $\beta_j$, so kann dessen Varianz ermittelt werden als
\begin{align}\label{Eq_var_beta_j}
\mathbb{V}\text{ar}(\hat{\beta}_j^{OLS})=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}_{jj}=\frac{\sigma^2}{1-R^2_j} \ ,
\end{align}  
wobei $0 \leq R^2_j \leq 1$ der (multiple) \textit{Determinationskoeffizient} für die Regression von $\mathbf{x}_j$ auf die übrigen $p-1$ Kovariablen ist. Je stärker $\mathbf{x}_j$ mit den restlichen Kovariablen korreliert, d.h. je größer $R^2$ ist, desto kleiner wird der Nenner von \eqref{Eq_var_beta_j} und damit umso größer die Varianz von $\hat{\beta}_j^{OLS}$. Zudem kann es bereits durch kleine Änderungen in der Kovarianzmatrix $\mathbf{X}'\mathbf{X}$ zu geänderten Vorzeichen bei den $\hat{\beta}_j^{OLS}$ kommen.\\
Aus \eqref{Eq_Laenge_OLS} wird deutlich, dass die erwartete quadrierte Länge von $\boldsymbol{\hat{\beta}}^{OLS}$ um $\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}]$ größer als die von $\boldsymbol{\beta}$ ist. Multikollinearität hat zur Folge, dass die Eigenwerte von $\mathbf{X}'\mathbf{X}$ stark schrumpfen und deswegen unter Berücksichtigung von \eqref{Eq_Lambda_Laenge_OLS} die Spur von $(\mathbf{X}'\mathbf{X})^{-1}$ ansteigt. Dadurch wird der Vektor $\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS}{'} \boldsymbol{\hat{\beta}}^{OLS})$ deutlich länger als $\mathbb{E}(\boldsymbol{\beta}' \boldsymbol{\beta})$. Interpretiert man die Längendifferenz als Maß für die \textit{Gesamtgenauigkeit} der Schätzer, werden diese durch Multikollinearität insgesamt schlechter und zu groß. Für die Absolutwerte $|\beta_j^{OLS}|$ bedeutet das, dass die OLS-Methode den Einfluss einzelner Kovariablen überschätzt \cite{Marquardt1975RidgePractice,Brook1980ExpectedLengthOLS}.\\
Bei einer \textit{perfekten} Multikollinearität kann eine Kovariable als Linearkombination der anderen Kovariablen dargestellt werden. In diesem Fall ist $\mathbf{X}$ und damit auch $\mathbf{X}'\mathbf{X}$ singulär \cite{strang09ointro_linalg}, weswegen die Normalengleichung aus \eqref{Eq_Normalengleichung} nicht nach $\boldsymbol{\beta}$ aufgelöst werden kann und es keine eindeutige Lösung für $\boldsymbol{\beta}$ gibt.\\
Bei hochdimensionalen Daten mit $p>n$ oder sogar $p \gg n$ ist $rg(\mathbf{X}) < p+1$ und die Normalengleichung kann keinen eindeutigen Schätzer für $\boldsymbol{\beta}$ liefern \cite{johnstone_statistical_2009}.\\

\section{Penalisierte Regression}\label{Kap_Penalisierte Regression}
Für \textit{hochdimensionale} Daten, wie sie in der Genetik vorliegen, kann aus \textit{inhaltlichen} Gründen häufig die Annahme vertreten werden, dass nur wenige Kovariablen einen Einfluss auf die Responsevariable haben. Wird dieser Zusammenhang als klassisches lineares Regressionsmodell $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}$ ausgedrückt, muss daher angenommen werden, dass der Regressionskoeffizientenvektor $\boldsymbol{\beta}$ in Wahrheit \textit{spärlich} besetzt ist, d.h. für die meisten Koeffizienten $\beta_j = 0$ gilt. Die Herausforderung besteht darin, durch die Regression sowohl die $\beta_j$, für die $\beta_j \neq 0$ gilt, zu \textit{selektieren} als auch deren Einfluss auf die Responsevariable möglichst genau zu schätzen. Wie in Kapitel \ref{Kap_Methode der Kleinsten Quadrate} und \ref{Kap_Multikollinearität und $p > n$} dargestellt, hängt $\mathbb{V}\text{ar}(\hat{\beta}_j)$ von der Kovarianzmatrix $\mathbf{X}'\mathbf{X}$ ab, weswegen es bei hoher Multikollinearität zu einem hohen $\text{MSE}(\hat{\beta}_j)$ kommt. Gleichzeitig wird der Einfluss der einzelnen Kovariablen insgesamt überschätzt. Sollte eine exakte Multikollinearität bzw. $p > n$ vorliegen, kann die OLS-Methode darüber hinaus keine (eindeutigen) Schätzer liefern.\\
Penalisierungsansätze verfolgen die Strategie, die Unverzerrtheit des OLS-Schätzers zu "`opfern"', d.h. absichtlich ein Bias zu erzeugen, um einerseits eine Variablenselektion für die spärliche Besetzung von $\boldsymbol{\hat{\beta}}$ zu ermöglichen und gleichzeitig den geschätzten Einfluss der einzelnen Kovariablen zu schrumpfen. Wegen $\text{MSE}(\hat{\beta}_j)=\mathbb{V}\text{ar}(\hat{\beta}_j)+ \text{Bias}(\hat{\beta}_j)$ darf der durch die Penalisierung erzeugte Bias nicht zu groß werden, da ansonsten der MSE basierend auf der Penalisierungsmethode den MSE basierend auf der OLS-Methode übertrifft.\\
Als grundlegendes Prinzip wird bei der Penalisierung die zu minimierende Verlustfunktion, in der Regel die Residuenquadratsumme $\text{RSS}(\boldsymbol{\beta})$, um einen Strafterm $P(\boldsymbol{\beta})$ erweitert, sodass sich eine zu minimierende \textit{penalisierte Residuenquadratsumme} (englisch: penalized residual sum of squares; \textit{PRSS}) als  
\begin{align}\label{Eq_PRSS_allgmein}
\text{PRSS}(\boldsymbol{\beta},\boldsymbol{\gamma})=\text{RSS}(\boldsymbol{\beta}) + P(\boldsymbol{\beta}, \boldsymbol{\gamma})
\end{align} mit einem oder mehreren speziell zu wählenden \textit{Tuningparametern} $\boldsymbol{\gamma}$ ergibt. Generell wird gefordert, dass die abhängige Variable $\mathbf{Y}$ zentriert ist, d.h. es muss $\frac{1}{n}\sum_{i}^{n}y_i =0$ für die Realisierungen $y$ gelten, und die Kovariablen $\mathbf{X}$ müssen standardisiert sein, sodass $\frac{1}{n}\sum_{i=1}^{n}x_{ij}=0$ und $\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2=1$ für $j=1,\dots,p$ gilt. Dadurch ist die Penalisierung unabhängig von den einzelnen Skalen der Kovariablen bzw. der Responsevariable und das Intercept fällt weg. Für die Notation gilt daher im Folgenden, wenn nicht anders gekennzeichnet, $\boldsymbol{\beta}=(\beta_1,\dots,\beta_p)'$ bzw. $\boldsymbol{\hat{\beta}}=(\hat{\beta}_1,\dots,\hat{\beta}_p)'$ und die Designmatrix $\mathbf{X}$ beinhaltet nur noch die Kovariablenvektoren.\\
Kapitel \ref{Kap_Ridge-Regression} und \ref{Kap_Lasso-Regression} stellen zunächst die "`klassischen"' Penalisierungsmethoden \textit{Ridge-Regression} und \textit{Lasso-Regression} vor. Auf dieser Grundlage wird in Kapitel \ref{Kap_Elastic-Net} die Methode \textit{Elastic Net} eingeführt, die eine Kombination aus Lasso und Ridge ist. Abschließend werden \textit{netzwerkbasierte Penalisierungen} in Kapitel \ref{Netzwerkbasierte_Penalisierung} näher betrachtet und Möglichkeiten zur Berücksichtigung der in Kapitel \ref{Kap_Zentralitätsstatistiken} eingeführten Zentralitätsmaße aufgezeigt.
\begin{comment}
Unverzerrtheit des $\hat{\beta}$ der Kleinste-Quadratmethode wird geopfert, um den \textit{mittleren quadratischen Fehler} (englisch: mean squared error; \textit{MSE}) zu senken\\
Ridge wird eingeführt, weil Teil von Elastic Net\\
kein Intercept (bei allen Penalisierungen?); durch Standardisierung bzw. Zentralisierung fällt $\beta_0$ weg \cite{montgomery2012introduction}\\
warum keine Subsetselection\\
hat penalisierung, auch die netzwerkbasierte, was baysianisches?\\
!!\\
LAGARGIAN MULTIPLIER?????\\
penalized bestraft größere Beta mehr - intercept raus, weil sonst bestrafung nicht unabhängig von der skala des outcomes wäre!!! (für allgemeine einführung: http://web.as.uky.edu/statistics/users/pbreheny/764-F11/notes/8-30.pdf)
\end{comment}

\subsection{Ridge-Regression}\label{Kap_Ridge-Regression}
Die \textit{Ridge-Regression} ist eine Penalisierungsmethode, bei der die geschätzten Regressionskoeffizienten in $\boldsymbol{\hat{\beta}}^{Ridge}$ \textit{relativ} im Verhältnis zu den Schätzern in $\boldsymbol{\hat{\beta}}^{OLS}$ \textit{geschrumpft} werden. Die Schätzung der Regressionskoeffizienten erfolgt durch
\begin{align}\label{Eq_Ridge_estimate}
\boldsymbol{\hat{\beta}}^{Ridge} = (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}.
\end{align}
Durch die Addition von $\gamma \mathbf{I}$ verringert sich die Varianz der Schätzer im Vergleich zu den OLS-Schätzern (siehe \eqref{Eq_Erw_Quad_Abst_ridge}) und für den Fall, dass $\text{rg}(\mathbf{X}) < p$ und damit $\mathbf{X}'\mathbf{X}$ singulär ist, wird der Ausdruck in der Klammer regulär und \eqref{Eq_Ridge_estimate} ist eindeutig lösbar \cite{montgomery2012introduction}.
Allerdings findet in der Regel \textit{keine} Variablenselektion statt, sodass die Annahme, dass $\boldsymbol{\beta}$ spärlich besetzt ist, sich nicht in $\boldsymbol{\hat{\beta}}^{Ridge}$ widerspiegelt \cite{tibshirani96regression}. Für die Ridge-Regression wird $\arg\min\limits_{\boldsymbol{\beta}} \sum_{i=1}^{n} (Y_i - \mathbf{x}_i' \boldsymbol{\beta})^2$ um die $L_2$-Norm-Nebenbedingung $\sum_{j=1}^{p}\beta_j^2 \le t$ mit $t > 0$ ergänzt. In diesem Zusammenhang wird $t$ als \textit{Restriktionsparameter} bezeichnet, der als Bedingung vorgibt, wie groß die Summe der quadrierten Regressionskoeffizienten maximal sein darf. 
%%%XXXXXXXXXXXXXXXXXXXXXXXX DAS ggf noch mal überarbeiten im Hinblick auf "`große Parameter"' XXXXXXXXXXXXXXX 
Da $t$ invers mit dem Tuningparameter $\gamma$ zusammenhängt, kann die Berechnung von $\boldsymbol{\hat{\beta}}^{Ridge}$ als Minimierungsproblem in der \textit{Lagrange-Form} (siehe Anhang \ref{App_Lagrange-Form})
%%%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\begin{align}\label{Eq_Ridge_lagrangian}
\boldsymbol{\hat{\beta}}^{Ridge}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j )^2  + \gamma \sum_{j=1}^{p}\beta_j^2 \right\rbrace \qquad \text{mit} \ \gamma \ge 0
\end{align}
dargestellt werden \cite{hastie2009elements}.
Wird $\sum_{j=1}^{p} (\hat{\beta}_j^{OLS})^2 \le t$ gesetzt, entsprechen die Ridge-Schätzer den OLS-Schätzern, da in diesem Fall $\gamma=0$ ist und sich \eqref{Eq_Ridge_lagrangian} zum einfachen OLS-Minimierungsproblem reduziert \cite{tibshirani96regression}.\\
Der bzgl. $\boldsymbol{\beta}$ zu minimierende Term aus \eqref{Eq_Ridge_lagrangian} lässt sich in Abhängigkeit von $\gamma$ durch
\begin{align}\label{Eq_PRSS_Ridge}
\text{PRSS}_{Ridge}(\boldsymbol{\beta}, \gamma)&=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+\boldsymbol{\beta}'\gamma\boldsymbol{\beta} \notag \\
&=\mathbf{Y}'\mathbf{Y} -2\mathbf{Y}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\gamma\boldsymbol{\beta}
\end{align}
darstellen. Differenzierung nach $\boldsymbol{\beta}$ ergibt
\begin{align}\label{Eq_Ableitung_Ridge}
\frac{\partial \text{PRSS}_{Ridge}(\boldsymbol{\beta}, \gamma)}{\partial  \boldsymbol{\beta}} = -2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + 2\gamma \boldsymbol{\beta}
\end{align}
und anschließendes Nullsetzen liefert den Schätzer $\boldsymbol{\hat{\beta}}^{Ridge}$ aus \eqref{Eq_Ridge_estimate}
\begin{align*}
&& 0&=-2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + 2\gamma \boldsymbol{\beta} \notag \\
&\Leftrightarrow & \mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma \boldsymbol{\beta} &= \mathbf{X}'\mathbf{Y} \notag \\
&\Leftrightarrow & (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I}) \boldsymbol{\beta} &= \mathbf{X}'\mathbf{Y} \notag\\
&\Rightarrow & \boldsymbol{\hat{\beta}}^{Ridge} &= (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}.
\end{align*}
Der Ridge-Schätzer aus \eqref{Eq_Ridge_estimate} kann auch als Transformation des OLS-Schätzers dargestellt werden \cite{hoerl_ridge_1970}
\begin{align}\label{Eq_ridge_transformation}
\boldsymbol{\hat{\beta}}^{Ridge} =&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y} \notag\\
\stackrel{\eqref{Eq_y_Schaetzung}}{=}&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1} \mathbf{X}'\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} \notag\\
=&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1} \mathbf{X}'\mathbf{X} \boldsymbol{\hat{\beta}}^{OLS} \notag\\
=&(\mathbf{I} + \gamma (\mathbf{X}'\mathbf{X})^{-1})^{-1}  \boldsymbol{\hat{\beta}}^{OLS}.
\end{align}
Daraus ist ersichtlich, dass $\boldsymbol{\hat{\beta}}^{Ridge} \ne \boldsymbol{\hat{\beta}}^{OLS}$ für $\gamma \ne 0$ gilt und in diesem Fall $\boldsymbol{\hat{\beta}}^{Ridge}$ nicht erwartungstreu ist \cite{seber2003linear}, weil der OLS-Schätzer erwartungstreu ist.\\ 
Es lässt sich zeigen, dass mit der Varianz aus \eqref{Eq_Cov_epsilon} für den erwarteten quadrierten Abstand für $\boldsymbol{\hat{\beta}}^{Ridge}$ zu $\boldsymbol{\beta}$ gilt (vgl. \citeA{hoerl_ridge_1970}):
\begin{align}\label{Eq_Erw_Quad_Abst_ridge}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{Ridge},\boldsymbol{\beta}) \right\rbrace=\sigma^2 \sum_{j=1}^{p} \frac{\lambda_j}{(\lambda_j + \gamma)^2} + \gamma^2 \boldsymbol{\beta}' (\mathbf{X}'\mathbf{X}+ \gamma \mathbf{I})^{-2}\boldsymbol{\beta}
\end{align}
wobei $\lambda_{max}= \lambda_1, \dots, \lambda_p= \lambda_{min}$ die Eigenwerte von $\mathbf{X}'\mathbf{X}$ sind. Der linke Summand von \eqref{Eq_Erw_Quad_Abst_ridge} beinhaltet in Anlehnung an \eqref{Eq_Lambda_Laenge_OLS} die Summe der Varianzen der $\hat{\beta}_j^{Ridge}$ und der rechte Summand den durch $\gamma$ erzeugte Bias. Anhand dieser Darstellung ist erkennbar, dass mit größer werdendem $\gamma$ die Summe der Varianzen des Schätzers sinkt und gleichzeitig die gesamte Verzerrung ansteigt. Umgekehrt wird an dieser Risikofunktion deutlich, dass für $\gamma=0$ der Ridge-Schätzer dem OLS-Schätzer entspricht und folglich dieselbe erwartete quadratische Distanz zum wahren $\boldsymbol{\beta}$ hat. Dieser Abstand ist wie in Abschnitt \ref{Kap_Multikollinearität und $p > n$} gezeigt deutlich von einer schlecht konditionierten Matrix $\mathbf{X}$ abhängig. Ziel bei der Wahl von $\gamma$ ist es deswegen, ein optimales $\gamma$ zu finden, dass $\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{Ridge},\boldsymbol{\beta}) \right\rbrace$ bzw. die einzelnen $\text{MSE}(\hat{\beta}_j^{Ridge})$ minimiert. \citeA{hoerl_ridge_1970} zeigen, dass es immer ein $\gamma$ gibt, für das $\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{Ridge},\boldsymbol{\beta}) \right\rbrace < \mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace$ gilt. In Bezug auf die Risikofunktion bedeutet das, dass die Ridge-Schätzer mit optimalem $\gamma$ für alle wahren $\boldsymbol{\beta}$ \textit{strikt besser} sind als die OLS-Schätzer \cite{gross2003linear}.\\
Die Wahl des Tuningparameters $\gamma$ muss dabei vom Anwender getroffen werden und kann, soll er nicht subjektiv festgelegt werden, durch iterative Methoden geschätzt werden. Eine Übersicht über verschiedene Methoden bzw. weiterführende Literatur zur Wahl von $\gamma$ findet sich u.a. bei \citeA{gross2003linear} und \citeA{montgomery2012introduction}.


\subsection{Lasso-Regression}\label{Kap_Lasso-Regression}
%%%WIESO KANN LASSO $p>n$ LÖSEN?????????? siehe übersichtspaper bzgl der quadrat durch betrag erweiterung?
Anders als bei der Ridge-Regression wird bei der \textit{Lasso-Regression} ein $L_1$-Norm-Strafterm in die Lagrange-Form als zusätzliches Argument eingeführt, womit sich
\begin{align}\label{Eq_Lasso_Lagrange}
\boldsymbol{\hat{\beta}}^{Lasso}(\gamma)=\arg \displaystyle\min_{\beta} \left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2  + \gamma \sum_{j=1}^{p}|\beta_j| \right\rbrace \qquad \text{mit} \ \gamma \ge 0.
\end{align}
als Minimierungsproblem ergibt \cite{tibshirani96regression}. Die Betragsfunktion in \eqref{Eq_Lasso_Lagrange} hat eine \textit{gleichmäßige} Schrumpfung der Koeffizienten zur Folge, wodurch für einzelne Regressionskoeffizienten $\hat{\beta}_j =0$  und damit eine Variablenselektion erreicht werden kann. Dieser Sachverhalt lässt sich geometrisch darstellen, wenn \eqref{Eq_Lasso_Lagrange} als Optimierungsproblem mit einer Nebenbedingung ausgedrückt wird
\begin{align}\label{Eq_Lasso_Constraint}
\boldsymbol{\hat{\beta}}^{Lasso}(t)=\arg \displaystyle\min_{\beta} \ &\left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2 \right\rbrace \notag  \\
\text{s.t.} \ & \sum_{j=1}^{p}|\beta_j| \le t  \qquad \text{mit} \ t \ge 0.
\end{align}
Für den Fall von zwei Kovariablen ist dann die Nebenbedingung aus \eqref{Eq_Lasso_Constraint} eine gleichwinkelige Raute und die zu minimierende Residuenquadratsumme kann als Ellipsen um den OLS-Schätzer im euklidischen Vektorraum $\mathbb{R}^2$ dargestellt werden. Die Form der Ellipsen ist dabei durch $\mathbf{X}'\mathbf{X}$ bestimmt und für die Stelle, an der die Raute zuerst tangiert wird, ist eine globale Extremstelle unter Einhaltung der Nebenbedingung erreicht \cite{tibshirani96regression}. Tangieren die Ellipsen eine \textit{Ecke} der Raute, muss einer der beiden Koeffizienten Null gesetzt werden. Diese geometrische Interpretation des Lasso-Schätzers gilt auch für höherdimensionale Daten, wobei die Nebenbedingung ein p-dimensionaler Kreuzpolytop, der um den Faktor $t$ skaliert wird, und die Zielfunktion ein p-dimensionaler Ellipsoid mit dem OLS-Schätzer im Zentrum ist (vgl. dazu \citeNP{Perty2009ShrinkagePolytopes}). Abbildung \ref{fig:Ridge_vs_Lasso} veranschaulicht den Schrumpfungs- und Selektionseffekt des Lasso- gegenüber dem Ridge-Schätzer für zwei Kovariablen. Bei beiden Methoden sind $\hat{\beta}_1$ und $\hat{\beta}_2$ im Vergleich zur OLS-Schätzung geschrumpft, allerdings kommt es nur bei der Lasso-Methode zu einer Variablenselektion.\\
<<echo=FALSE, Ridge_vs_Lasso, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.49\\linewidth', fig.lp="fig:", fig.scap="Graphische Veranschaulichung des Lasso- und Ridge-Strafterms", fig.cap="Graphische Veranschaulichung des Lasso-Strafterms ($L_1$-Norm, Raute $|\\beta_1|+|\\beta_2|=t$) und des Ridge-Strafterms ($L_2$-Norm, Einheitskreis $ \\beta_1^2 + \\beta_2^2 = t$ ) hinsichtlich des Schrumpfungseffekts und der Variablenselektion. Die Ellipsen um den OLS-Schätzer (rot) werden solange vergrößert, bis sie die Nebenbedingung erfüllen (hellblau) und die Raute bzw. den Einheitskreis tangieren. Während es bei beiden Methoden zu einer Schrumpfung der Koeffizientenschätzer kommt, wird nur bei der Lasso-Penalisierung ein Koeffizient Null gesetzt, da die Ellipse die Raute an einer Kante tangiert.">>=
#### R-Code Lasso vs Ridge Ellipsen

library(ellipse)
#
# Plot an ellipse corresponding to a 95% probability region for a 
# bivariate normal distribution with mean scale, variances scale and 
# correlation 0.8. 
#
x <- y <- seq(-12,12,.1)
covmat <- matrix(c(1,0.5,0.5,1),nrow=2,ncol=2)


### LASSO
plot(ellipse(x=covmat,scale=c(1.88,1.88),centre=c(4,9.35),level=0.95, lwd=3),
     type = 'l',xlim=c(-5,10),ylim=c(-5,10),xlab=expression(beta[1]),
     ylab=expression(beta[2]),xaxt="n",yaxt="n", cex.lab=1, asp=1) 
abline(a=0,b=0,col="grey")
abline(v=0,col="grey")
lines(ellipse(x=covmat,scale=c(1.15,1.15),centre=c(4,9.35),level=0.95))
lines(ellipse(x=covmat,scale=c(.3,.3),centre=c(4,9.35),level=0.95))
points(4,9.35, pch=20, col="red", cex=1.7)
text(5.15,8.6, expression(hat(beta)^{OLS}), cex=1.4)

axis(1,at=0)
axis(2,at=0)
edge <- 5.4
lines(x=c(-edge,0),y=c(0,edge), lwd=1)
lines(x=c(-edge,0),y=c(0,-edge), lwd=1)
lines(x=c(0,edge),y=c(-edge,0), lwd=1)
lines(x=c(0,edge),y=c(edge,0), lwd=1)

points(0,5.4, col="cyan2", pch=20, cex=1.7)
text(1.2,6, expression(hat(beta)^{Lasso}), cex=1.4)



### RIDGE

plot(ellipse(x=covmat,scale=c(2.26,2.26),centre=c(4,9.35),level=0.95),
     type = 'l',xlim=c(-5,10),ylim=c(-5,10),xlab=expression(beta[1]),
          ylab=expression(beta[2]),xaxt="n",yaxt="n", cex.lab=1, asp=1)
abline(a=0,b=0,col="grey")
abline(v=0,col="grey")
lines(ellipse(x=covmat,scale=c(1.15,1.15),centre=c(4,9.35),level=0.95))
lines(ellipse(x=covmat,scale=c(.3,.3),centre=c(4,9.35),level=0.95))
points(4,9.35, pch=20, col="red", cex=1.7)
text(5.15,8.6, expression(hat(beta)^{OLS}), cex=1.4)

axis(1,at=0)
axis(2,at=0)
covmatc <- matrix(c(1,0,0,1),nrow=2,ncol=2)
lines(ellipse(x=covmatc,scale=c(2,2),level=0.85))
points(0.68,3.85, col="cyan2", pch=20, cex=1.7)
text(1.4,4.8, expression(hat(beta)^{Ridge}), cex=1.4)
@
Eine geschlossene Darstellung des Minimierungsproblems \eqref{Eq_Lasso_Lagrange} und damit eine analytische Lösung ist durch den $L_1$-Norm-Strafterm für die Lasso-Methode nicht möglich. Allerdings handelt es sich sowohl bei $\text{RSS}(\boldsymbol{\beta})$ als auch bei $P(\boldsymbol{\beta}, \boldsymbol{\gamma})=\gamma \sum_{j=1}^{p}|\beta_j|$ um \textit{konvexe} Funktionen (siehe Anhang \ref{App_Konvexe_Funktionen}), weswegen eine Lösung für das Minimierungsproblem existiert \cite{Osborne99onthe}.\\ 
Für den Fall, dass $\mathbf{X}'\mathbf{X}$ orthonormal ist, d.h. $\mathbf{X}'\mathbf{X}=\mathbf{I}$ gilt, können die Lasso-Schätzer durch die OLS-Schätzer bestimmt werden \cite{tibshirani96regression}:
\begin{align}\label{Eq_Lasso_Orthonormaldesign}
\beta_j^{Lasso}=\text{sign}(\beta_j^{OLS})(|\beta_j^{OLS}|-\gamma)^+
\end{align}
Mit $\text{sign}(\beta_j^{OLS})$ wird das Vorzeichen des entsprechenden OLS-Schätzers bezeichnet und $(\cdot)^+$ gibt an, dass der Ausdruck in der Klammer nur ausgewertet wird, wenn $|\beta_j^{OLS}|-\gamma > 0$ gilt. Somit wird $\beta_j^{Lasso} = 0$ gesetzt, wenn $\gamma \ge |\beta_j^{OLS}|$ ist. Ein Beweis für \eqref{Eq_Lasso_Orthonormaldesign} findet sich in Anhang \ref{App_Lasso_Orthonormal}. ABER ORTHONORMALES DESIGN SETZT $p=n$ VORAUS!!!

%%% Veerweisen:
%Fused lasso Tibshirani 2005
%Fused lasso for unordered predictors


\subsection{Elastic Net}\label{Kap_Elastic-Net}
\subsection{Netzwerkbasierte Penalisierung}\label{Netzwerkbasierte_Penalisierung}



\bibliography{Masterarbeit}


\begin{appendix}
\chapter{Beweise}
\section{Erwarteter quadrierter Abstand von $\boldsymbol{\hat{\beta}}^{OLS}$ zu $\boldsymbol{\beta}$}\label{App_Beweis_Erwarteter_quadrierter_Abstand}
Bezeichnet $\mathbf{B}$ eine $n \times p$-Matrix und $\mathbf{C}$ eine $p \times n$-Matrix, so gilt
\begin{align}\label{Eq_Spurgleichheit}
\text{sp}(\mathbf{BC})=\text{sp}(\mathbf{CB})
\end{align}
und
\begin{align}\label{Eq_Spurmultiplikation}
\text{sp}(\mathbf{M}\mathbf{D})=d \cdot \text{sp}(\mathbf{M})
\end{align}
für eine $m \times m$-Diagonalmatrix $\mathbf{D}=diag(d)$ mit den Werten $d$ und eine $m \times m$-Matrix $\mathbf{M}$ \cite{fahrmeir2009regression}.\\

Sei $\mathbf{b}$ ein $n \times 1$-Spaltenvektor und $\mathbf{A}$ eine $n \times n$-Matrix, so lässt sich zeigen \cite{mathai1992quadratic}, dass für den Erwartungswert der quadratischen Funktion $Q(\mathbf{b})=\mathbf{b}'\mathbf{A}\mathbf{b}$
\begin{align}\label{Eq_Erwartung_Quadrat}
\mathbb{E}(\mathbf{b}'\mathbf{A}\mathbf{b})=\text{sp}(\mathbf{A}\boldsymbol{\Sigma})+\mathbb{E}(\mathbf{b})'\mathbf{A}\mathbb{E}(\mathbf{b})
\end{align}
gilt, wobei $\boldsymbol{\Sigma}$ die Kovarianzmatrix von $\mathbf{b}$ ist.\\

Unter Zuhilfenahme von \eqref{Eq_Spurgleichheit} bis \eqref{Eq_Erwartung_Quadrat} kann in dieser Arbeit der in der Literatur nur abgekürzte Beweis für $\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace = \sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}]$ ausführlich dargestellt werden:
\begin{align}\label{Eq_Beweis_Erwarteter_Quadrierter_Abstand}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace
&=\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace \notag \\
&=\mathbb{E} \left\lbrace (\boldsymbol{\hat{\beta}}^{OLS} - \boldsymbol{\beta} )'(\boldsymbol{\hat{\beta}}^{OLS}-\boldsymbol{\beta}) \right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} - \boldsymbol{\beta}]' [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} - \boldsymbol{\beta}] \right\rbrace \notag \\
&\stackrel{\eqref{Eq_klass_lin_matrix}}{=}\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} +\boldsymbol{\varepsilon} ) -\boldsymbol{\beta}]' [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} +\boldsymbol{\varepsilon} ) -\boldsymbol{\beta}] \right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [\mathbf{I}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon} - \boldsymbol{\beta}]'[\mathbf{I}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon} - \boldsymbol{\beta}]
\right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]'[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}] 
\right\rbrace \notag \\
&=\mathbb{E}\left\lbrace
\boldsymbol{\varepsilon}'\underbrace{\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'}_{\substack{n \times n \text{-Matrix}}}\boldsymbol{\varepsilon}
\right\rbrace \notag \\
&\stackrel{\eqref{Eq_Erwartung_Quadrat}}{=}\text{sp}[\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\underbrace{\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})}_{\substack{\sigma^2\mathbf{I}}}] + \underbrace{\mathbb{E}(\boldsymbol{\varepsilon})'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbb{E}(\boldsymbol{\varepsilon})}_{\substack{0}} \notag \\
&\stackrel{\eqref{Eq_Spurmultiplikation}}{=}\sigma^2\text{sp}[\underbrace{\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}}_{\substack{:=\mathbf{C}}}\underbrace{(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'}_{\substack{:=\mathbf{D}}}] \notag \\
&\stackrel{\eqref{Eq_Spurgleichheit}}{=}\sigma^2\text{sp}[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}] 
\notag \\
&=\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}].
\end{align}

\section{Lasso-Schätzer für ein orthonormales Design}\label{App_Lasso_Orthonormal}
Im orthonormalen Design gilt wegen $\mathbf{X}'\mathbf{X}=\mathbf{I}$ für den OLS-Schätzer  $\hat{\beta}^{OLS}=\mathbf{X}'\mathbf{Y}$, wodurch sich $\text{PRSS}_{Lasso}(\boldsymbol{\beta}, \gamma)$ schreiben lässt als
\begin{align*}
\text{PRSS}_{Lasso}(\boldsymbol{\beta}, \gamma)
&=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+ \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&=\mathbf{Y}'\mathbf{Y}-2\mathbf{Y}'\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&=\mathbf{Y}'\mathbf{Y}-2\boldsymbol{\hat{\beta}}^{OLS}{'}\boldsymbol{\beta} + \boldsymbol{\beta}'\boldsymbol{\beta}+ \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&=\mathbf{Y}'\mathbf{Y}-2 \sum_{j=1}^{p}\hat{\beta}_j^{OLS} \beta_j + \sum_{j=1}^{p} \beta^2 + \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&=\mathbf{Y}'\mathbf{Y}+\sum_{j=1}^{p}\left( -2 \hat{\beta}_j^{OLS}  \beta_j + \beta^2 + \gamma |\beta_j| \right).
\end{align*}
Somit ergeben sich $j=1,\dots,p$ Gleichungssysteme, die nach $\beta_j$ abgeleitet und Null gesetzt werden müssen:
\begin{align}\label{Eq_Lasso_Einzelbeta}
\hat{\beta}_j^{Lasso}=\frac{\partial}{\partial \beta_j} \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS} \beta_j + \beta_j^2 + \gamma |\beta_j| \qquad \text{für} \ j=1,\dots,p .
\end{align}
Anhand der Darstellung in \eqref{Eq_Lasso_Einzelbeta} wird deutlich, dass der Fall $\hat{\beta}_j^{OLS}>0$ von $\hat{\beta}_j^{OLS}<0$ unterschieden werden muss. Denn für $\hat{\beta}_j^{OLS}>0$ muss $\beta_j > 0$ sein, da ansonsten ein kleinerer Wert möglich wäre. Umgekehrt muss für den Fall $\hat{\beta}_j^{OLS}<0$ auch $\beta_j < 0$ gelten, um ebenfalls auszuschließen, dass es durch einen Vorzeichenwechsel einen noch kleineren Wert für das Minimum gibt. FÜR BETA GLEICH = NICHT DEFINIERT: DURCH UNTERSCHEIDUNG KANN BETRAG BEI ABLEITEN WEGGELASSEN WERDEN UND VORZEICHEN VOR GAMMA GEÄNDERT WERDEN!! Für den Fall $\hat{\beta}_j^{OLS}>0$ ergibt sich
\begin{align}
&& 0&=\frac{\partial}{\partial \beta_j} \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS}  \beta_j + \beta_j^2 + \gamma \beta_j \notag \\
&\Rightarrow& 0&=-2\hat{\beta}_j^{OLS} + 2\beta_j + \gamma \notag \\
&\Leftrightarrow& \hat{\beta}_j^{Lasso}&=\hat{\beta}_j^{OLS}-\frac{\gamma}{2} \notag \\
&\Leftrightarrow&\hat{\beta}_j^{Lasso}&=\text{sign}(\hat{\beta}_j^{OLS})(|\hat{\beta}_j^{OLS}|-\frac{\gamma}{2})^+
\end{align}

\begin{align}
\frac{\partial \text{PRSS}_{Lasso}(\boldsymbol{\beta}, \gamma)}{\partial \beta_j}
	&=-2\hat{\beta}_j^{OLS} + 2\beta_j + \gamma \frac{\beta_j}{|\beta_j|} \notag \\
\end{align}
BLA
\begin{align}
\frac{\partial \text{PRSS}_{Lasso}(\boldsymbol{\beta}, \gamma)}{\partial \boldsymbol{\beta}}
	&=-2\boldsymbol{\hat{\beta}}^{OLS} + 2\boldsymbol{\beta} + \gamma \sum_{j=1}^{p} \frac{\beta_j}{|\beta_j|} \notag \\
	&=-2\boldsymbol{\hat{\beta}}^{OLS} + 2\boldsymbol{\beta} + \gamma \text{sign}(\boldsymbol{\beta})
\end{align}


\chapter{Erläuterungen}
\section{Verlust- und Risikofunktionen}\label{App_Risikofunktion}
Wird die Schätzung $\boldsymbol{\hat{\theta}}$ für den wahren Parametervektor $\boldsymbol{\theta}$ als eine \textit{Entscheidung} $d$ aufgefasst, stellt sich die Frage, wie gut diese Entscheidung war. Die Grundlage der Beantwortung ist der \textit{Parameterraum} $\Theta$ aller möglichen $\boldsymbol{\theta}$, der \textit{Entscheidungsraum} $\text{D}$ aller möglichen $d$ sowie eine \textit{Verlustfunktion} $\mathscr{L}(d,\boldsymbol{\theta})$ \cite{judge1985theory}. Formal ist die Verlustfunktion eine Abbildung
\begin{align*}
\mathscr{L}:\Theta \times \text{D} \rightarrow \mathbb{R},
\end{align*}
die den Verlust als einen Wert aus $\mathbb{R}$ angibt, wenn sich beim wahren Parametervektor $\boldsymbol{\theta}$ für $d$ entschieden wird \cite{gross2003linear}. Dabei wird die Entscheidung $d$ über eine Entscheidungsfunktion $\nu$ für die Beobachtungen $\mathbf{y}$ getroffen. In der Regel wird $\mathbf{y}$ als Realisierung des Zufallsvektors $\mathbf{Y}$ angesehen, sodass $\nu(\mathbf{Y})=d$ auch als Zufallsvektor aufgefasst werden kann. Im Hinblick auf die Eingangsfrage, wie gut eine Entscheidung $d$ ist, ist daher nicht der Verlust durch die Funktion $\nu(\mathbf{y})$ für die konkrete Beobachtung, sondern für die Zufallsvariable $\mathbf{Y}$ von Interesse. Dafür bietet sich der \textit{erwartete Verlust} 
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
an, welcher der Erwartungswert der Verlustfunktion ist und als \textit{Risko} oder \textit{Riskofunktion} bezeichnet wird \cite{gross2003linear,judge1985theory}.
Beispielsweise kann bei der klassischen linearen Regression $\boldsymbol{\hat{\beta}}^{OLS}$ als Entscheidung $d$ durch die Methode der kleinsten Quadrate aufgefasst und als Verlustfunktion $\mathscr{L}(\boldsymbol{\hat{\beta}}^{OLS})$ der quadrierte Abstand zu $\boldsymbol{\beta}$ gewählt werden. Die zugehörige Risikofunktion ist dann die erwartete quadrierte Länge von $\boldsymbol{\hat{\beta}}^{OLS}$.\\
Für den Vergleich von zwei verschiedenen Entscheidungsfunktionen $\nu_1(\mathbf{Y})$ und $\nu_2(\mathbf{Y})$, im Fall dieser Arbeit von Schätzmethoden für $\boldsymbol{\beta}$, können nach \citeNP{gross2003linear} drei Fälle unterschieden werden: 
\begin{enumerate}
\item Die Funktion $\nu_1(\mathbf{Y})$ ist \textit{gleichmäßig nicht schlechter} als eine Funktion $\nu_1(\mathbf{Y})$, wenn
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu_1(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace \leq \mathbb{E} \left\lbrace \mathscr{L}(\nu_2(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
für alle $\boldsymbol{\theta} \in \Theta$ gilt.

\item Die Funktion $\nu_1(\mathbf{Y})$ ist \textit{gleichmäßig besser} als eine Funktion $\nu_1(\mathbf{Y})$, wenn
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu_1(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace < \mathbb{E} \left\lbrace \mathscr{L}(\nu_2(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
für mindestens ein $\boldsymbol{\theta} \in \Theta$ und ansonsten Fall $1.$ gilt.

\item Die Funktion $\nu_1(\mathbf{Y})$ ist \textit{strikt besser} als eine Funktion $\nu_1(\mathbf{Y})$, wenn
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu_1(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace < \mathbb{E} \left\lbrace \mathscr{L}(\nu_2(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
für alle $\boldsymbol{\theta} \in \Theta$gilt.
\end{enumerate}

\section{Lagrange-Form und -Multiplikatoren}\label{App_Lagrange-Form}
Optimierungsprobleme können um Nebenbedingungen für die Variablen, die variiert werden sollen, ergänzt werden. Sei $\mathbf{x} \in R^p$, so kann beispielsweise
\begin{align}\label{Eq_Optimierung_Equal}
&&\arg \displaystyle\min_{\mathbf{x}} \ & f(\mathbf{x}) \notag \\ 
&&\text{s.t.} \ & g_i(\mathbf{x})=t_i \qquad \text{für} \  i=1,\dots,p^* \qquad \text{mit} \ p^* \le p
\end{align}
ein Minimierungsproblem sein, wobei $f(\mathbf{x})$ die Zielfunktion und $g_j(\mathbf{x})=t_i$ für $i=1,\dots,p^*$ die zu erfüllenden Nebenbedingungen sind. 
\begin{comment}Es muss stets $p^* < p$ gelten, da ansonsten die Lösung für das Optimierungsproblem durch die Bedingungen nicht eingeschränkt, sondern bestimmt wäre.
\end{comment}
Werden die Nebenbedingungen als $g_i(\mathbf{x})-t_i=0$ formuliert, kann \eqref{Eq_Optimierung_Equal} alternativ in der \textit{Lagrange-Form} dargestellt werden \cite{haftka1992elements}, die definiert ist als
\begin{align}\label{Eq_lagrange_Allgemein}
\mathcal{L}(\mathbf{x},\boldsymbol{\gamma})=f(\mathbf{x})+ \sum_{i=1}^{p^*}\gamma_i (g_i(\mathbf{x})-t_i).
\end{align}
Die Lagrange-Form hat keine zusätzlichen Nebenbedingungen, ist dafür allerdings um $p^*$ Argumente, welche die Nebenbedingung aus \eqref{Eq_Optimierung_Equal} widerspiegeln, und den zugehörigen \textit{Lagrange-Multiplikatoren} $\gamma_i$ erweitert. Um eine Extremstelle zu finden, die die Nebenbedingungen erfüllt, muss $\mathcal{L}(\mathbf{x},\boldsymbol{\gamma})$ sowohl nach $\mathbf{x}$ als auch nach $\boldsymbol{\gamma}$ abgeleitet und Null gesetzt werden
\begin{align*}
\frac{\partial \mathcal{L}}{\partial x_j}=\frac{\partial f}{\partial x_j} + \sum_{i=1}^{p^*}\gamma_i \frac{\partial g_i}{\partial x_j}&=0 \qquad \text{für} \ j=1,\dots,p , \\ 
\frac{\partial \mathcal{L}}{\partial \gamma_i}=g_i(\mathbf{x})-t_i&=0 \qquad \text{für} \ i=1,\dots,p^* ,
\end{align*}
womit sich $p+p^*$ zu lösende Gleichungssysteme ergeben \cite{strang1991calculus}.\\
Geometrisch lassen sich die Nebenbedingungen als spezielle \textit{Konturlinien} der Nebenbedingungsfunktionen betrachten, die sich mit den Konturlinien der Zielfunktion überschneiden und an bestimmten Punkten diese \textit{tangieren}. Gesucht werden diese Tangentenpunkte, da nur dort ein stationärer Punkt für $f(\mathbf{x})$ und damit eine Extremstelle unter Einhaltung der Nebenbedingungen erreicht werden kann. Wird beispielsweise die Funktion $f(x,y)=(x-1)^2 + (y-1)^2$ mit der Nebenbedingung $g(x,y)=(x-1)^2+(y-1)^2=0.5$ betrachtet, überschneiden sich die Konturlinien der Zielfunktion mit der Konturlinie der Nebenbedingungen an allen Punkten bis auf an zwei Tangentenpunkten (siehe Abbildung \ref{Fig:Lagrange_Geo_Interpret}). Diese beiden Punkte sind das Minimum $(x=0.5,y=0.5)$ und Maximum $(x=-0.5,y=-0.5)$, da es unter Berücksichtigung der Nebenbedingung keine extremeren Punkte für $f(x,y)$ gibt.\\

\begin{figure}[ht]
 \begin{minipage}{0.5\linewidth}
  \centering
  \setkeys{Gin}{width=0.99\textwidth}
  \includegraphics{Konturplot.png}
 \end{minipage}%
 \begin{minipage}{0.5\linewidth}
  \centering
  \setkeys{Gin}{width=0.99\textwidth}
  \includegraphics{Funktionsplot.png}
 \end{minipage}
 \caption[Graphische Veranschaulichung einer Nebenbedingung]{Das linke Bild zeigt die Konturlinien (rote Kreise) der Zielfunktion $f(x,y)=(x-1)^2+(y-1)^2$ und als blauen Kreis die spezielle Konturlinie $g(x,y)=0.5$ der Nebenbedingung $g(x,y)=x^2+y^2$. Zusätzlich sind die Gradienten am Punkt $(0.5,0.5)$ für beide Funktionen eingezeichnet, die dort parallel zueinander sind. Im rechten Bild ist die Zielfunktion als elliptischer Paraboloid mit verschiedenen Konturlinien in rot eingezeichnet. Die blaue Ellipse kennzeichnet alle Punkte auf dem elliptischen Paraboloid, die die Nebenbedingung erfüllen. Am untersten Punkt dieser Ellipse ist ein Minimum und am obersten Punkt ein Maximum für die Zielfunktion unter Einhaltung der Nebenbedingung erreicht. Aus dem rechten Bild wird auch deutlich, dass Extremstellen unter Einhaltung der Nebenbedingung keine Extremstelle für die Zielfunktion ohne Nebenbedingung sein müssen.}
 \label{Fig:Lagrange_Geo_Interpret}
\end{figure} 
Bezeichnet $\nabla$ den \textit{Gradienten} einer Funktion, so befinden sich die gesuchten Tangentenpunkte an allen Punkten, für welche die Gleichung
\begin{align}\label{Eq_Gradientengleichung}
\nabla f(\mathbf{x}) &= \sum_{i=1}^{p^*} \gamma_i \nabla  (g_i(\mathbf{x})-t_i)
\end{align}
erfüllt ist. Mit \eqref{Eq_Gradientengleichung} wird gefordert, dass der Gradient von $f(\mathbf{x})$ parallel zur der Linearkombination der Gradienten $\sum_{i=1}^{p^*} \nabla  (g_i(\mathbf{x})-t_i)$ ist, die jeweils um $\gamma_i$ gestreckt bzw. gestaucht sind. Da Gradienten immer senkrecht zu den Konturlinien stehen \cite{strang1991calculus}, kann die Parallelität aus \eqref{Eq_Gradientengleichung} in den \textit{Tangentenpunkt} der Konturlinien erfüllt sein. Durch Umstellen von \eqref{Eq_Gradientengleichung} ist ersichtlich, dass für die Extremstellen ein Vektor $\mathbf{x}$ gesucht wird, der
\begin{align*}
\nabla \mathcal{L}(\mathbf{x},\boldsymbol{\gamma})&= \nabla f(\mathbf{x}) - \sum_{i=1}^{p^*} \gamma_i \nabla  (g_i(\mathbf{x})-t_i)=\mathbf{0}
\end{align*}
erfüllt.\\

Für den Fall, dass die Bedingungen für die Zielfunktion aus \ref{Eq_Optimierung_Equal} \textit{Ungleichungen} beinhalten, d.h. für
\begin{align}\label{Eq_Optimierung_Unequal}
&&\arg \displaystyle\min_{\mathbf{x}} \ & f(\mathbf{x}) \notag \\ 
&&\text{s.t.} \ & h_i(\mathbf{x}) \le t_i \qquad \text{für} \  i=1,\dots,p^* \qquad \text{mit} \ p^* \le p ,
\end{align}
müssen sogenannte \textit{Slackvariablen} eingeführt werden, durch die die Ungleichungen zu Gleichungen umgewandelt wird. Dieses Vorgehen beruht auf der Tatsache, dass es für $h_i(\mathbf{x})-t_i \le 0$ eine Variable $s_i^2$ geben muss, durch die $h_i(\mathbf{x})-t_i + s_i^2 = 0$ gilt, was wiederum einer Nebenbedingung als Gleichung entspricht \cite{haftka1992elements}. Dadurch ergibt sich als Lagrange-Form
\begin{align}
\mathcal{L}(\mathbf{x}, \boldsymbol{\gamma}, \mathbf{s})=f(\mathbf{x})+ \sum_{i=1}^{p^*}\gamma_i (h_i(\mathbf{x})-t_i + s_i^2),
\end{align}
die nach $\mathbf{x}$, $\boldsymbol{\gamma}$ und $\mathbf{s}$ abgeleitet und Null gesetzt werden muss:
\begin{align}
\frac{\partial \mathcal{L}}{\partial x_j}=\frac{\partial f}{\partial x_j} + \sum_{i=1}^{p^*} \gamma_i \frac{\partial h_i}{\partial x_j} &= 0 \qquad \text{für} \ j=1,\dots,p  , \label{Eq_Lag_Part_1} \\
\frac{\partial \mathcal{L}}{\partial \gamma_i}=h_i - t_i + s_i^2 &=0 \qquad \text{für} \ i=1,\dots,p^*  , \label{Eq_Lag_Part_2} \\
\frac{\partial \mathcal{L}}{\partial s_i}=2\gamma_i s_i &=0
\qquad \text{für} \ i=1,\dots,p^*  . \label{Eq_Lag_Part_3}
\end{align}
Anhand von \eqref{Eq_Lag_Part_2} und \eqref{Eq_Lag_Part_3} ist erkennbar, für welche Nebenbedingungen später ein Lagrange-Multiplikator existieren muss, da für den Fall, dass $g(\mathbf{x})_i-t_i < 0$ ist, die Slackvariable $s^2_i > 0$ sein muss und damit wegen \eqref{Eq_Lag_Part_3} für den Lagrange-Multiplikator $\gamma_i=0$ gilt. Die Nebenbedingung ist dann \textit{deaktiviert}, wohingegen für $s_i^2=0$ die Nebenbedingung \textit{aktiviert} ist und es einen Lagrange-Multipliaktor $\gamma_i \ge 0$ gibt \cite{arora2011introductionOptimum}. Geometrische bedeutet eine Deaktivierung, dass sich eine Extremstelle der Zielfunktion innerhalb der Fläche befindet, die durch die Konturlinie einer Nebenbedingung entsteht, und deswegen die Lösung unabhängig von dieser Nebenbedingung ist. Um eindeutige Lagrange-Multiplikatoren $\gamma_i$ zu erhalten, müssen die Gradienten der aktiven Nebenbedingungen linear unabhängig sein.\\
Neben den Ableitungen \eqref{Eq_Lag_Part_1} bis \eqref{Eq_Lag_Part_3} muss für die Nebenbedingungen $h_i(\mathbf{x}) - t_i \le 0$ bei Minimierungsproblemen gefordert werden, dass alle $\gamma_i \ge 0$ sind. Für die Begründung muss zunächst die \textit{Sensitivitätsanalyse} für die Zielfunktionen mit Nebenbedingungen eingeführt werden. Sei $\mathbf{x}^*$ ein Punkt, der eine Extremstelle unter Einhaltung der Nebenbedingungen aus \eqref{Eq_Optimierung_Unequal} beschreibt und abhängig von der Wahl von $t_i$ für $i=1,\dots,p^*$ ist, d.h. $\mathbf{x}^*=\mathbf{x}^*(\mathbf{t})$. Somit hängt auch die Zielfunktion $f^*=f^*(\mathbf{t})$, die die Nebenbedingung \textit{erfüllt}, von $\mathbf{t}$ ab. Beschreibt $\boldsymbol{\gamma}^* > \mathbf{0}$ die für die Lösung $\mathbf{x}^*$ zugehörigen Lagrange-Multiplikatoren, d.h. alle Nebenbedingungen sind aktiviert, gilt im Fall von $\mathbf{t}=\mathbf{0}$
\begin{align}\label{Eq_Ableitung_t_0}
\frac{\partial f^*}{\partial t_i}=\frac{\partial f^*(\mathbf{x}^*(\mathbf{0}))}{\partial t_i}=\frac{\partial \mathcal{L}(\mathbf{x}^*(\mathbf{0}), \boldsymbol{\gamma}^*, \mathbf{t})}{\partial t_i}=-\gamma^*_i \qquad \text{für} \ i=1,\dots,p^* .
\end{align}
Mittels der \textit{Taylorreihe} kann dann für einen beliebigen Vektor $\mathbf{t}$ die Zielfunktion $f(\mathbf{t})$ als 
\begin{align}\label{Eq_Taylor_Sensi_Lag-Multipli}
f(\mathbf{t})=f^*(\mathbf{x}^*(\mathbf{0})) + \sum_{i=1}^{p^*} \frac{\partial f^*(\mathbf{x}^*(\mathbf{0}))}{\partial t_i} t_i \stackrel{\eqref{Eq_Ableitung_t_0}}{=} f^*(\mathbf{x}^*(\mathbf{0})) -\sum_{i=1}^{p^*} \gamma^*_i \cdot t_i
\end{align}
dargestellt werden  \cite{arora2011introductionOptimum}. Durch Umstellen von \eqref{Eq_Taylor_Sensi_Lag-Multipli} gibt
\begin{align}\label{Eq_Differenz_Sensi_t_0}
\delta f^*=f(\mathbf{t})-f^*(\mathbf{x}^*(\mathbf{0}))=- \sum_{i=1}^{p^*} \gamma^* \cdot t_i
\end{align}
an, wie sich die optimale Lösung der Zielfunktion unter Einhaltung der Nebenbedingung verändert, wenn $\mathbf{t}$ variiert wird. Werden bspw. einzelne $t_i$ erhöht, kann dies wegen $h_i(\mathbf{x}) \le t_i$ als eine \textit{Abmilderung} der Nebenbedingungen aufgefasst werden, da sich das Set der möglichen Lösungen für das Optimierungsproblem vergrößert. Umgekehrt hat eine Verkleinerung von einzelnen $t_i$ eine \textit{Verstärkung der Einschränkung} durch die Nebenbedingungen zur Folge, weil das Set der möglichen Lösungen verkleinert wird.\\
Für das Minimierungsproblem aus \eqref{Eq_Optimierung_Unequal} bedeutet das, dass alle $\gamma_i$ positiv sein müssen, da \eqref{Eq_Differenz_Sensi_t_0} der Anstiegt bzw. den Abfall in $f(\mathbf{t})$ beschreibt, wenn einzelne $t_i$ verändert werden. Für ein negatives $\gamma_i$ würde eine Vergrößerung eines $t_i$ einen Anstieg in $f(t_i)$ bedeuten, was ein Widerspruch zur Vergrößerung des Sets der möglichen Lösungen für das Minimierungsproblem ist.

\begin{comment}

%%%
Dazu wird die 
Eine intuitive Erklärung hierfür ist der Umstand, dass für eine Nebenbedingung $h_i(\mathbf{x})-t_i \le 0$ \textit{gelockert} werden kann, indem sie zu $h_i(\mathbf{x})-t_i \le u_i$ mit $u_i > 0$ erweitert wird. Dadurch erweitern sich das Set der Möglichen Punkte für eine Extremstelle

\end{comment}

\section{Konvexe Funktionen}\label{App_Konvexe_Funktionen}
Eine Funktion $f:X \rightarrow \mathbb{R}$ heißt \textit{konvex}, wenn
\begin{align*}\label{Eq_Convex_Funktion}
f(\epsilon a + (1-\epsilon)b) \leq \epsilon f(a) + (1-\epsilon) f(b) \qquad \text{mit} \quad 0 \leq \epsilon \leq 1
\end{align*}
für alle $a,b \in X$ gilt \cite{matousek2006understanding}. Es handelt sich bei $f$ um eine \textit{streng konvexe} Funktion, wenn für alle $a \neq b$ gilt
\begin{align*}
f(\epsilon a + (1-\epsilon)b) < \epsilon f(a) + (1-\epsilon) f(b) \qquad \text{mit} \quad 0 \leq \epsilon \leq 1.
\end{align*}
Sind $g$ und $h$ zwei konvexe Funktionen und $p,q \ge 0$, so lässt sich zeigen, dass
\begin{align*}
f=p \cdot g + q \cdot h
\end{align*}
ebenfalls eine konvexe Funktion ist \cite{boyd2004convex}.

% Die eidesstattliche Erklärung auf einer neuen Seite
\newpage
% Keine Nummerierung für diesen Teil
\section*{Eigenständigkeitserklärung}
% Keine Kopf- und Fußzeilen ausgeben
\thispagestyle{empty}
% Aber trotzdem ins Inhaltsverzeichnis aufnehmen
\addcontentsline{toc}{chapter}{Eigenständigkeitserklärung}
% Hier der offizielle Text der eidesstattlichen Erklärung
Hiermit versichere ich, dass ich die vorliegende Arbeit "`XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXX"' selbstständig und ohne Benutzung anderer als der angegebenen 
Hilfsmittel angefertigt habe; die aus fremden Quellen direkt oder indirekt übernommenen Gedanken sind als solche kenntlich gemacht. 
Die Arbeit wurde bisher in gleicher oder ähnlicher Form keiner anderen Prüfungskommission vorgelegt und auch nicht veröffentlicht.
% Etwas Abstand für die Unterschrift
\vspace{3cm}
% Hier kommt die Unterschrift drüber
\begin{tabbing}
\hspace{6cm}  \= \kill
\textit{Bremen, \today} \> \textit{Moritz Hanke}
\end{tabbing}

\end{appendix}

\end{document}


