\documentclass[12pt, a4paper]{report}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[ngerman]{babel}
%\usepackage[pdftex]{graphicx}
\usepackage{a4wide}
\usepackage[onehalfspacing]{setspace}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[marginal]{footmisc}
\usepackage{url}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{longtable}
\usepackage{comment} 

\usepackage{rotating}


%für die Kopfzeile
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\nouppercase{\leftmark}}
\fancyhead[R]{\thepage} 

%für römische Zahlen
\newcommand{\RM}[1]{\MakeUppercase{\romannumeral #1}}


\usepackage{multirow}
\usepackage{tabularx}

%für Mathe
\usepackage{amsmath}
\usepackage{amsfonts}



\usepackage{apager}


\usepackage{float}
\restylefloat{figure}


% Für Lit.-Verzeichnis folgenden Befehl im Terminal ausführen: bibtex Masterarbeit_Moritz_Hanke
\bibliographystyle{apager_dgps}


% Fürs Abkürzungsverzeichnis folgenden Befehl im Terminal ausführen: makeindex Masterarbeit_Moritz_Hanke.nlo -s nomencl.ist -o Masterarbeit_Moritz_Hanke.nls
\usepackage{nomencl}
\let\abk\nomenclature
\renewcommand{\nomname}{Abkürzungsverzeichnis}
\setlength{\nomlabelwidth}{.20\hsize}
\renewcommand{\nomlabel}[1]{#1 \dotfill}
\setlength{\nomitemsep}{-\parsep}
\makenomenclature 


%\usepackage{Sweave}



\begin{document}

% Titelblatt
\thispagestyle{empty}

\begin{center}
\textbf{\LARGE{TITEL DER ARBEIT\\}} 
\end{center}

\begin{center}
\textbf{\Large{\\Masterarbeit}}
\end{center}
\begin{verbatim}

\end{verbatim}

\begin{figure}[htbp]
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
\includegraphics{Universitaet_Bremen.png}
\end{center}
\end{figure}

\begin{center}
\textbf{Fachbereich 3: Mathematik \\
Studiengang Medical Biometry/Biostatistics (M.Sc.)\\}
\end{center}
\begin{verbatim}

\end{verbatim}

\begin{flushleft}
\begin{tabular}{lll}
& & \\
& & \\
\textbf{Eingereicht von:} & & Hanke, Moritz \\
\textbf{Geboren am:} & & 07.08.1985 \\
\textbf{Matrikelnummer:} & & 2404575 \\
& & \\
& & \\
\textbf{Betreuung:} & & Prof. Dr. Iris Pigeot-Kübler\\
& & Dr. Ronja Foraita \\
& & \\
& & \\
\textbf{Eingereicht am:} & & \today\\
& & \\
& & \\
\end{tabular}
\end{flushleft}





%Verzeichnisse
\pagenumbering{Roman}
\tableofcontents

\listoffigures

\listoftables

\printnomenclature 

\abk{Lasso}{Least absolute shrinkage and selection operator}
\abk{DAG}{Directed acyclic graph}
\abk{OLS}{Ordinary least squares}
\abk{MSE}{Mean squared error}
\abk{RSS}{Residual sum of squares}
\abk{PRSS}{Penalized residual sum of squares}


% Begin der Arbeit
\chapter{Einleitung}
\pagenumbering{arabic}
Im Zusammenhang mit linearen Regressionen bei \textit{hochdimensionalen Daten} stellt sich häufig das so genannte \textit{"`large p, small n"'}-Problem. Dabei handelt es sich um den Umstand, dass ein Datensatz weniger \textit{Beobachtungen n} als \textit{erklärende Variablen p} beinhaltet. Speziell in der Genetik gilt oftmals sogar $p \gg n$, d.h. es werden beispielsweise die Expressionslevel von mehreren Tausend Genen bei wenigen Hundert Probanden gemessen. Für das klassische lineare Regressionsmodell $\mathbf{y}=\textbf{X}\boldsymbol\beta+\boldsymbol\epsilon$ liefert in diesem Fall die Schätzung $\boldsymbol{\hat{\beta}}$ mittels der \textit{Methode der kleinsten Quadrate }(englisch: ordinary least squares; \textit{OLS}) keine eindeutige Lösung und das Modell wird an die Daten überangepasst \cite{hastie_efficient_2004}. Deshalb wird gefordert, dass $\boldsymbol{\beta}$ \textit{spärlich} besetzt ist, d.h. für die meisten Regressionskoeffizienten $\beta_i=0$ gilt \cite{buehlmann2011statistics}. Hierbei handelt es sich speziell in der Genetik um eine realistische Annahme, da in der Regel nur wenige Gene für ein spezifische Responsevariablenausprägung verantwortlich sind. Die Schwierigkeit besteht darin, den Einfluss der wenigen aktiven Kovariablen zu schätzen und gleichzeitig eine Variablenselektion vorzunehmen, sodass Kovariablen ohne Einfluss aus dem Modell entfernt werden.\\
Ein verbreiteter Ansatz ist in diesem Zusammenhang die Einführung einer \textit{Penalisierungen} bei der Schätzung der Regressionskoeffizienten, was eine Schrumpfung und Selektion der Regressionskoeffizienten zur Folge hat. Dazu wird das Kleinste-Quadrat-Kriterium $\arg\min\limits_{\boldsymbol{\beta}} \sum_{i=1}^{n}(y_i - \mathbf{x}_i^T \boldsymbol{\beta})^2$ des OLS \cite{fahrmeir2009regression} um einen \textit{Strafterm} $P(\boldsymbol{\beta})$ erweitert, wodurch einzelne Regressionkoeffizienten $\beta_j$ als $\hat{\beta}_j=0$ geschätzt werden können. Die Folge ist eine eindeutige Lösung und der Vektor $\boldsymbol{\hat{\beta}}$ ist wie für $\boldsymbol{\beta}$ angenommen spärlich besetzt.\\ 
Für diese Strategie ist der \textit{Least absolute shrinkage and selection operator} (\textit{Lasso}) von \citeA{tibshirani96regression} eine der bekanntesten Methoden und wurde in den letzten Jahren im Hinblick auf die Einbeziehung von vorab bekannten Informationen im Strafterm erweitert. \citeA{li_network-constrained_2008} haben dazu einen Strafterm entwickelt, der die Netzwerkstruktur der erklärenden Variablen untereinander berücksichtigt. Dieser Ansatz ist besonders in der Genetik vielversprechend, da durch Datenbanken wie der \textit{Kyoto Encyclopedia of Genes and Genomes} (\textit{KEGG}; \citeNP{Kanehisa2000KEGG}) relativ umfangreiche Informationen über Genregulationsnetzwerke (sogenannte \textit{Pathways}) und damit über die Abhängigkeit von einzelnen Genen untereinander vorliegen.\\
In der vorliegenden Arbeit wird eine Weiterentwicklung der netzwerkbasierten Penalisierung durch \citeA{kim_network-based_2013} um verschiedene \textit{Zentralitätsmaße} bei der Schätzung der Parameter $\boldsymbol{\beta}$ vorgestellt. Die Ergebnisse werden untereinander sowie mit den "`klassischen"' Penalisierungsansätzen Lasso und \textit{Elastic net} \cite{zou_regularization_2005} verglichen. Ziel ist es dabei, den Einfluss der zusätzlichen Information über die erklärenden Variablen, die ein Netzwerk bilden, auf die Güte der Schätzung darzustellen und darauf basierende Empfehlungen für die Anwendung von netzwerkbasierten Penalisierungen abzugeben.






\chapter{Theorie}\label{Kap_Theorie}
\section{Graphen und Netzwerkstatistiken}\label{Kap_Graphen und Netzwerkstatistiken}
%
%
% Netzwerke <--> Graphen
%
%
In den nachfolgenden beiden Unterkapiteln wird eine Auswahl an grundlegenden Begriffen aus der Graphentheorie (Kapitel \ref{Kap_Grundlegende Begriffe der Graphentheorie}) sowie Statistiken für Netzwerkanalysen, insbesondere zur Zentralität und Verbundenheit von einzelnen Knoten (Kapitel \ref{Kap_Zentralitätsstatistiken}), eingeführt, die für diese Arbeit benötigt werden. Für eine ausführliche Erläuterung zur Graphentheorie bzw. zu Netzwerkstatistiken sei auf \citeA{diestel2006graph} sowie \citeA{kolaczyk2009statistical} und \citeA{newman2010networks} verwiesen.\\

\subsection{Grundlegende Begriffe der Graphentheorie}\label{Kap_Grundlegende Begriffe der Graphentheorie}
Ein Graph ist ein Paar $G=(V,E)$, das aus einer Menge $V=\{v_1,\dots,v_{N_V}\}$ an \textit{Knoten} und einer Menge $E=\{e_1,\dots,e_{N_E}\}$ an \textit{Kanten} besteht \cite{brandes2005graphfunda}. Die Kardinalitäten $N_V =|V|$ und $N_E=|E|$ geben die \textit{Ordnung} und die \textit{Größe} des Graphen an. Eine ungerichtete Kante $e \in E$, ist die Zuordnung zweier Knoten $u,v \in V$, für $u \neq v$, eines Graphen $G=(V,E)$, d.h. es gilt $\{u,v\} = e$. Die Knoten $u$ und $v$ werden in diesem Fall als \textit{inzident} zu der Kante $e$ und \textit{adjazent} zueinander bezeichnet. Für $e=\{u,v\}$ handelt es sich um ein \textit{ungeordnetes} Knotenpaar an Knoten, d.h. $\{u,v\}=\{(u,v),(v,u)\}$. Graphen, die nur ungeordnete Knotenpaare beinhalten, sind \textit{ungerichtet}. Dagegen bezeichnet $e=(u,v) \neq (v,u)$, dass es sich um ein \textit{geordnetes} Paar an Knoten handelt, wobei die Kante $e$ vom \textit{Anfangsknoten} $u$ auf den \textit{Endknoten} $v$ zeigt \cite{kolaczyk2009statistical}. Als \textit{gerichtete} Graphen werden Graphen bezeichnet, die nur geordnete Knotenpaare beinhalten. Der Graph $G_A=(V_A,E_A)$ ist ein \textit{Teilgraph} von $G=(V,E)$ wenn $V_A \subseteq V$ und $E_A \subseteq E$ gilt. Die in dieser Arbeit verwendeten Graphen sind \textit{schlichte} Graphen, d.h. es gibt weder \textit{Schlingen} noch mehrere Kanten zwischen zwei Knoten. Als Schlinge wird eine Kante bezeichnet, die nur einen Endknoten an beiden Kantenenden besitzen, d.h. $e=\{v,v\}$ bzw. $e=(v,v)$ \cite{tittmann2011graphen}. Darüber hinaus werden nur \textit{endliche} Graphen ($N_V < \infty$) berücksichtigt, die entweder gerichtet oder ungerichtet sind.\\

Der Knotengrad $d(v)$ für $v \in V$ gibt die Anzahl der Kanten an, die inzident auf den Knoten $v$ sind. Für einen Graphen $G=(V,E)$ sind der \textit{Minimumknotengrad} $\delta(G)$, der \textit{Maximumknotengrad} $\Delta(G)$ und der \textit{durchschnittliche Knotengrad} $\bar{d}(G)$ definiert \cite{diestel2006graph} als
\begin{align}
\delta(G)=\min\{d(v) \ | \ v \in V\}\label{Minimumknotengrad}
\end{align}
\begin{align}
\Delta(G)=\max\{d(v) \ | \ v \in V\}\label{Maximumknotengrad}
\end{align}
\begin{align}
\bar{d}(G)=\frac{1}{N_V}\sum_{v \in V}d(v).\label{durchschnittlicher Knotengrad}
\end{align}
Handelt es sich um einen gerichteten Graphen, kann für jeden Knoten $v \in V$ mittels $d^{in}(v)$ die Anzahl auf $v$ und mit $d^{out}(v)$ die Anzahl von $v$ gerichteten Kanten angegeben werden. Entsprechend können die Maße aus \eqref{Minimumknotengrad}, \eqref{Maximumknotengrad} und \eqref{durchschnittlicher Knotengrad} angepasst werden, indem nur $d^{in}(v)$ bzw. $d^{out}(v)$ berücksichtigt werden.\\

Die \textit{Adjazenzmatrix} $\textbf{A}=(a_{u,v})_{N_V \times N_V}$ kennzeichnet die Konnektivität, d.h. ob es eine Kante zwischen zwei Knoten gibt. Dafür sei 
\begin{align}
a_{u,v} = \begin{cases}
1, \qquad &\text{wenn} \ (u,v) \in E \ \textit{für} \ u \neq v \ ,\\
0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Handelt es sich bei $G=(V,E)$ um einen ungerichteten Graphen ist $\textbf{A}$ eine symmetrische Matrix \cite{kolaczyk2009statistical}.\\
Die \textit{Inzidenzmatrix} $\textbf{B}=(b_{v,e})_{N_V \times N_E}$ eines \textit{ungerichteten} Graphen $G=(V,E)$ gibt an, ob $e \in E$ inzident zu $v \in V$ ist \cite{kolaczyk2009statistical} und es sei
%nicht gut, dass da schon wieder "sei" ist
\begin{align}
b_{v,e} = \begin{cases}
1, \qquad &\text{wenn} \ v \in e \ ,\\
0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Für einen \textit{gerichteten} Graphen $G=(V,E)$ kann unterschieden werden, ob $v \in e$ der Anfangs- oder Endknoten für einen beliebigen Knoten $u \in V$ ist. In diesem Fall ist die Inzidenzmatrix $\textbf{\~B}=(\tilde{b}_{v,e})_{N_V \times N_E}$ definiert \cite{brandes2005graphfunda} als
\begin{align}
\tilde{b}_{v,e} = \begin{cases}
-1, \qquad &\text{wenn} \ (v,u) = e \  ,\\
\ 1, \qquad &\text{wenn} \ (u,v) = e \ ,\\
\ 0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Sei $\textbf{D}=\text{diag}(d(v), v \in V)$ eine \textit{Diagonalmatrix} und $\textbf{\~B}^T$ die transponierte Matrix von $\textbf{\~B}$, dann gilt $\textbf{\~B\~B}^T=\textbf{D}-\textbf{A}$. Dabei handelt es sich bei $\textbf{\~B\~B}^T$ um eine $N_V \times N_V$ Matrix, die als \textit{Laplace-Matrix} $\textbf{L}$ bezeichnet wird. Deren zweitkleinster \textit{Eigenwert} $\lambda_2$ ist umso größer, je mehr Kanten es zwischen den einzelnen Knoten gibt und kann als ein Maß für die \textit{Verbundenheit} des Graphen interpretiert werden \cite{kolaczyk2009statistical}.\\ 
\begin{comment}
Für die \textit{normierte Laplace-Matrix} $\textbf{\~L}$ gilt $\textbf{\~L}=\textbf{D}^{1/2}\textbf{L}\textbf{D}^{1/2}$, wobei $\textbf{D}^{1/2}=\text{diag}\frac{1}{\sqrt{d(v)}}$ für $d(v) \neq 0$ und ansonsten $0$ ist \cite{brandes2005graphfunda}.
\end{comment}
% WIRD das gebraucht, wenn der Graph partioniert werden soll??? -> Spectrum

Ein \textit{Weg} $\mathcal{W}(v_1,v_m)$ zwischen den Knoten $v_1$ und $v_m$ in einem Graphen $G=(V,E)$ bezeichnet eine Abfolge $\{v_1, e_1, v_2,\dots,e_{m-1},v_m \}$ mit $v_1, \dots, v_m \in V$ und $e_1,\dots,e_{m-1} \in E$, die die beiden Knoten miteinander verbindet und die \textit{Länge} $m-1$ hat \cite{diestel2006graph}. Für ungerichtete Graphen gilt hierbei $e_k=\{v_{k-1}, v_k\}$ und für gerichtete Graphen $e_k=(v_{k-1}, v_k)$. Wird zusätzlich gefordert, dass $e_k \neq e_l$ für $k \neq l$ gilt, d.h. dass keine Kante $e_k$ mehrmals in der Abfolge vorkommt, handelt es sich um eine \textit{Spur} $\mathcal{S}(v_1,v_m)$. Erfüllt ein Weg die Bedingung $v_k \neq v_l$ für $k \neq l$, wird dieser \textit{Pfad} $\mathcal{P}(v_1,v_m)$ genannt \cite{brandes2005graphfunda}. Für $v_1 = v_m$ ist $\mathcal{W}(v_1,v_m)$ ein \textit{Zyklus} und für $\mathcal{P}(v_1,v_m)$ ein \textit{Kreis}. Der kürzeste Pfad zwischen zwei Knoten $v_1,v_m \in V$ heißt \textit{Distanz} $D(v_1,v_m)$ und seine Länge entspricht der Anzahl an zugehörigen Kanten. Mit der \textit{größten} Distanz aus allen Knotenkombinationen eines zusammenhängenden Graphen $G=(V,E)$ wird dessen Durchmesser $\phi(G)$ angegeben \cite{diestel2006graph}. Wege, Pfade, Spuren, Zyklen und Kreise sind gerichtet, wenn der zugrunde liegende Graph gerichtet ist. Dieser wird, wenn er keinen Zyklus beinhaltet, als \textit{gerichteter, azyklischer Graph}  (englisch: directed acyclic graph; \textit{DAG}) bezeichnet \cite{kolaczyk2009statistical}.\\

Ein ungerichteter Graph $G=(V,E)$ ist \textit{zusammenhängend}, wenn jeder Knoten $v \in V$ von jedem anderen Knoten $u \in V$ über einen Weg erreicht werden kann. Entsprechend ist ein Graph \textit{unzusammenhängend}, wenn mindestens ein Knoten nicht von allen übrigen Knoten erreicht werden kann. 
Als \textit{Komponente} eines Graphen wird ein \textit{maximal zusammenhängender} Subgraph $G_A=(V_A,E_A)$ mit $V_A \subseteq V$ und $E_A \subseteq E$ für $G=(V,E)$ bezeichnet, wobei "`maximal"' die Eigenschaft ist, dass es keinen zusammenhängenden Subgraphen  $G_B=(V_B, E_B)$ in $G=(V,E)$ gibt, für den $V_B \supseteq V_A$ und $E_B \supseteq E_A$ gilt. Eine \textit{Clique} ist ein Subgraph, in dem alle Knoten adjazent zueinander sind.
\begin{comment}
Ein maximal zusammenhängender Teilgraph $G_A=(V_A,E_A)$, für den $V_A \subseteq V$ und $E_A \subseteq E$ für $G=(V,E)$ gilt und es keinen zusammenhängenden Subgraphen  gibt, heißt \textit{Komponente}.
\end{comment} 
Ist $G=(V,E)$ gerichtet, so wird dieser als \textit{schwach verbunden} bezeichnet, wenn der zugrunde liegende ungerichtete Graph zusammenhängend ist, und als \textit{stark verbunden}, wenn jeder Knoten von jedem anderen Knoten über einen direkten Weg erreichbar ist \cite{brandes2005graphfunda}. Mit der $k$-\textit{Verbundeheit} $\kappa(G)$ wird angegeben, wie viele Knoten mindestens aus einem zusammenhängenden Graphen $G=(V,E)$ \textit{entfernt} werden müssen, damit dieser unzusammenhängend wird \cite{diestel2006graph}.\\
% Cluster?
% tree?
% Cliquen?

\subsection{Zentralitätsstatistiken}\label{Kap_Zentralitätsstatistiken}
Um die \textit{Bedeutung} eines Knotens $u \in V$ zu bestimmen, kann seine \textit{Zentralität} innerhalb des Graphen $G=(V,E)$ angegeben werden. In der Literatur finden sich diverse Maße, die Zentralität teilweise sehr unterschiedlich definieren und daher bei der Beurteilung, wie zentral ein Knoten ist, zu unterschiedlichen Ergebnissen kommen können (vgl. dazu bspw. \citeA{koschuetzki2005centralityindices}). Ein Beispiel für die Mehrdeutigkeit des Begriffs Zentralität sind die Knoten $1$, $7$ und $8$ im Graphen in Abbildung \ref{fig:Degree_vs_Path}. Wird die Zentralität schlicht als die Anzahl an benachbarten Knoten aufgefasst, hat Knoten $1$ (gelb) eine relativ zentrale Position ($d(1)=6$), Knoten $8$ (rot) hingegen ist nach dieser Definition weniger zentral gelegen ($d(8)=5$). Wird dagegen die Zentralität als die Anzahl an Pfaden, auf denen ein Knoten liegt aufgefasst, ist unmittelbar ersichtlich, dass der Knoten $8$ (rot) zentraler als der Knoten $1$ ist.\\


<<echo=FALSE>>=

#             1  2  3  4  5  6 6.5 7  8  9 10 11 12 13 14 15 16 17 18 19 
A <- matrix(c(0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #1 
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #2
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #3
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #4
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #5
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #6
              1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #6.5
              0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #7
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,  #8
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,  #9
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,  #10
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,  #11
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #12
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #13
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #14
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #15
              0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #16
              0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #17
              0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #18
              0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), ncol=20, byrow=T)

library(igraph)

g <- graph.adjacency(A, "undirected")
V(g)$color <- 5
V(g)[1]$color <- "yellow"
V(g)[8]$color <- "red"
#V(g)[7]$color <- "green"
@
<<echo=FALSE, Degree_vs_Path, fig.pos="H", out.width='.6\\linewidth', fig.align='center', fig.lp="fig:", fig.cap="Beispiel für einen Graphen, in dem in Abhängigkeit von der Definition von Zentralität, entweder der rote oder der gelbe Knoten die bedeutsamste Position einnimmt.">>=
plot(g, edge.width=3)
@

\paragraph*{Eigenvektor-Zentralität}\label{Kap_Eigenvektor-Zentralität}
\paragraph*{Closeness-Zentralität}\label{Kap_Closeness-Zentralität}
\paragraph*{Betweeness-Zentralität}\label{Kap_Betweeness-Zentralität}

% gibt es Statistik d(v_i)/\kappa(G)? würde die was taugen?
% statt d(v_i) sowas wie nachbran von nachbarn?


\section{Lineare Regression}\label{Kap_Lineare Regression}
In den drei folgenden Unterkapiteln werden die Grundlagen und Annahmen des klassischen linearen Regressionsmodells vorgestellt, die für die Methoden der penalisierten Regression in Kapitel \ref{Kap_Penalisierte Regression} Voraussetzung sind. Wenn nicht anders gekennzeichnet, bezieht sich die Darstellung auf \citeA{fahrmeir2009regression}, \citeA{montgomery2012introduction} bzw. \citeA{seber2003linear}, die eine vertiefende und weiterführende Einführung in die lineare Regression geben. Kapitel \ref{Kap_Das klassische lineare Modell} beinhaltet eine kurze Übersicht über die Annahmen des klassischen linearen Modells. Die für die Schätzung der Regressionskoeffizienten am häufigsten verwendete Methode der kleinsten Quadrate wird in Kapiten \ref{Kap_Methode der Kleinsten Quadrate} erläutert. Im abschließenden Unterkapitel \ref{Kap_Multikollinearität und $p > n$} werden die Probleme Multikollinearität und $p > n$, die beide in der Genetik vorkommen und Penalisierungsansätze notwendig machen, vorgestellt.

\begin{comment}
$\mathbf{X}$ sind ZVs\\
$\mathbf{y}$ ist eine kontinuierliche ZVs
\end{comment}
\subsection{Das klassische lineare Modell}\label{Kap_Das klassische lineare Modell}
Im klassischen linearen Regressionsmodell wird eine Beziehung zwischen der \textit{Responsevariable} $y$ und den \textit{Kovariablen} $x_1, \dots, x_p$ durch eine \textit{Linearkombination} modelliert
\begin{align}\label{klassisches_modell}
y=\beta_0 + \beta_1 x_1 + \dots + \beta_p k_p + \varepsilon.
\end{align}
Dafür müssen die Parameter $\beta_0, \beta_1, \dots, \beta_p$ geschätzt werden, wobei $\beta_0$ eine \textit{Konstante} (englisch: Intercept) ist. $\varepsilon$ in \eqref{klassisches_modell} bezeichnet eine \textit{zufällige Störgröße}, die unabhängig von den Beobachtungen auftritt und \textit{additiv} wirkt. Liegen $n$ Beobachtungen sowie $p$ Kovariablen vor und seien
\begin{align*}
\mathbf{y} = (y_1,\dots,y_n)'
\qquad \text{und} \qquad
\boldsymbol{\beta} = (\beta_1,\dots,\beta_n
)'
\qquad \text{sowie} \qquad
\boldsymbol{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n
)'
\end{align*}
Spaltenvektoren und
\begin{align*}
\mathbf{X} = \begin{pmatrix}
1 & x_{11} & \dots & x_{1p}\\ 
\vdots & \vdots & &\vdots\\
1 & x_{n1} & \dots & x_{np}
\end{pmatrix}
\end{align*}
die \textit{Designmatrix}, so kann \eqref{klassisches_modell} für die $n$ Beobachtungen als
\begin{align}\label{klass_lin_matrix}
\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
\end{align}
dargestellt werden. Dazu muss angenommen werden, dass $\mathbf{X}$ \textit{vollen Spaltenrang} $\text{rg}(\mathbf{X})=p$ besitzt und die erste Spalte aus dem Vektor $\mathbf{1}=(1,\dots, 1)'$ besteht, um den Intercept zu berücksichtigen. Sowohl die Responsevariable als auch die Kovariable werden als Realisierungen von Zufallsvariablen aufgefasst. Für die Fehler $\boldsymbol{\varepsilon}$ wird angenommen, dass  $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$, $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$ und $\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}')=\sigma^2\mathbf{I}$ sei, wobei $\mathbf{I}$ die Einheitsmatrix bezeichnet. Für die Responsevariable gilt
\begin{align}
\mathbb{E}(\mathbf{y})&=\mathbb{E}(\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon})=\mathbf{X} \boldsymbol{\beta} \label{Erwartungswert_y} \\ 
\mathbb{C}\text{ov}(\mathbf{y})&=\mathbb{C}\text{ov}(\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon})=\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\sigma^2\mathbf{I} \label{Covarianz_y}
\end{align}
und unter zusätzlicher Annahme von \textit{normalverteilten} Fehlern, dass
\begin{align}
\mathbf{y} \sim \text{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}).
\end{align} 

\subsection{Methode der Kleinsten Quadrate}\label{Kap_Methode der Kleinsten Quadrate}
Die Schätzung der Regressionskoeffizienten $\boldsymbol{\beta}$ für $\boldsymbol{\beta} \in \mathbb{R}^p$ kann bei vollem Spaltenrang von $\mathbf{X}$ mittels der \textit{Methode der kleinsten Quadrate} (\textit{KQ-Methode}; englisch: ordinary least squares; \textit{OLS}) erfolgen. Für die Schätzer $\boldsymbol{\hat{\beta}}^{OLS}$ wird die \textit{Residuenquadratsumme} (englisch: residual sum of squares; \textit{RSS}) 
\begin{align}\label{RSS_OLS}
RSS(\boldsymbol{\beta})&=\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}=\sum_{i=1}^{n}(y_i - \mathbf{x}_i'\boldsymbol{\beta})^2=(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{y}-\mathbf{X}\boldsymbol{\beta}) \notag \\
&=\mathbf{y}^T\mathbf{y} -2\mathbf{y}^T\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
\end{align}
bzgl. $\boldsymbol{\beta}$ minimiert, d.h. 
\begin{align}
\boldsymbol{\beta}^{OLS}&=\arg \displaystyle\min_{\beta} (\mathbf{y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align}
Differenzieren von \eqref{RSS_OLS} nach $\boldsymbol{\beta}$
\begin{align}\label{Ableitung_OLS}
\frac{\partial \text{RSS}(\boldsymbol{\beta})}{\partial  \boldsymbol{\beta}} = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
\end{align}
und anschließendes Nullsetzen ergibt die sogenannte \textit{Normalengleichung}
\begin{align}\label{Normalengleichung}
&& 0&=-2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} \notag \\
&\Leftrightarrow & \mathbf{X}^T\mathbf{X}\boldsymbol{\beta}&= \mathbf{X}^T\mathbf{y}.
\end{align}
Sind die Spalten von $\mathbf{X}$ \textit{linear unabhängig} und gilt $p \le n$, sodass $rg(\mathbf{X})=p$, handelt es sich bei $(\mathbf{X}'\mathbf{X})$ um eine \textit{positiv definite} und damit invertierbare Matrix. Als Schätzer $\boldsymbol{\hat{\beta}}^{OLS}$ ergibt sich aus \eqref{Normalengleichung}
\begin{align}\label{OLS_Schätzer}
\boldsymbol{\hat{\beta}}^{OLS} &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.
\end{align}
Mittels $\boldsymbol{\hat{\beta}}^{OLS}$ kann die Responsevariable $\mathbf{y}$ als
\begin{align}\label{y_Schaetzung}
\mathbf{\hat{y}}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}=\mathbf{X}\boldsymbol{\hat{\beta}}^{OLS}
\end{align}
geschätzt werden, sodass sich die Residuen $\boldsymbol{\hat{\varepsilon}}$ als Abweichungen der geschätzten von den beobachteten Responsevariablenausprägungen ergeben
\begin{align}
\boldsymbol{\hat{\varepsilon}}=\mathbf{y}-\mathbf{\hat{y}}=\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{OLS}.
\end{align}
Unter Berücksichtigung der Modellannahme aus \eqref{klass_lin_matrix} und $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$ ist der OLS-Schätzer ein unverzerrter Schätzer für $\boldsymbol{\beta}$, da
\begin{align}\label{Erwartungswert_OLS}
\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS})&=\mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}\right]=\mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon})\right] \notag \\
&= \mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon} \right] \notag \\
&=\boldsymbol{\beta}
\end{align}
und für die Kovarianz von $\boldsymbol{\hat{\beta}}^{OLS}$ gilt wegen $\mathbb{C}\text{ov}(\mathbf{y})=\sigma^2 \mathbf{I}$, dass
\begin{align}\label{Cov_OLS}
\mathbb{C}\text{ov}(\boldsymbol{\hat{\beta}}^{OLS})&=\mathbb{C}\text{ov}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}\right]=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \mathbb{C}\text{ov}(\mathbf{y})\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\right]' \notag \\
&=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \sigma^2 \mathbf{I}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\right]'=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1} \notag \\
&=\sigma^2(\mathbf{X}'\mathbf{X})^{-1}.
\end{align}
Mit dem Varianzschätzer $\hat{\sigma}^2=\frac{1}{n-p}\boldsymbol{\hat{\varepsilon}}'\boldsymbol{\hat{\varepsilon}}$ kann die Kovarianz von $\boldsymbol{\hat{\beta}}^{OLS}$ als
\begin{align}
\widehat{\mathbb{C}\text{ov}(\boldsymbol{\hat{\beta}}^{OLS})}=\frac{1}{n-p}\boldsymbol{\hat{\varepsilon}}'\boldsymbol{\hat{\varepsilon}}(\mathbf{X}'\mathbf{X})^{-1}
\end{align}
geschätzt werden.\\
Mittels des \textit{Gauß-Markov-Theorems} lässt sich zeigen, dass der KQ-Schätzer unter allen \textit{unverzerrten linearen} Schätzern die kleinste Varianz hat. Das bedeutet, für jeden anderen erwartungstreuen linearen Schätzer $\boldsymbol{\tilde{\beta}}$ gilt
\begin{align}
\mathbb{V}\text{ar}(\boldsymbol{\tilde{\beta}}) \ge \mathbb{V}\text{ar}(\boldsymbol{\hat{\beta}}^{OLS})
\end{align}
und somit für den\textit{ mittleren quadratischen Fehler} (englisch: mean squared error; \textit{MSE}) wegen $\text{Bias}(\boldsymbol{\tilde{\beta}})=\text{Bias}(\boldsymbol{\hat{\beta}}^{OLS})=0$
\begin{align}
\text{MSE}(\boldsymbol{\tilde{\beta}}) \ge \text{MSE}(\boldsymbol{\hat{\beta}}^{OLS}).
\end{align}

\subsection{Multikollinearität und $p > n$}\label{Kap_Multikollinearität und $p > n$}
Korrelieren zwei oder mehr Kovariablen der Matrix $\mathbf{X}$ hoch miteinander, liegt eine \textit{approximative Multikollinearität} vor. In diesem Fall ist $\mathbf{X}$ und damit auch $\mathbf{X}'\mathbf{X}$ invertierbar, allerdings kommt es zu einer Inflation der Varianz von $\boldsymbol{\hat{\beta}}^{OLS}$. Sei $\hat{\beta}_j$ der Schätzer für die Kovariable $\mathbf{x}_j$, so kann dessen Varianz dargestellt werden als
\begin{align}\label{var_beta_j}
\mathbb{V}\text{ar}(\hat{\beta}_j)=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}_{jj}=\frac{\sigma^2}{1-R^2_j} \ ,
\end{align}  
wobei $R^2_j$ der (multiple) \textit{Determinationskoeffizient} für die Regression von $\mathbf{x}_j$ auf die übrigen $p-1$ Kovariablen ist. Je stärker $\mathbf{x}_j$ mit den restlichen Kovariablen korreliert, desto kleiner wird der Nenner von \eqref{var_beta_j} und damit umso größer die Varianz von $\hat{\beta}_j$.\\
Bei einer \textit{exakten} Multikollinearität kann eine Kovariable als Linearkombination von anderen Kovariablen dargestellt werden. In diesem Fall ist $\mathbf{X}$ und damit auch $\mathbf{X}'\mathbf{X}$ singulär \cite{strang09ointro_linalg}, weswegen die Normalengleichung aus \eqref{Normalengleichung} nicht nach $\boldsymbol{\beta}$ aufgelöst werden kann und es keine eindeutige Lösung für $\boldsymbol{\beta}$ gibt. Bei hochdimensionalen Daten mit $p>n$ oder sogar $p \gg n$ ist $rg(X) < p$ und die Normalengleichung kann keinen eindeutigen Schätzer für $\boldsymbol{\beta}$ liefern \cite{johnstone_statistical_2009}.


\section{Penalisierte Regression}\label{Kap_Penalisierte Regression}
Für \textit{hochdimensionalen} Daten, wie sie in der Genetik vorliegen, kann aus \textit{inhaltlichen Gründen} häufig die Annahme vertreten werden, dass nur wenige Kovariablen einen Einfluss auf die Responsevariable haben. Wird dieser Zusammenhang als klassische lineare Regressionsmodell $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}$ ausgedrückt, wird deshalb angenommen, dass der Regressionskoeffizientenvektor $\boldsymbol{\beta}$ in Wahrheit \textit{spärlich} besetzt ist, d.h. für die meisten der Kovariablen $\beta_j = 0$ gilt. Die Herausforderung besteht darin, durch die Regression sowohl die korrekten $\beta_j$ zu selektieren als auch deren Einfluss auf die Responsevariable möglichst genau zu schätzen. Wie in Kapitel \ref{Kap_Multikollinearität und $p > n$} dargestellt, hängt $\mathbb{V}\text{ar}(\hat{\beta}_j)$ einer Kovariable $j$ von der Kovarianzmatrix $\mathbf{X}'\mathbf{X}$ ab, weswegen es bei hoher Multikollinearität zu einer hohen Varianz und damit trotz der Unverzerrtheit des KQ-Schätzers zu einem hohen $\text{MSE}(\hat{\beta}_j)$ kommt. Dadurch sind die Schätzer häufig inhaltlich nicht mehr interpretierbar. Zudem kann die KQ-Methode bei exakter Multikollinearität bzw. $p > n$ keine (eindeutigen) Schätzer liefern.\\
Penalisierungsansätze verfolgen die Strategie die Unverzerrtheit des KQ-Schätzer zu "`opfern"', d.h. absichtlich ein Bias zu erzeugen, um einerseits eine Variablenselektion für die spärliche Besetzung von $\boldsymbol{\hat{\beta}}$ zu ermöglichen und gleichzeitig den $\text{MSE}(\boldsymbol{\hat{\beta}})$ zu senken. Dazu wird die zu minimierende Residuenquadratsumme $\text{RSS}(\boldsymbol{\beta})$ um einen Strafterm $P(\boldsymbol{\beta})$ erweitert, sodass sich eine zu minimierende \textit{penalisierte Residuenquadratsumme} (englisch: penalized residual sum of squares; \textit{PRSS}) als  \begin{align}
\text{PRSS}(\boldsymbol{\beta};\boldsymbol{\gamma})=\text{RSS}(\boldsymbol{\beta}) + P(\boldsymbol{\beta}; \boldsymbol{\gamma})
\end{align} mit einem oder mehreren speziell zu wählenden \textit{Tuningparametern} $\boldsymbol{\gamma}$ ergibt.
\begin{comment}
Unverzerrtheit des $\hat{\beta}$ der Kleinste-Quadratmethode wird geopfert, um den \textit{mittleren quadratischen Fehler} (englisch: mean squared error; \textit{MSE}) zu senken\\
Ridge wird eingeführt, weil Teil von Elastic Net\\
kein Intercept (bei allen Penalisierungen?); durch Standardisierung bzw. Zentralisierung fällt $\beta_0$ weg \cite{montgomery2012introduction}\\
warum keine Subsetselection\\
hat penalisierung, auch die netzwerkbasierte, was baysianisches?\\
!!\\
LAGARGIAN MULTIPLIER?????\\
penalized bestraft größere Beta mehr - intercept raus, weil sonst bestrafung nicht unabhängig von der skala des outcomes wäre!!! (für allgemeine einführung: http://web.as.uky.edu/statistics/users/pbreheny/764-F11/notes/8-30.pdf)
\end{comment}
\subsection{Ridge-Regression}\label{Kap_Ridge-Regression}
Die \textit{Ridge-Regression} ist eine Penalisierungsmethode, bei der die geschätzten Regressionskoeffizienten in $\boldsymbol{\hat{\beta}}^{ridge}$ \textit{realtiv} im Verhältnis zu den Schätzern in $\boldsymbol{\hat{\beta}}^{OLS}$ \textit{geschrumpft} werden. Dadurch verringert sich die Varianz der Schätzer und für den Fall, dass $\mathbf{X}$ keinen vollen Rang besitzt, wird die Matrix $\mathbf{X}'\mathbf{X}$ invertierbar, da die Diagonalmatrix $\gamma \mathbf{I}$ zuvor addiert wird \cite{montgomery2012introduction}. Allerdings findet in der Regel \textit{keine} Variablenselektion statt, sodass die Annahme, dass $\boldsymbol{\beta}$ spärlich besetzt ist, sich nicht in $\boldsymbol{\hat{\beta}}^{ridge}$ widerspiegelt \cite{tibshirani96regression}. Für die Ridge-Regression wird $\arg\min\limits_{\boldsymbol{\beta}} \sum_{i=1}^{n} (y_i - \mathbf{x}_i^T \boldsymbol{\beta})^2$ um die $L_2$-Norm-Nebenbedingung $\sum_{j=1}^{p}\beta_j^2 \le t$ mit $t > 0$ ergänzt. In diesem Zusammenhang wird $t$ als \textit{Restriktionsparameter} bezeichnet. Zusätzlich muss die abhängige Variable $\mathbf{y}$ zentriert, d.h. es muss $\frac{1}{n}\sum_{i}^{n}y_i =0$ gelten, und die Kovariablen $\boldsymbol{X}$ müssen standardisiert sein, sodass $\frac{1}{n}\sum_{i=1}^{n}x_{ij}=0$ und $\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2=1$ für $j=1,\dots,p$ gilt. Dadurch ist die Penalisierung unabhängig von den einzelnen Skalen der Kovariablen bzw. der Responsevariable.\\
Der Restriktionsparameter $t$ hängt invers mit einem \textit{Tuningparameter} $\gamma$ zusammen, wodurch sich die Schätzung von $\boldsymbol{\hat{\beta}}^{ridge}$ als Minimierungsproblem in der \textit{Lagrange-Form} (siehe \ref{Lagrange-Multiplikator})
\begin{align}\label{Ridge_lagrangian}
\boldsymbol{\beta}^{ridge}(\gamma)=\arg \displaystyle\min_{\beta} \left\lbrace \left(\sum_{i=1}^{n} y_i - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2  + \gamma \sum_{j=1}^{p}\beta_j^2 \right\rbrace \qquad \text{mit} \ \gamma \ge 0.
\end{align}
darstellen lässt \cite{hastie2009elements}.
Wird $t \ge \sum_{j=1}^{p} (\hat{\beta}_j^{OLS})^2$ gesetzt, entsprechen die Ridge-Schätzer den OLS-Schätzern, da in diesem Fall $\gamma=0$ ist und \eqref{Ridge_lagrangian} sich zum einfachen KQ-Minimierungsproblem reduziert \cite{tibshirani96regression}.\\
Der bzgl. $\boldsymbol{\beta}$ zu minimierende Term aus \eqref{Ridge_lagrangian} lässt sich in Abhängigkeit von $\gamma$ durch
\begin{align}\label{PRSS_Ridge}
\text{PRSS}(\boldsymbol{\beta}, \gamma)&=(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{y}-\mathbf{X}\boldsymbol{\beta})+\boldsymbol{\beta}'\gamma\boldsymbol{\beta} \notag \\
&=\mathbf{y}'\mathbf{y} -2\mathbf{y}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\gamma\boldsymbol{\beta}
\end{align}
darstellen. Differenzierung nach $\boldsymbol{\beta}$ ergibt
\begin{align}\label{Ableitung_Ridge}
\frac{\partial \text{PRSS}(\boldsymbol{\beta}, \gamma)}{\partial  \boldsymbol{\beta}} = -2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + 2\gamma \boldsymbol{\beta}
\end{align}
und anschließendes Nullsetzen liefert den Schätzer $\boldsymbol{\beta}^{ridge}$ \cite{montgomery2012introduction}
\begin{align}\label{Ridge_estimate}
&& 0&=-2\mathbf{X}'\mathbf{y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + 2\gamma \boldsymbol{\beta} \notag \\
&\Leftrightarrow & \mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma \boldsymbol{\beta} &= \mathbf{X}'\mathbf{y} \notag \\
&\Leftrightarrow & (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I}) \boldsymbol{\beta} &= \mathbf{X}'\mathbf{y} \notag\\
&\Rightarrow & \boldsymbol{\hat{\beta}}^{ridge} &= (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{y}.
\end{align}
Der Ridge-Schätzer aus \eqref{Ridge_estimate} kann auch als Transformation des OLS-Schätzers dargestellt werden \cite{hoerl_ridge_1970}
\begin{align}\label{ridge_transformation}
\boldsymbol{\hat{\beta}}^{ridge} =&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{y} \notag\\
\stackrel{\eqref{y_Schaetzung}}{=}&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1} \mathbf{X}'\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y} \notag\\
=&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1} \mathbf{X}'\mathbf{X} \boldsymbol{\hat{\beta}}^{OLS} \notag\\
=&(\mathbf{I} + \gamma (\mathbf{X}^T\mathbf{X})^{-1})^{-1}  \boldsymbol{\hat{\beta}}^{OLS}.
\end{align}
Daraus ist ersichtlich, dass $\boldsymbol{\beta}^{ridge} \ne \boldsymbol{\hat{\beta}}^{OLS}$ für $\gamma \ne 0$ gilt und in diesem Fall $\boldsymbol{\beta}^{ridge}$ nicht erwartungstreu ist \cite{seber2003linear}, weil der OLS-Schätzer erwartungstreu ist.\\ 
Es lässt sich zeigen, dass für den $\text{MSE}$ für $\boldsymbol{\beta}^{ridge}$ 
\begin{align}\label{MSE_ridge}
\text{MSE}(\boldsymbol{\beta}^{ridge})=\sigma^2 \sum_{j=1}^{p} \frac{\lambda_j}{(\lambda_j + \gamma)^2} + \gamma^2 \boldsymbol{\beta} (\mathbf{X}'\mathbf{X}+ \gamma \mathbf{I})^{-2}\boldsymbol{\beta}
\end{align}
gilt (vgl. \citeA{hoerl_ridge_1970}), wobei $\lambda_j$ für $j=1,\dots,p$ die Eigenwerte von $\mathbf{X}'\mathbf{X}$ sind. Der linke Summand von \eqref{MSE_ridge} ist die Varianz von $\boldsymbol{\hat{\beta}}^{ridge}$ und der rechte Summand der durch $\gamma$ erzeugte Bias, woran erkennbar ist, dass mit größer werdendem $\gamma$ die Varianz des Schätzers sinkt, die Verzerrung hingegen ansteigt. Ziel ist es, ein optimales $\gamma$ zu finden, dass den $\text{MSE}(\boldsymbol{\beta}^{ridge})$ minimiert. \citeA{hoerl_ridge_1970} zeigen, dass es immer ein $\gamma$ gibt, für das $\text{MSE}(\boldsymbol{\hat{\beta}}^{ridge}) < \text{MSE}(\boldsymbol{\hat{\beta}}^{OLS})$ gilt. Die Wahl des Tuningparameters muss dabei vom Anwender getroffen werden und kann bspw. mittels eines Kreuzvalidierungsverfahrens bestimmt werden. Eine Übersicht über verschiedene Methoden bzw. weiterführende Literatur zur Wahl von $\gamma$ findet sich u.a. bei \citeA{montgomery2012introduction}.\\

\subsection{Lasso-Regression}\label{Kap_Lasso-Regression}

%beta_0 fällt weg?

<<echo=FALSE, Ridge_vs_Lasso, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.49\\linewidth', fig.lp="fig:", fig.cap="Geometrische Veranschaulichung des Ridge-Strafterms ($L_2$-Norm, Einheitskreis $ \\beta_1^2 + \\beta_2^2 = t$ ) und des Lasso-Strafterms ($L_1$-Norm) hinsichtlich des Schrupmfungseffekts und der Variablenselektion.">>=
#### R-Code Lasso vs Ridge Ellipsen

library(ellipse)
#
# Plot an ellipse corresponding to a 95% probability region for a 
# bivariate normal distribution with mean scale, variances scale and 
# correlation 0.8. 
#
x <- y <- seq(-12,12,.1)
covmat <- matrix(c(1,0.5,0.5,1),nrow=2,ncol=2)



### RIDGE

plot(ellipse(x=covmat,scale=c(2.26,2.26),centre=c(4,9.35),level=0.95),
     type = 'l',xlim=c(-5,10),ylim=c(-5,10),xlab=expression(beta[1]),
          ylab=expression(beta[2]),xaxt="n",yaxt="n", cex.lab=1, asp=1)
abline(a=0,b=0,col="grey")
abline(v=0,col="grey")
lines(ellipse(x=covmat,scale=c(1.15,1.15),centre=c(4,9.35),level=0.95))
lines(ellipse(x=covmat,scale=c(.3,.3),centre=c(4,9.35),level=0.95))
points(4,9.35, pch=20, col="red", cex=1.7)
text(5.15,8.6, expression(hat(beta)^{OLS}), cex=1.4)

axis(1,at=0)
axis(2,at=0)
covmatc <- matrix(c(1,0,0,1),nrow=2,ncol=2)
lines(ellipse(x=covmatc,scale=c(2,2),level=0.85))
points(0.68,3.85, col="cyan2", pch=20, cex=1.7)
text(1.4,4.8, expression(hat(beta)^{ridge}), cex=1.4)

### LASSO
plot(ellipse(x=covmat,scale=c(1.88,1.88),centre=c(4,9.35),level=0.95, lwd=3),
     type = 'l',xlim=c(-5,10),ylim=c(-5,10),xlab=expression(beta[1]),
     ylab=expression(beta[2]),xaxt="n",yaxt="n", cex.lab=1, asp=1) 
abline(a=0,b=0,col="grey")
abline(v=0,col="grey")
lines(ellipse(x=covmat,scale=c(1.15,1.15),centre=c(4,9.35),level=0.95))
lines(ellipse(x=covmat,scale=c(.3,.3),centre=c(4,9.35),level=0.95))
points(4,9.35, pch=20, col="red", cex=1.7)
text(5.15,8.6, expression(hat(beta)^{OLS}), cex=1.4)

axis(1,at=0)
axis(2,at=0)
edge <- 5.4
lines(x=c(-edge,0),y=c(0,edge), lwd=1)
lines(x=c(-edge,0),y=c(0,-edge), lwd=1)
lines(x=c(0,edge),y=c(-edge,0), lwd=1)
lines(x=c(0,edge),y=c(edge,0), lwd=1)

points(0,5.4, col="cyan2", pch=20, cex=1.7)
text(1.2,6, expression(hat(beta)^{Lasso}), cex=1.4)
@





\bibliography{Masterarbeit}


\begin{appendix}

\chapter{Erläuterungen}
\section{Lagrange-Multiplikator}\label{Lagrange-Multiplikator}


% Die eidesstattliche Erklärung auf einer neuen Seite
\newpage
% Keine Nummerierung für diesen Teil
\section*{Eigenständigkeitserklärung}
% Keine Kopf- und Fußzeilen ausgeben
\thispagestyle{empty}
% Aber trotzdem ins Inhaltsverzeichnis aufnehmen
\addcontentsline{toc}{chapter}{Eigenständigkeitserklärung}
% Hier der offizielle Text der eidesstattlichen Erklärung
Hiermit versichere ich, dass ich die vorliegende Arbeit "`XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXX"' selbstständig und ohne Benutzung anderer als der angegebenen 
Hilfsmittel angefertigt habe; die aus fremden Quellen direkt oder indirekt übernommenen Gedanken sind als solche kenntlich gemacht. 
Die Arbeit wurde bisher in gleicher oder ähnlicher Form keiner anderen Prüfungskommission vorgelegt und auch nicht veröffentlicht.
% Etwas Abstand für die Unterschrift
\vspace{3cm}
% Hier kommt die Unterschrift drüber
\begin{tabbing}
\hspace{6cm}  \= \kill
\textit{Bremen, \today} \> \textit{Moritz Hanke}
\end{tabbing}

\end{appendix}

\end{document}


