\documentclass[12pt, a4paper]{report}
\usepackage{ucs}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage[ngerman]{babel}
%\usepackage[pdftex]{graphicx}
\usepackage{a4wide}
\usepackage[onehalfspacing]{setspace}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[marginal]{footmisc}
\usepackage{url}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{longtable}
\usepackage{comment} 

\usepackage{rotating}


%für die Kopfzeile
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\nouppercase{\leftmark}}
\fancyhead[R]{\thepage} 

%für römische Zahlen
\newcommand{\RM}[1]{\MakeUppercase{\romannumeral #1}}

%für geschwungene Variablenbuchstaben
\usepackage{mathrsfs}

\usepackage{multirow}
\usepackage{tabularx}

%für Pseudo-Algoirthmen-Code
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}


%für Mathe
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{lemma}{Lemma}
\newtheorem{satz}{Satz}

\usepackage{apager}

\allowdisplaybreaks

\usepackage{float}
\restylefloat{figure}


% Für Lit.-Verzeichnis folgenden Befehl im Terminal ausführen: bibtex Masterarbeit_Moritz_Hanke
\bibliographystyle{apager_dgps}

% Fürs Abkürzungsverzeichnis folgenden Befehl im Terminal ausführen: makeindex Masterarbeit_Moritz_Hanke.nlo -s nomencl.ist -o Masterarbeit_Moritz_Hanke.nls
\usepackage{nomencl}
\let\abk\nomenclature
\renewcommand{\nomname}{Liste mathematischer Symbole}
\setlength{\nomlabelwidth}{.20\hsize}
\renewcommand{\nomlabel}[1]{#1 \dotfill}
\setlength{\nomitemsep}{-\parsep}
\makenomenclature 

%\usepackage{Sweave}



\begin{document}

% Titelblatt
\thispagestyle{empty}

\begin{center}
\textbf{\LARGE{Der Einfluss von Zentralitätsmaßen bei der Netzwerk-basierten penalisierten Regression\\}} 
\end{center}

\begin{center}
\textbf{\Large{\\Masterarbeit}}
\end{center}
\begin{verbatim}

\end{verbatim}


\begin{figure}[htbp]
 \begin{minipage}{0.4\linewidth}
  \centering
  \setkeys{Gin}{width=0.4\textwidth}
  \includegraphics{BIPS_Logo_deutsch.png}
 \end{minipage}%
 \begin{minipage}{0.5\linewidth}
  \centering
  \setkeys{Gin}{width=0.99\textwidth}
  \includegraphics{Universitaet_Bremen.png}
 \end{minipage}
\end{figure} 


\begin{comment}
\begin{subfigure}[htbp]
\begin{center}
\setkeys{Gin}{width=0.2\textwidth}
\includegraphics{BIPS_Logo_deutsch.png}
\end{center}
\end{subfigure}


\begin{subfigure}[htbp]
\begin{center}
\setkeys{Gin}{width=0.5\textwidth}
\includegraphics{Universitaet_Bremen.png}
\end{center}
\end{subfigure}
\end{comment}
\begin{center}
\textbf{Fachbereich 3: Mathematik \\
Studiengang: Medical Biometry/Biostatistics (M.Sc.)\\}
\end{center}
\begin{verbatim}

\end{verbatim}

\begin{flushleft}
\begin{tabular}{lll}
& & \\
& & \\
\textbf{Eingereicht von:} & & Hanke, Moritz \\
%\textbf{Geboren am:} & & 07.08.1985 \\
\textbf{Matrikelnummer:} & & 2404575 \\
& & \\
& & \\
\textbf{Betreuung:} & & Prof. Dr. Iris Pigeot-Kübler\\
& & Dr. Ronja Foraita \\
& & \\
& & \\
\textbf{Eingereicht am:} & & \today\\
& & \\
& & \\
\end{tabular}
\end{flushleft}





%Verzeichnisse
\pagenumbering{Roman}
\tableofcontents

\listoffigures

\listoftables

\printnomenclature 
\begin{comment}
\abk{Lasso}{Least absolute shrinkage and selection operator}
\abk{PRSS}{Penalized residual sum of squares}
\abk{KEGG}{Kyoto Encyclopedia of Genes and Genomes}
\abk{Enet}{Elastic Net}
\abk{nEnet}{naives Elastic Net}
\abk{s.d.}{so dass}
\abk{Grace}{graph constrained regression}
\abk{nGrace}{naive graph constrained regression}
\abk{aGrace}{adaptive graph constrained regression}
\abk{fLasso}{fused Least absolute shrinkage and selection operator}
\end{comment}
\abk{$\xrightarrow{d}$}{konvergiert nach Verteilung}
\abk{$\rightarrow$}{geht gegen}
\abk{$p$}{Anzahl Kovariablen}
\abk{$n$}{Anzahl Beobachtungen}
\abk{$\mathbf{Y}$}{Spaltenvektor der Responsevariable der Länge $n$}
\abk{$\mathbf{X}$}{Designmatrix}
\abk{$\boldsymbol{\beta}$}{Regressionskoeffizientenvektor der Länge $p$}
\abk{$\varepsilon$}{Fehlerterm des klassischen linearen Modells}
\abk{$G$}{ein Graph}
\abk{$V$}{Menge Knoten in $G$}
\abk{$E$}{Menge Kanten in $G$}
\abk{$N_V$}{Anzahl an Knoten in $G$}
\abk{$E_V$}{Anzahl an Knoten in $G$}
\abk{$e$}{Kante, für die $e \in E$ gilt}
\abk{$G_A$}{Teilgraph von $G$}
\abk{$d(u)$}{Knotengrad des Knotens $u$}
\abk{$\Delta(G)$}{Maximaler Knotengrad von $G$}
\abk{$\delta(G)$}{Minimaler Knotengrad von $G$}
\abk{$\bar{d}(G)$}{Durchschnittlicher Knotengrad von $G$}
\abk{$\mathbf{A}$}{Adjazenzmatrix von $G$}
\abk{$a$}{Eintrag in $\mathbf{A}$}
\abk{$\mathbf{B}$}{Inzidenzmatrix von $G$}
\abk{$b$}{Eintrag in $\mathbf{B}$}
\abk{$\textbf{\~B}$}{Adjazenzmatrix von einem gerichteten Graphen $G$}
\abk{$\tilde{b}$}{Eintrag in $\textbf{\~B}$}
\abk{$\mathbf{L}$}{Laplace-Matrix von $G$}
\abk{$\textbf{\~L}$}{normierte Laplace-Matrix von $G$}
\abk{$\textbf{\~L}^*$}{modifizierte normierte Laplace-Matrix von $G$}
\abk{$\lambda_j$}{$j$-te Eigenwert zum Eigenvektor $\mathbf{e}_j$}
\abk{$\mathcal{W}$}{Weg auf $G$}
\abk{$\mathcal{S}$}{Spur auf $G$}
\abk{$\mathcal{P}$}{Pfad auf $G$}
\abk{$\mathcal{D}$}{Distanz auf $G$}
\abk{$\phi(G)$}{Durchmesser von $G$}
\abk{$\kappa(G)$}{$\kappa$-Verbundenheit von $G$}
\abk{$\mathscr{B}(u)$}{Betweenness-Zentralität des Knotens $u$}
\abk{$\mathscr{C}(u)$}{Closeness-Zentralität des Knotens $u$}
\abk{$\mathscr{E}(u)$}{Eigenvektor-Zentralität des Knotens $u$}
\abk{$\mathbf{e}_j$}{Eigenvektor zu Eigenwert $\lambda_j$}
\abk{$\sim$}{verteilt wie}
\abk{s.d.}{so dass (Nebenbedingung)}
\abk{$\sigma^2$}{Varianz}
\abk{DAG}{Directed acyclic graph}
\abk{MSE}{Mean squared error}
\abk{RSS}{Residual sum of squares}
\abk{$\mathbb{E}(\cdot)$}{Erwartungswert}
\abk{$\mathbb{C}\text{ov}(\cdot)$}{Kovarianz}
\abk{$\mathbb{V}\text{ar}(\cdot)$}{Varianz}
\abk{$(\cdot)^{-1}$}{Inverse einer Matrix}
\abk{$(\cdot)'$}{Transponierte einer Matrix}
\abk{$\boldsymbol{\hat{\beta}}$}{Schätzung von $\boldsymbol{\beta}$}
\abk{$R^2$}{(multipler) Determinationskoeffizient}
\abk{$P(\cdot)$}{Strafterm für eine Zielfunktion}
\abk{$\boldsymbol{\gamma}$}{Tuningparametervektor}
\abk{$\mathbf{I}$}{Einheitsmatrix}
\abk{$t$}{Iterationsschritt $t$ oder Restriktionsparamter einer Nebenbedingung}
\abk{$\mathbb{R}^p$}{Euklidischer Vektorraum mit Dimension $p$}
\abk{$||\cdot||_p$}{$p$-Norm eines Vektors}
\abk{$D(\cdot)$}{Differenz zwischen zwei Regressionskoeffizientenschätzern}
\abk{$\text{sign}(\cdot)$}{Vorzeichen der Einträge eines Vektors}
\abk{$|\cdot|$}{Betrag von den Einträgen eines Vektors}
\abk{$\rho_{k,l}$}{Korrelation zwischen zwei Variablen $k$ und $l$}
\abk{$I(\cdot)$}{Indikatorfunktion}
\abk{$\tau$}{Tuningparameter}
\abk{$c_u$}{Gewichtung eines Knotens}
\abk{ME}{Modellfehler}
\abk{RP}{Richtig-positiv}
\abk{FP}{Falsch-positiv}
\abk{PE}{Vorhersagefehler}
\abk{$N(\mu,\sigma^2)$}{normalverteilt mit Erwartungswert $\mu$ und Varianz $\sigma^2$}
\abk{$\alpha$}{Verhältnisparameter zwischen Ridge- und Lasso-Strafterm bei der Enet-Regression}
\abk{$||\mathbf{x}||_{\infty}$}{definiert als $\max (|x_1|,\dots,|x_p|)$}
\abk{$\mathcal{I}$}{Spaltenvektor, der an den Stelle $j$ für $\hat{\beta}_j \neq 0$ eine eins und sonst eine null hat}
\abk{$\overset{a}{\sim}$}{approximativ verteilt wie}
\abk{$\pi$}{Anteil an Simulationen mit numerisch stabilen Schätzern}



% Begin der Arbeit
\chapter{Einleitung}
\pagenumbering{arabic}
Im Zusammenhang mit linearen Regressionen bei \textit{hochdimensionalen Daten} stellt sich häufig das so genannte \textit{"`large p, small n"'}-Problem. Dabei handelt es sich um den Umstand, dass ein Datensatz weniger \textit{Beobachtungen n} als \textit{erklärende Variablen p} beinhaltet. Speziell in der Genetik gilt oftmals sogar $p \gg n$, d.h. es werden beispielsweise die Expressionslevel von mehreren Tausend Genen bei wenigen Hundert Probanden gemessen. Für das klassische lineare Regressionsmodell $\mathbf{Y}=\textbf{X}\boldsymbol\beta+\boldsymbol\varepsilon$ liefert in diesem Fall die Schätzung $\boldsymbol{\hat{\beta}}$ mittels der \textit{Methode der kleinsten Quadrate }(englisch: ordinary least squares; \textit{OLS}) keine eindeutige Lösung und das Modell wird an die Daten überangepasst \cite{hastie_efficient_2004}. Deshalb wird gefordert, dass $\boldsymbol{\beta}$ \textit{spärlich} besetzt ist, d.h. für die meisten Regressionskoeffizienten $\beta_i=0$ gilt \cite{buehlmann2011statistics}. Hierbei handelt es sich speziell in der Genetik um eine realistische Annahme, da in der Regel nur wenige Gene für ein spezifische Responsevariablenausprägung verantwortlich sind. Die Schwierigkeit besteht darin, den Einfluss der wenigen aktiven Kovariablen zu schätzen und gleichzeitig eine Variablenselektion vorzunehmen, sodass Kovariablen ohne Einfluss aus dem Modell entfernt werden.\\
Ein verbreiteter Ansatz ist in diesem Zusammenhang die Einführung einer \textit{Penalisierungen} bei der Schätzung der Regressionskoeffizienten, was eine Schrumpfung und Selektion der Regressionskoeffizienten zur Folge hat. Dazu wird das Kleinste-Quadrat-Kriterium $\arg\min\limits_{\boldsymbol{\beta}} \sum_{i=1}^{n}(Y_i - \mathbf{x}_i' \boldsymbol{\beta})^2$ des OLS \cite{fahrmeir2009regression} um einen \textit{Strafterm} $P(\boldsymbol{\beta})$ erweitert, wodurch einzelne Regressionkoeffizienten $\beta_j$ als $\hat{\beta}_j=0$ geschätzt werden können. Die Folge ist eine eindeutige Lösung und der Vektor $\boldsymbol{\hat{\beta}}$ ist wie für $\boldsymbol{\beta}$ angenommen spärlich besetzt.\\ 
Für diese Strategie ist der \textit{Least absolute shrinkage and selection operator} (\textit{Lasso}) von \citeA{tibshirani96regression} eine der bekanntesten Methoden und wurde in den letzten Jahren im Hinblick auf die Einbeziehung von vorab bekannten Informationen im Strafterm erweitert. \citeA{li_network-constrained_2008} haben dazu einen Strafterm entwickelt, der die Netzwerkstruktur der erklärenden Variablen untereinander berücksichtigt. Dieser Ansatz ist besonders in der Genetik vielversprechend, da durch Datenbanken wie der \textit{Kyoto Encyclopedia of Genes and Genomes} (\textit{KEGG}; \citeNP{Kanehisa2000KEGG}) relativ umfangreiche Informationen über Genregulationsnetzwerke (sogenannte \textit{Pathways}) und damit über die Abhängigkeit von einzelnen Genen untereinander vorliegen.\\
In der vorliegenden Arbeit wird eine Weiterentwicklung der netzwerkbasierten Penalisierung durch \citeA{kim_network-based_2013} um verschiedene \textit{Zentralitätsmaße} bei der Schätzung der Parameter $\boldsymbol{\beta}$ vorgestellt. Die Ergebnisse werden untereinander sowie mit den "`klassischen"' Penalisierungsansätzen Lasso und \textit{Elastic net} \cite{zou_regularization_2005} verglichen. Ziel ist es dabei, den Einfluss der zusätzlichen Information über die erklärenden Variablen, die ein Netzwerk bilden, auf die Güte der Schätzung darzustellen und darauf basierende Empfehlungen für die Anwendung von netzwerkbasierten Penalisierungen abzugeben.






\chapter{Theorie}\label{Kap_Theorie}
\section{Graphen und Netzwerkstatistiken}\label{Kap_Graphen und Netzwerkstatistiken}
%
%
% Netzwerke <--> Graphen
%
%
In den nachfolgenden beiden Abschnitten wird eine Auswahl an grundlegenden Begriffen aus der Graphentheorie (Kapitel \ref{Kap_Grundlegende Begriffe der Graphentheorie}) sowie Statistiken für Netzwerkanalysen, insbesondere zur Zentralität und Verbundenheit von einzelnen Knoten (Kapitel \ref{Kap_Zentralitätsstatistiken}), eingeführt, die für diese Arbeit benötigt werden. Für eine ausführliche Erläuterung zur Graphentheorie bzw. zu Netzwerkstatistiken sei auf \citeA{diestel2006graph} sowie \citeA{kolaczyk2009statistical} und \citeA{newman2010networks} verwiesen.\\

\subsection{Grundlegende Begriffe der Graphentheorie}\label{Kap_Grundlegende Begriffe der Graphentheorie}
Ein Graph ist ein Paar $G=(V,E)$, das aus einer Menge $V=\{v_1,\dots,v_{N_V}\}$ an \textit{Knoten} und einer Menge $E=\{e_1,\dots,e_{N_E}\}$ an \textit{Kanten} besteht \cite{brandes2005graphfunda}. Die Kardinalitäten $N_V =|V|$ und $N_E=|E|$ geben die \textit{Ordnung} und die \textit{Größe} des Graphen an. Eine ungerichtete Kante $e \in E$ ist die Zuordnung zweier Knoten $u,v \in V$ mit $u \neq v$ eines Graphen $G=(V,E)$, d.h. es gilt $\{u,v\} = e$. Die Knoten $u$ und $v$ werden in diesem Fall als \textit{inzident} zu der Kante $e$ und \textit{adjazent} zueinander bezeichnet. Für $e=\{u,v\}$ handelt es sich um ein \textit{ungeordnetes} Knotenpaar, d.h. $\{u,v\}=\{(u,v),(v,u)\}$. Graphen, die nur ungeordnete Knotenpaare beinhalten, sind \textit{ungerichtet}. Dagegen bezeichnet $e=(u,v) \neq (v,u)$, dass es sich um ein \textit{geordnetes} Paar an Knoten handelt, wobei die Kante $e$ vom \textit{Anfangsknoten} $u$ auf den \textit{Endknoten} $v$ zeigt \cite{kolaczyk2009statistical}. Als \textit{gerichtete} Graphen werden jene Graphen bezeichnet, die nur geordnete Knotenpaare beinhalten. Der Graph $G_A=(V_A,E_A)$ ist ein \textit{Teilgraph} von $G=(V,E)$, wenn $V_A \subseteq V$ und $E_A \subseteq E$ gilt. Die in dieser Arbeit verwendeten Graphen haben maximal eine Kante zwischen zwei Knoten, wobei es sich dabei auch um eine \textit{Schlinge} handeln kann. Als Schlinge wird eine Kante bezeichnet, die nur einen Endknoten an beiden Kantenenden besitzt, d.h. $e=\{v,v\}$ bzw. $e=(v,v)$ \cite{tittmann2011graphen}. Darüber hinaus werden nur \textit{endliche} Graphen ($N_V < \infty$) berücksichtigt, die entweder gerichtet oder ungerichtet sind.\\

Der Knotengrad $d(v)$ für $v \in V$ gibt die Anzahl der Kanten an, die inzident auf den Knoten $v$ sind. Für einen Graphen $G=(V,E)$ sind der \textit{Minimumknotengrad} $\delta(G)$, der \textit{Maximumknotengrad} $\Delta(G)$ und der \textit{durchschnittliche Knotengrad} $\bar{d}(G)$ definiert \cite{diestel2006graph} als:
\begin{align}
\delta(G)=\min\{d(v) \ | \ v \in V\}\label{Eq_Minimumknotengrad}
\end{align}
\begin{align}
\Delta(G)=\max\{d(v) \ | \ v \in V\}\label{Eq_Maximumknotengrad}
\end{align}
\begin{align}
\bar{d}(G)=\frac{1}{N_V}\sum_{v \in V}d(v).\label{Eq_durchschnittlicher Knotengrad}
\end{align}
Handelt es sich um einen gerichteten Graphen, kann für jeden Knoten $v \in V$ mittels $d^{in}(v)$ die Anzahl auf $v$ und mit $d^{out}(v)$ die Anzahl von $v$ gerichteten Kanten angegeben werden. Entsprechend können die Maße aus \eqref{Eq_Minimumknotengrad}, \eqref{Eq_Maximumknotengrad} und \eqref{Eq_durchschnittlicher Knotengrad} angepasst werden, indem nur $d^{in}(v)$ bzw. $d^{out}(v)$ berücksichtigt werden.\\

Die \textit{Adjazenzmatrix} $\textbf{A}=(a_{u,v})_{N_V \times N_V}$ kennzeichnet die Konnektivität, d.h. ob es eine Kante zwischen zwei Knoten gibt. Dafür sei 
\begin{align}\label{Eq_Adjazenzmatrix}
a_{u,v} = \begin{cases}
1, \qquad &\text{wenn} \ (u,v) \in E \ \textit{für} \ u \neq v \ ,\\
0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Handelt es sich bei $G=(V,E)$ um einen ungerichteten Graphen ist $\textbf{A}$ eine symmetrische Matrix \cite{kolaczyk2009statistical}.\\
Die \textit{Inzidenzmatrix} $\textbf{B}=(b_{v,e})_{N_V \times N_E}$ eines \textit{ungerichteten} Graphen $G=(V,E)$ gibt an, ob $e \in E$ inzident zu $v \in V$ ist \cite{kolaczyk2009statistical} und es sei
%nicht gut, dass da schon wieder "sei" ist
\begin{align}\label{Eq_Inzidenzmatrix_unger}
b_{v,e} = \begin{cases}
1, \qquad &\text{wenn} \ v \in e \ ,\\
0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Für einen \textit{gerichteten} Graphen $G=(V,E)$ kann unterschieden werden, ob $v \in e$ der Anfangs- oder Endknoten für einen beliebigen Knoten $u \in V$ ist. In diesem Fall ist die Inzidenzmatrix $\textbf{\~B}=(\tilde{b}_{v,e})_{N_V \times N_E}$ definiert \cite{brandes2005graphfunda} als
\begin{align}\label{Eq_Inzidenzmatrix_ger}
\tilde{b}_{v,e} = \begin{cases}
-1, \qquad &\text{wenn} \ (v,u) = e \  ,\\
\ 1, \qquad &\text{wenn} \ (u,v) = e \ ,\\
\ 0, \qquad &\text{sonst.}\\
\end{cases}
\end{align}
Sei $\textbf{D}=\text{diag}(d(v), v \in V)$ eine \textit{Diagonalmatrix} und $\textbf{\~B}'$ die transponierte Matrix von $\textbf{\~B}$, dann gilt $\textbf{\~B\~B}'=\textbf{D}-\textbf{A}$. Dabei handelt es sich bei $\textbf{\~B\~B}'$ um eine $N_V \times N_V$ Matrix, die als \textit{Laplace-Matrix} $\textbf{L}$ bezeichnet wird. Deren zweitkleinster \textit{Eigenwert} $\lambda_2$ ist umso größer, je mehr Kanten es zwischen den einzelnen Knoten gibt und kann als ein Maß für die \textit{Verbundenheit} des Graphen interpretiert werden \cite{kolaczyk2009statistical}.\\ 
\begin{comment}
Für die \textit{normierte Laplace-Matrix} $\textbf{\~L}$ gilt $\textbf{\~L}=\textbf{D}^{1/2}\textbf{L}\textbf{D}^{1/2}$, wobei $\textbf{D}^{1/2}=\text{diag}\frac{1}{\sqrt{d(v)}}$ für $d(v) \neq 0$ und ansonsten $0$ ist \cite{brandes2005graphfunda}.
\end{comment}
% WIRD das gebraucht, wenn der Graph partioniert werden soll??? -> Spectrum

Ein \textit{Weg} $\mathcal{W}(v_1,v_m)$ zwischen den Knoten $v_1$ und $v_m$ in einem Graphen $G=(V,E)$ bezeichnet eine Abfolge $\{v_1, e_1, v_2,\dots,e_{m-1},v_m \}$ mit $v_1, \dots, v_m \in V$ und $e_1,\dots,e_{m-1} \in E$, die die beiden Knoten miteinander verbindet und die \textit{Länge} $m-1$ hat \cite{diestel2006graph}. Für ungerichtete Graphen gilt hierbei $e_k=\{v_{k-1}, v_k\}$ und für gerichtete Graphen $e_k=(v_{k-1}, v_k)$. Wird zusätzlich gefordert, dass $e_k \neq e_l$ für $k \neq l$ gilt, d.h. dass keine Kante $e_k$ mehrmals in der Abfolge vorkommt, handelt es sich um eine \textit{Spur} $\mathcal{S}(v_1,v_m)$. Erfüllt ein Weg die Bedingung $v_k \neq v_l$ für $k \neq l$, wird dieser \textit{Pfad} $\mathcal{P}(v_1,v_m)$ genannt \cite{brandes2005graphfunda}. Für $v_1 = v_m$ ist $\mathcal{W}(v_1,v_m)$ ein \textit{Zyklus} und für $\mathcal{P}(v_1,v_m)$ ein \textit{Kreis}. Der kürzeste Pfad zwischen zwei Knoten $v_1,v_m \in V$ heißt \textit{Distanz} $D(v_1,v_m)$ und seine Länge entspricht der Anzahl an zugehörigen Kanten. Mit der \textit{größten} Distanz aus allen Knotenkombinationen eines zusammenhängenden Graphen $G=(V,E)$ wird dessen Durchmesser $\phi(G)$ angegeben \cite{diestel2006graph}. Wege, Pfade, Spuren, Zyklen und Kreise sind gerichtet, wenn der zugrunde liegende Graph gerichtet ist. Dieser wird, wenn er keinen Zyklus beinhaltet, als \textit{gerichteter, azyklischer Graph}  (englisch: directed acyclic graph; \textit{DAG}) bezeichnet \cite{kolaczyk2009statistical}.\\

Ein ungerichteter Graph $G=(V,E)$ ist \textit{zusammenhängend}, wenn jeder Knoten $v \in V$ von jedem anderen Knoten $u \in V$ über einen Weg erreicht werden kann. Entsprechend ist ein Graph \textit{unzusammenhängend}, wenn mindestens ein Knoten nicht von allen übrigen Knoten erreicht werden kann. 
Als \textit{Komponente} eines Graphen wird ein \textit{maximal zusammenhängender} Subgraph $G_A=(V_A,E_A)$ mit $V_A \subseteq V$ und $E_A \subseteq E$ für $G=(V,E)$ bezeichnet, wobei "`maximal"' die Eigenschaft ist, dass es keinen zusammenhängenden Subgraphen  $G_B=(V_B, E_B)$ in $G=(V,E)$ gibt, für den $V_B \supseteq V_A$ und $E_B \supseteq E_A$ gilt. Eine \textit{Clique} ist ein Subgraph, in dem alle Knoten adjazent zueinander sind.
\begin{comment}
Ein maximal zusammenhängender Teilgraph $G_A=(V_A,E_A)$, für den $V_A \subseteq V$ und $E_A \subseteq E$ für $G=(V,E)$ gilt und es keinen zusammenhängenden Subgraphen  gibt, heißt \textit{Komponente}.
\end{comment} 
Ist $G=(V,E)$ gerichtet, so wird dieser als \textit{schwach verbunden} bezeichnet, wenn der zugrunde liegende ungerichtete Graph zusammenhängend ist, und als \textit{stark verbunden}, wenn jeder Knoten von jedem anderen Knoten über einen direkten Weg erreichbar ist \cite{brandes2005graphfunda}. Mit der $k$-\textit{Verbundeheit} $\kappa(G)$ wird angegeben, wie viele Knoten mindestens aus einem zusammenhängenden Graphen $G=(V,E)$ \textit{entfernt} werden müssen, damit dieser unzusammenhängend wird \cite{diestel2006graph}.\\
% Cluster?
% tree?
% Cliquen?

\subsection{Zentralitätsstatistiken}\label{Kap_Zentralitätsstatistiken}
Um die \textit{Bedeutung} eines Knotens $u \in V$ zu bestimmen, kann seine \textit{Zentralität} innerhalb des Graphen $G=(V,E)$ angegeben werden. In der Literatur finden sich diverse Maße, die Zentralität teilweise sehr unterschiedlich definieren und daher bei der Beurteilung, wie zentral ein Knoten ist, zu unterschiedlichen Ergebnissen kommen können (vgl. dazu bspw. \citeNP{koschuetzki2005centralityindices}). Ein Beispiel für die Mehrdeutigkeit des Begriffs Zentralität sind die Knoten $1$ und $8$ im Graphen in Abbildung \ref{fig:Degree_vs_Path}. Wird die Zentralität schlicht als die Anzahl an benachbarten Knoten aufgefasst, hat Knoten $1$ (gelb) eine relativ zentrale Position ($d(1)=6$), Knoten $8$ (rot) hingegen ist nach dieser Definition weniger zentral gelegen ($d(8)=5$). Wird dagegen die Zentralität als die Anzahl an kürzesten Pfaden zwischen zwei Knoten $u_k$ und $u_m$, auf denen ein Knoten $u_l$ liegt, aufgefasst (die sogenannte Betweenness-Zentralität $\mathscr{B}(u)$, s.u.), ist unmittelbar ersichtlich, dass der Knoten $8$ zentraler ($\mathscr{B}(8)=138$) als der Knoten $1$ ($\mathscr{B}(1)=80$) ist.\\
<<echo=FALSE>>=
#             1  2  3  4  5  6 6.5 7  8  9 10 11 12 13 14 15 16 17 18 19 
A <- matrix(c(0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #1 
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #2
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #3
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #4
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #5
              1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #6
              1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #6.5
              0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #7
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,  #8
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,  #9
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,  #10
              0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,  #11
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #12
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  #13
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #14
              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #15
              0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #16
              0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #17
              0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  #18
              0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), ncol=20, byrow=T)

library(igraph)

g <- graph.adjacency(A, "undirected")
V(g)$color <- 5
V(g)[1]$color <- "yellow"
V(g)[8]$color <- "red"
#V(g)[7]$color <- "green"
@
<<echo=FALSE, Degree_vs_Path, fig.pos="H", out.width='.6\\linewidth', fig.align='center', fig.scap="Beispielgraph", fig.lp="fig:", fig.cap="Beispiel für einen Graphen, in dem in Abhängigkeit von der Definition von Zentralität entweder der rote oder der gelbe Knoten die bedeutsamste Position einnimmt.">>=
plot(g, edge.width=3)
@
Neben der bereits erwähnten Betweenness-Zentralität $\mathscr{B}(u)$ sollen nachfolgend die Eigenvektor-Zentralität $\mathscr{E}(u)$ sowie die Closeness-Zentralität $\mathscr{C}(u)$ vorgestellt werden, da es sich hierbei um "`Standard"'-Zentralitätsmaße in der Literatur handelt und diese später bei der Netzwerk-basierten penalisierten Regression als Gewichtung für die zu schätzenden Regressionskoeffizienten verwendet werden (vgl. dazu Kapitel \ref{Kap_Netzwerkbasierte_Penalisierung}).
\paragraph{Betweenness-Zentralität}\label{Kap_Betweenness-Zentralität}
Die Betweenness-Zentralität für einen Knoten $u$ ist definiert als
\begin{align}\label{Eq_Betweenness_Definition}
\mathscr{B}(u)=\sum_{u \neq v \neq w \in V} \frac{\varpi(v,w|u)}{\varpi(v,w)}
\end{align}
wobei $\varpi(v,w|u)$ die Anzahl an Distanzen $D(v,w)$ zwischen den Knoten $v$ und $w$ ist auf denen $u$ liegt, und $\varpi(v,w)$ die Gesamtzahl an Distanzen $D(v,w)$ zwischen $v$ und $w$ für alle $u \in V$, d.h. $\varpi(v,w)=\sum_{u \in V} \varpi(v,w|u)$ \cite{kolaczyk2009statistical}. Nach diesem Maß wird die Bedeutung eines Knoten $u$ darüber interpretiert, wie sehr er "`unausweichlich"' ist, wenn in der kleinstmöglichen Anzahl an Kantenschritten von einem Knoten $v$ zu einem Knoten $w$ gelangt werden soll. Dadurch gilt für alle Knoten mit $d(u) \leq 1$ immer $\mathscr{B}(u)=0$. Da $\mathscr{B}(u)$ abhängig von der Ordnung eines Graphen ist, kann durch Division durch $\frac{(N_V-1)(N_V-2)}{2}$ eine normierte und damit über verschiedene Graphen vergleichbare Betweenness-Zentralität berechnet werden. 

\paragraph{Closeness-Zentralität}\label{Kap_Closeness-Zentralität}
Bei der Closeness-Zentralität werden für einen Knoten $u$ alle Distanzen zu den Knoten $v \in V \setminus u$ summiert und davon der reziproke Wert genommen:
\begin{align}\label{Eq_Closeness_Definition}
\mathscr{C}(u)=\frac{1}{\sum_{v \in V \setminus u} D(u, v)}.
\end{align}
Anhand von Abbildung \ref{fig:Closeness_normalized} wird deutlich, dass $\mathscr{C}(u)$ nicht allein von der Lage von $u$ innerhalb des Graphen, sondern auch von der Ordnung des Graphen abhängt, weswegen $\mathscr{C}(u)$ durch die Multiplikation mit dem Faktor $(N_V -1)$ normiert werden kann, um Knotenzetralitäten über verschiedene Graphen vergleichbar zu machen. Allein für den Fall $d(u)=0$ gilt $\mathscr{C}(u)=0$, ansonsten ist die Closeness-Zentralität strikt größer als null.
<<echo=FALSE, Closeness_normalized, fig.pos="h", fig.show='hold', out.width='.49\\linewidth', fig.align='center', fig.scap="Beispiel für die Normalisierung der Closeness", fig.lp="fig:", fig.cap="Beispiel für den Einfluss der Normalisierung der Closeness. Ohne eine Normalisierung mit dem Faktor $(N_V -1)$ hat der rote Knoten in Graph A eine Closeness-Zentralität von $\\mathscr{C}(1)=0.25$ und der gelbe Knoten in Graph B eine Closeness-Zentralität von $\\mathscr{C}(1)=0.125$. Somit würde in diesem Fall der rote Knoten als doppelt so bedeutsam für den Graphen A als der gelbe Knoten für den Graphen B interpretiert werden. Diese naive Bewertung der beiden Knoten ist für die vorliegenden Fälle allerdings ungeeignet, da offensichtlich beide Knoten in ihrem jeweiligen Graphen so zentral sind, dass alle Wege zwischen den übrigen Knoten immer über sie laufen bzw. die Entfernung des roten und gelben Knotens alle Kanten der Graphen aufheben würde. Nach einer Normalisierung mit $(N_V -1)$ haben beide Knoten hingegen einen Closeness-Zentralitätswert von $\\mathscr{C}(1)=1$ und sind gleich bedeutsam für ihre jeweiligen Graphen.">>=
A1 <- matrix(c(
# 1  2  3  4  5
  0, 1, 1, 1, 1,
  1, 0, 0, 0, 0,
  1, 0, 0, 0, 0,
  1, 0, 0, 0, 0,
  1, 0, 0, 0, 0), ncol=5, byrow=T)
g1 <- graph.adjacency(A1, "undirected")
V(g1)$color <- 5
V(g1)[1]$color <- "red" 

A2 <- matrix(c(
# 1  2  3  4  5  6  7  8  9
  0, 1, 1, 1, 1, 1, 1, 1, 1,
  1, 0, 0, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 0, 0, 0, 0,
  1, 0, 0, 0, 0, 0, 0, 0, 0), ncol=9, byrow=T)
g2 <- graph.adjacency(A2, "undirected")
V(g2)$color <- 5
V(g2)[1]$color <- "yellow"

plot(g1, edge.width=3, main="Graph A")
plot(g2, edge.width=3, main="Graph B")
@

\paragraph{Eigenvektor-Zentralität}\label{Kap_Eigenvektor-Zentralität}
Im Gegensatz zu $d(u)$, $\mathscr{B}(u)$ und $\mathscr{C}(u)$ wird bei $\mathscr{E}(u)$ die Zentralität eines Knotens $u$ durch die Zentralität seiner Nachbarn bestimmt. Deshalb wird in der Literatur dieses ursprünglich in den Sozialwissenschaften entwickelte Maß als Angabe über das "`Prestige"' eines Knotens $u$ interpretiert. Dadurch kann ein Knoten $u$, der auf relativ wenigen Distanzen zwischen den Knoten $v,w \in V \setminus u$ liegt oder einen niedrigen Knotengrad aufweist, gleichzeitig aber zu besonders zentralen Knoten adjazent ist, eine hohe Zentralität aufweisen. 
Über das Eigenwertproblem
\begin{align*}
\mathbf{A}\mathbf{e}=\lambda_{max} \mathbf{e}
\end{align*}
mit dem größten Eigenwert $\lambda_{max}$ von $\mathbf{A}$ und dem zugehörigen Eigenvektor $\mathbf{e}=(e_1,\dots, e_{N_V})'$ kann die Zentralität eines Knotens $u$ als
\begin{align}\label{Eq_EVcent}
&& \lambda_{max} e_u &= \sum_{v \in V\setminus u} e_v a_{u,v} \notag \\
&\Leftrightarrow& e_u &=\frac{1}{\lambda_{max}} \sum_{v \in V\setminus u} e_v a_{u,v}
\end{align}
angegeben werden \cite{bonacich72factoring}. In der Literatur finden sich diverse Erweiterungen dieses Maßes, durch die beispielsweise mit Hilfe eines "`Einflussgebietsparameters"' bestimmt werden kann, in wie weit auch die Zentralität von nicht direkt benachbarten Knoten die Zentralität von $u$ beeinflussen oder ob sich die Zentralität eines Knotens umgekehrt zur Zentralität der umliegenden Knoten verhält (vgl. dazu \citeNP{Bonacich1987EigenvektorCent}). Im Rahmen dieser Arbeit wird neben $\mathscr{B}(u)$ und $\mathscr{C}(u)$ die Definition der Eigenwert-Zentralität aus Gleichung \eqref{Eq_EVcent} verwendet, da es keine begründete Annahme gibt, das Einflussgebiet zu erhöhen bzw. zu verringern.

% gibt es Statistik d(v_i)/\kappa(G)? würde die was taugen?
% statt d(v_i) sowas wie nachbran von nachbarn?


\section{Lineare Regression}\label{Kap_Lineare Regression}
In den drei folgenden Abschnitten werden die Grundlagen und Annahmen des klassischen linearen Regressionsmodells vorgestellt, die für die Methoden der penalisierten Regression in Kapitel \ref{Kap_Penalisierte Regression} Voraussetzung sind. Wenn nicht anders gekennzeichnet, bezieht sich die Darstellung auf \citeA{gross2003linear}, \citeA{fahrmeir2009regression} bzw. \citeA{seber2003linear}, die eine vertiefende und weiterführende Einführung in die lineare Regression geben. Kapitel \ref{Kap_Das klassische lineare Modell} beinhaltet eine kurze Übersicht über die Annahmen des klassischen linearen Modells. Die für die Schätzung der Regressionskoeffizienten am häufigsten verwendete Methode der kleinsten Quadrate wird in Kapitel \ref{Kap_Methode der kleinsten Quadrate} erläutert. Im abschließenden Abschnitt \ref{Kap_Multikollinearität und $p > n$} werden die Probleme Multikollinearität und "`$p > n$"' vorgestellt, die beide in der Genetik auftreten und Penalisierungsansätze notwendig machen.
\begin{comment}
$\mathbf{X}$ sind ZVs\\
$\mathbf{y}$ ist eine kontinuierliche ZVs
\end{comment}
\subsection{Das klassische lineare Modell}\label{Kap_Das klassische lineare Modell}
Im klassischen linearen Regressionsmodell wird eine \textit{lineare} Beziehung zwischen der \\ \textit{Responsevariable} $Y$ und den \textit{Kovariablen} $X_1, \dots, X_p$ angenommen:
\begin{align}\label{Eq_klassisches_modell}
Y=\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \varepsilon.
\end{align}
Dabei wird $Y$ als Zufallsvariable und $x_j$ für $j=1,\dots,p$ entweder als Realisierungen der Zufallsvariablen $X_1,\dots,X_p$ oder als deterministische Werte für $X_1,\dots,X_p$ aufgefasst. Da reale Datensätze, insbesondere in der Genetik, häufig Beobachtungsdaten beinhalten, wird für die folgende Arbeit angenommen, dass es sich bei $x_1,\dots,x_p$ um Realisierungen von Zufallsvariablen handelt. Mit der Zufallsvariable $\varepsilon$ in \eqref{Eq_klassisches_modell} wird ein additiv wirkender Fehler bezeichnet, der unabhängig von den Kovariablen bzw. der Responsevariable auftritt.\\
Ziel bei der linearen Regression ist die Schätzung der unbekannten Parameter $\beta_0, \beta_1, \dots, \beta_p$, die als \textit{Regressionskoeffizienten} den Einfluss der einzelnen Kovariablen auf die Responsevariable angeben. Bei $\beta_0$ handelt es sich um eine \textit{Konstante} (englisch: Intercept), die unabhängig von den Kovariablenausprägungen einen "`Grundwert"' für die Responsevariable angibt. Liegen $n$ beobachtbare $Y$ sowie $p$ Kovariablen vor und seien
\begin{align*}
\mathbf{Y} = (Y_1,\dots,Y_n)'
\qquad \text{und} \qquad
\boldsymbol{\beta} = (\beta_0,\beta_1,\dots,\beta_p
)'
\qquad \text{sowie} \qquad
\boldsymbol{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n
)'
\end{align*}
einspaltige Matrizen sowie
\begin{align*}
\mathbf{X} = \begin{pmatrix}
1 & x_{1,1} & \dots & x_{1,p}\\ 
\vdots & \vdots & &\vdots\\
1 & x_{n,1} & \dots & x_{n,p}
\end{pmatrix}
\end{align*}
die \textit{Designmatrix} mit den Realisierungen der Kovariablen, so kann \eqref{Eq_klassisches_modell} in Matrixnotation als
\begin{align}\label{Eq_klass_lin_matrix}
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
\end{align}
dargestellt werden. Dazu muss angenommen werden, dass $\mathbf{X}$ \textit{vollen Spaltenrang} $\text{rg}(\mathbf{X})=p+1$ besitzt und die erste Spalte aus dem Vektor $\mathbf{1}=(1,\dots, 1)'$ besteht, um den Intercept zu berücksichtigen. Für die Fehler $\boldsymbol{\varepsilon}$ wird angenommen, dass  $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$, $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$ und $\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}')=\sigma^2\mathbf{I}$ seien, wobei $\mathbf{I}$ die Einheitsmatrix bezeichnet. Für die Responsevariable gilt
\begin{align}
\mathbb{E}(\mathbf{Y})&=\mathbb{E}(\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon})=\mathbf{X} \boldsymbol{\beta} \notag \\ 
\mathbb{C}\text{ov}(\mathbf{Y})&=\mathbb{C}\text{ov}(\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\varepsilon})=\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})=\sigma^2\mathbf{I} \label{Eq_Cov_epsilon}
\end{align}
und unter zusätzlicher Annahme von \textit{normalverteilten} Fehlern, dass
\begin{align*}
\mathbf{Y} \sim \text{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}).
\end{align*} 

\subsection{Methode der kleinsten Quadrate}\label{Kap_Methode der kleinsten Quadrate}
Die Schätzung der Regressionskoeffizienten $\boldsymbol{\beta}$ für $\boldsymbol{\beta} \in \mathbb{R}^p$ kann bei vollem Spaltenrang von $\mathbf{X}$ mittels der \textit{Methode der kleinsten Quadrate} (englisch: ordinary least squares; \textit{OLS}) erfolgen. Für die Schätzer $\boldsymbol{\hat{\beta}}^{OLS}$ wird die \textit{Residuenquadratsumme} (englisch: residual sum of squares; \textit{RSS}) 
\begin{align}\label{Eq_RSS_OLS}
\text{RSS}(\boldsymbol{\beta})&=\boldsymbol{\varepsilon}'\boldsymbol{\varepsilon}=\sum_{i=1}^{n}(Y_i - \mathbf{X}_i'\boldsymbol{\beta})^2=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}) \notag \\
&=\mathbf{Y}'\mathbf{Y} -2\mathbf{Y}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
\end{align}
bzgl. $\boldsymbol{\beta}$ minimiert, d.h. 
\begin{align*}
\boldsymbol{\hat{\beta}}^{OLS}&=\arg \displaystyle\min_{\boldsymbol{\beta}} (\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}
Differenzieren von \eqref{Eq_RSS_OLS} nach $\boldsymbol{\beta}$
\begin{align}\label{Eq_Ableitung_OLS}
\frac{\partial \text{RSS}(\boldsymbol{\beta})}{\partial  \boldsymbol{\beta}} = -2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}
\end{align}
und anschließendes Nullsetzen ergibt die so genannte \textit{Normalengleichung}
\begin{align}\label{Eq_Normalengleichung}
&& 0&=-2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} \notag \\
&\Leftrightarrow & \mathbf{X}'\mathbf{X}\boldsymbol{\beta}&= \mathbf{X}'\mathbf{Y}.
\end{align}
Sind die Spalten von $\mathbf{X}$ \textit{linear unabhängig} und gilt $p \le n$, sodass $rg(\mathbf{X})=p$, handelt es sich bei $(\mathbf{X}'\mathbf{X})$ um eine \textit{positiv definite} und damit invertierbare Matrix. Als Schätzer $\boldsymbol{\hat{\beta}}^{OLS}$ ergibt sich aus \eqref{Eq_Normalengleichung}
\begin{align}\label{Eq_OLS_Schätzer}
\boldsymbol{\hat{\beta}}^{OLS} &= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}.
\end{align}
Mittels $\boldsymbol{\hat{\beta}}^{OLS}$ können die Responsevariablen $\mathbf{Y}$ als
\begin{align}\label{Eq_y_Schaetzung}
\mathbf{\hat{Y}}=\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}=\mathbf{X}\boldsymbol{\hat{\beta}}^{OLS}
\end{align}
geschätzt werden, sodass sich die Residuen $\boldsymbol{\hat{\varepsilon}}$ als Abweichungen der geschätzten von den beobachteten Responsevariablenausprägungen ergeben
\begin{align}
\boldsymbol{\hat{\varepsilon}}=\mathbf{Y}-\mathbf{\hat{Y}}=\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{OLS}.
\end{align}
Unter Berücksichtigung der Modellannahme aus \eqref{Eq_klass_lin_matrix} und $\mathbb{E}(\boldsymbol{\varepsilon})=\mathbf{0}$ ist der OLS-Schätzer ein unverzerrter Schätzer für $\boldsymbol{\beta}$, da
\begin{align}\label{Eq_Erwartungswert_OLS}
\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS})&=\mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}\right]=\mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon})\right] \notag \\
&= \mathbb{E}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon} \right] \notag \\
&=\boldsymbol{\beta}
\end{align}
und für die Kovarianzmatrix von $\boldsymbol{\hat{\beta}}^{OLS}$ gilt wegen $\mathbb{C}\text{ov}(\mathbf{Y})=\sigma^2 \mathbf{I}$, dass
\begin{align}\label{Eq_Cov_OLS}
\mathbb{C}\text{ov}(\boldsymbol{\hat{\beta}}^{OLS})&=\mathbb{C}\text{ov}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}\right]=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \mathbb{C}\text{ov}(\mathbf{Y})\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\right]' \notag \\
&=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}' \sigma^2 \mathbf{I}\left[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\right]'=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1} \notag \\
&=\sigma^2(\mathbf{X}'\mathbf{X})^{-1}.
\end{align}
Mit dem Varianzschätzer $\hat{\sigma}^2=\frac{1}{n-p}\boldsymbol{\hat{\varepsilon}}'\boldsymbol{\hat{\varepsilon}}$ kann die Kovarianzmatrix von $\boldsymbol{\hat{\beta}}^{OLS}$ als
\begin{align}
\widehat{\mathbb{C}\text{ov}(\boldsymbol{\hat{\beta}}^{OLS})}=\frac{1}{n-p}\boldsymbol{\hat{\varepsilon}}'\boldsymbol{\hat{\varepsilon}}(\mathbf{X}'\mathbf{X})^{-1}
\end{align}
geschätzt werden.\\
Mittels des \textit{Gauß-Markov-Theorems} lässt sich zeigen, dass für den OLS-Schätzer und alle anderen \textit{unverzerrten linearen} Schätzer $\boldsymbol{\tilde{\beta}}$ für einen beliebigen Vektor $\mathbf{b} \in \mathbb{R}^{p+1}$ folgendes Verhältnis gilt:
\begin{align*}
\mathbb{V}\text{ar}(\mathbf{b}'\boldsymbol{\tilde{\beta}}) \ge \mathbb{V}\text{ar}(\mathbf{b}'\boldsymbol{\hat{\beta}}^{OLS}).
\end{align*}
Somit gilt $\mathbb{V}\text{ar}(\tilde{\beta}_j) \ge \mathbb{V}\text{ar}(\hat{\beta}_j^{OLS})$, wenn $\mathbf{b}$ als kanonischer Einheitsvektor mit der Eins an der j-ten Stelle definiert wird.\\

Wird die Genauigkeit des Schätzers $\hat{\beta}_j^{OLS}$ für $\beta_j$ betrachtet, bietet sich als Risikofunktion (siehe Anhang \ref{App_Risikofunktion}) der \textit{mittlere quadratische Fehler} (englisch: mean squared error; \textit{MSE}) an, der die \textit{erwartete mittlere Abweichung} eines geschätzten vom wahren Regressionskoeffizienten angibt:
\begin{align*}
\text{MSE}(\hat{\beta}_j)=\mathbb{E}[(\hat{\beta}_j - \beta_j)^2]=\mathbb{V}\text{ar}(\hat{\beta}_j)+(Bias(\hat{\beta}_j))^2.
\end{align*}
Motiviert wird dieses Maß durch die Überlegung, dass ein unverzerrter Schätzer im Hinblick auf die Nähe zum wahren Parameter durch eine große Varianz dennoch sehr ungenau sein kann. Umgekehrt kann ein Schätzer, der eine nicht zu große systematische Verzerrung (Bias) und eine geringe Varianz aufweist, im Bezug auf seine erwartete Lage relativ dicht am wahren Parameter liegen. In Abschnitt \ref{Kap_Multikollinearität und $p > n$} wird erläutert, wodurch die einzelnen OLS-Schätzer trotz ihrer Unverzerrtheit jeweils einen relativ großen MSE haben können.\\

Als eine Verallgemeinerung des MSE kann die Verlustfunktion $\mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) = (\boldsymbol{\hat{\beta}}^{OLS} - \boldsymbol{\beta} )'(\boldsymbol{\hat{\beta}}^{OLS}-\boldsymbol{\beta})$ aufgefasst werden, die den \textit{quadrierten Abstand} des OLS-Parametervektors $\boldsymbol{\hat{\beta}}$ zum wahren Parametervektor $\boldsymbol{\beta}$ wiedergibt. Die zugehörige Risikofunktion ist der \textit{erwartete quadrierte Abstand} (siehe Anhang \ref{App_Beweis_Erwarteter_quadrierter_Abstand}):
\begin{align}\label{Eq_Erwarteter_Quadrierter_Abstand}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace=\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}].
\end{align}
Da $\mathbf{X}'\mathbf{X}$ auf der Diagonalen nur Werte $\geq 0$ haben kann und $\sigma^2 \geq 0$ gilt, muss stets auch $\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}] \geq 0$ gelten. Unter Berücksichtigung, dass die Spur einer positiv definiten Matrix gleich der Summe der Eigenwerte dieser Matrix ist und für die Inverse der Matrix die Eigenwerte die Reziproke der ursprünglichen Eigenwerte sind \cite{strang09ointro_linalg}, kann \eqref{Eq_Erwarteter_Quadrierter_Abstand} auch als \begin{align}\label{Eq_Lambda_Laenge_OLS}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace=\sigma^2 \sum_{j=0}^{p} \frac{1}{\lambda_j}
\end{align}
dargestellt werden, wobei $\lambda_{max}=\lambda_{0},\lambda_{1},\dots,\lambda_p=\lambda_{min}$ für die Eigenwerte von $\mathbf{X}'\mathbf{X}$ gilt \cite{hoerl_ridge_1970}.
Für den \textit{erwarteten Quadratwert} von $\boldsymbol{\hat{\beta}}^{OLS}$ ergibt sich
\begin{align}\label{Eq_Laenge_OLS}
\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS} {'}\boldsymbol{\hat{\beta}}^{OLS})
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}]'[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}] \right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})]'
[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})]
\right\rbrace \notag \\
&=\mathbb{E}\left\lbrace
[\mathbf{I}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]'[\mathbf{I}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]
\right\rbrace \notag \\
&=\boldsymbol{\beta}'\boldsymbol{\beta} +
\mathbb{E}\left\lbrace
[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]'[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]
\right\rbrace \notag \\
&=\boldsymbol{\beta}'\boldsymbol{\beta} +
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace \notag \\
&\stackrel{\eqref{Eq_Beweis_Erwarteter_Quadrierter_Abstand}}{=}\boldsymbol{\beta}'\boldsymbol{\beta} + 
\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}].
\end{align}
Anhand von \eqref{Eq_Laenge_OLS} ist erkennbar, dass die OLS-Schätzung trotz ihrer Erwartungstreue im Vergleich zu dem wahren Parametervektor zu groß ist. 


\subsection{Multikollinearität und "`$p > n$"'}\label{Kap_Multikollinearität und $p > n$}
Korrelieren zwei oder mehr Kovariablen der Matrix $\mathbf{X}$ hoch miteinander, ohne dass sich eine Variable als Linearkombination der anderen Kovariablen beschreiben lässt, liegt \textit{Multikollinearität} vor. In diesem Fall ist $\mathbf{X}'\mathbf{X}$ invertierbar, allerdings kommt es zu einer Inflation der Varianz von $\boldsymbol{\hat{\beta}}^{OLS}$. Sei $\hat{\beta}_j^{OLS}$ der Schätzer für den Regressionskoeffizienten $\beta_j$, so kann dessen Varianz ermittelt werden als
\begin{align}\label{Eq_var_beta_j}
\mathbb{V}\text{ar}(\hat{\beta}_j^{OLS})=\sigma^2 (\mathbf{X}'\mathbf{X})^{-1}_{jj}=\frac{\sigma^2}{1-R^2_j} \ ,
\end{align}  
wobei $0 \leq R^2_j \leq 1$ der (multiple) \textit{Determinationskoeffizient} für die Regression von $\mathbf{x}_j$ auf die übrigen $p-1$ Kovariablen ist. Je stärker $\mathbf{x}_j$ mit den restlichen Kovariablen korreliert, d.h. je größer $R^2$ ist, desto kleiner wird der Nenner von \eqref{Eq_var_beta_j} und damit umso größer die Varianz von $\hat{\beta}_j^{OLS}$. Zudem kann es bereits durch kleine Änderungen in der Matrix $\mathbf{X}'\mathbf{X}$ zu geänderten Vorzeichen bei den $\hat{\beta}_j^{OLS}$ kommen.\\
Aus \eqref{Eq_Laenge_OLS} wird deutlich, dass die erwartete quadrierte Länge von $\boldsymbol{\hat{\beta}}^{OLS}$ um $\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}]$ größer als die von $\boldsymbol{\beta}$ ist. Multikollinearität hat zur Folge, dass die Eigenwerte von $\mathbf{X}'\mathbf{X}$ stark schrumpfen und deswegen unter Berücksichtigung von \eqref{Eq_Lambda_Laenge_OLS} die Spur von $(\mathbf{X}'\mathbf{X})^{-1}$ ansteigt. Dadurch wird $\mathbb{E}(\boldsymbol{\hat{\beta}}^{OLS}{'} \boldsymbol{\hat{\beta}}^{OLS})$ deutlich größer als $\mathbb{E}(\boldsymbol{\beta}' \boldsymbol{\beta})$. Interpretiert man die Differenz als Maß für die \textit{Gesamtgenauigkeit} der Schätzer, wird diese durch Multikollinearität insgesamt größer und daher schlechter. Für die Absolutwerte $|\hat{\beta}_j^{OLS}|$ bedeutet das, dass die OLS-Methode den Einfluss einzelner Kovariablen überschätzt \cite{Marquardt1975RidgePractice,Brook1980ExpectedLengthOLS}.\\
Bei einer \textit{perfekten} Multikollinearität kann eine Kovariable als Linearkombination der anderen Kovariablen dargestellt werden. In diesem Fall sind $\mathbf{X}$ und damit auch $\mathbf{X}'\mathbf{X}$ singulär \cite{strang09ointro_linalg}, weswegen die Normalengleichung aus \eqref{Eq_Normalengleichung} nicht nach $\boldsymbol{\beta}$ aufgelöst werden kann und es keine eindeutige Lösung für $\boldsymbol{\beta}$ gibt.\\
Bei hochdimensionalen Daten mit $p>n$ oder sogar $p \gg n$ ist $rg(\mathbf{X}) < p+1$ und die Normalengleichung kann keinen eindeutigen Schätzer für $\boldsymbol{\beta}$ liefern \cite{johnstone_statistical_2009}.\\

\section{Penalisierte Regression}\label{Kap_Penalisierte Regression}
Für \textit{hochdimensionale} Daten, wie sie in der Genetik vorliegen, kann aus \textit{inhaltlichen} Gründen häufig die Annahme vertreten werden, dass nur wenige Kovariablen einen Einfluss auf die Responsevariable haben. Wird dieser Zusammenhang als klassisches lineares Regressionsmodell $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}$ ausgedrückt, muss daher angenommen werden, dass der Regressionskoeffizientenvektor $\boldsymbol{\beta}$ in Wahrheit \textit{spärlich} besetzt ist, d.h. für die meisten Koeffizienten $\beta_j = 0$ gilt. Die Herausforderung besteht darin, durch die Regression sowohl die $\beta_j$, für die $\beta_j \neq 0$ gilt, zu \textit{selektieren} als auch deren Einfluss auf die Responsevariable möglichst genau zu schätzen. Wie in Kapitel \ref{Kap_Methode der kleinsten Quadrate} und \ref{Kap_Multikollinearität und $p > n$} dargestellt, hängt $\mathbb{V}\text{ar}(\hat{\beta}_j)$ von $\mathbf{X}'\mathbf{X}$ ab, weswegen es bei hoher Multikollinearität zu einem hohen $\text{MSE}(\hat{\beta}_j)$ kommt. Gleichzeitig wird der Einfluss der einzelnen Kovariablen insgesamt überschätzt. Sollte eine exakte Multikollinearität bzw. $p > n$ vorliegen, kann die OLS-Methode darüber hinaus keine (eindeutigen) Schätzer liefern.\\
Penalisierungsansätze verfolgen die Strategie, die Unverzerrtheit des OLS-Schätzers zu "`opfern"', d.h. absichtlich ein Bias zu erzeugen, um einerseits eine Variablenselektion für die spärliche Besetzung von $\boldsymbol{\hat{\beta}}$ zu ermöglichen und gleichzeitig den geschätzten Einfluss der einzelnen Kovariablen zu schrumpfen. Wegen $\text{MSE}(\hat{\beta}_j)=\mathbb{V}\text{ar}(\hat{\beta}_j)+ \text{Bias}(\hat{\beta}_j)$ darf der durch die Penalisierung erzeugte Bias nicht zu groß werden, da ansonsten der MSE basierend auf der Penalisierungsmethode den MSE basierend auf der OLS-Methode übertrifft.\\
Als grundlegendes Prinzip wird bei der Penalisierung die zu minimierende Verlustfunktion, in der Regel die Residuenquadratsumme $\text{RSS}(\boldsymbol{\beta})$, um einen Strafterm $P(\boldsymbol{\beta})$ erweitert, sodass sich eine zu minimierende \textit{penalisierte Residuenquadratsumme} (englisch: penalized residual sum of squares; \textit{PRSS}) als  
\begin{align}\label{Eq_PRSS_allgmein}
\text{PRSS}(\boldsymbol{\beta},\boldsymbol{\gamma})=\text{RSS}(\boldsymbol{\beta}) + P(\boldsymbol{\beta}, \boldsymbol{\gamma})
\end{align} mit einem oder mehreren speziell zu wählenden \textit{Tuningparametern} $\boldsymbol{\gamma}$ ergibt. Generell wird gefordert, dass die abhängige Variable $\mathbf{Y}$ zentriert ist, d.h. es muss $\frac{1}{n}\sum_{i}^{n}y_i =0$ für die Realisierungen $y$ gelten, und die Kovariablen $\mathbf{X}$ müssen standardisiert sein, sodass $\frac{1}{n}\sum_{i=1}^{n}x_{ij}=0$ und $\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2=1$ für $j=1,\dots,p$ gilt. Dadurch ist die Penalisierung unabhängig von den einzelnen Skalen der Kovariablen bzw. der Responsevariable und das Intercept fällt weg. Für die Notation gilt daher im Folgenden, wenn nicht anders gekennzeichnet, $\boldsymbol{\beta}=(\beta_1,\dots,\beta_p)'$ bzw. $\boldsymbol{\hat{\beta}}=(\hat{\beta}_1,\dots,\hat{\beta}_p)'$ und die Designmatrix $\mathbf{X}$ beinhaltet nur noch die Kovariablenvektoren.\\
Kapitel \ref{Kap_Ridge-Regression} und \ref{Kap_Lasso-Regression} stellen zunächst die "`klassischen"' Penalisierungsmethoden \textit{Ridge-Regression} und \textit{Lasso-Regression} vor. Auf dieser Grundlage wird in Kapitel \ref{Kap_Elastic-Net} die Methode \textit{Elastic Net} eingeführt, die eine Kombination aus Lasso und Ridge ist. Abschließend werden \textit{netzwerkbasierte penalisierte Regressionen} in Kapitel \ref{Kap_Netzwerkbasierte_Penalisierung} näher betrachtet und Möglichkeiten zur Berücksichtigung der in Kapitel \ref{Kap_Zentralitätsstatistiken} eingeführten Zentralitätsmaße aufgezeigt.
\begin{comment}
Unverzerrtheit des $\hat{\beta}$ der Kleinste-Quadratmethode wird geopfert, um den \textit{mittleren quadratischen Fehler} (englisch: mean squared error; \textit{MSE}) zu senken\\
Ridge wird eingeführt, weil Teil von Elastic Net\\
kein Intercept (bei allen Penalisierungen?); durch Standardisierung bzw. Zentralisierung fällt $\beta_0$ weg \cite{montgomery2012introduction}\\
warum keine Subsetselection\\
hat penalisierung, auch die netzwerkbasierte, was baysianisches?\\
!!\\
LAGARGIAN MULTIPLIER?????\\
penalized bestraft größere Beta mehr - intercept raus, weil sonst bestrafung nicht unabhängig von der skala des outcomes wäre!!! (für allgemeine einführung: http://web.as.uky.edu/statistics/users/pbreheny/764-F11/notes/8-30.pdf)
\end{comment}

\subsection{Ridge-Regression}\label{Kap_Ridge-Regression}
Die \textit{Ridge-Regression} ist eine Penalisierungsmethode, bei der die geschätzten Regressionskoeffizienten in $\boldsymbol{\hat{\beta}}^{Ridge}$ \textit{relativ} im Verhältnis zu den Schätzern in $\boldsymbol{\hat{\beta}}^{OLS}$ \textit{geschrumpft} werden. Die Schätzung der Regressionskoeffizienten erfolgt durch
\begin{align}\label{Eq_Ridge_estimate}
\boldsymbol{\hat{\beta}}^{Ridge} = (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}.
\end{align}
Durch die Addition von $\gamma \mathbf{I}$ verringert sich die Varianz der Schätzer im Vergleich zu den OLS-Schätzern (siehe \eqref{Eq_Erw_Quad_Abst_ridge}) und für den Fall, dass $\text{rg}(\mathbf{X}) < p$ und damit $\mathbf{X}'\mathbf{X}$ singulär ist, wird der Ausdruck in der Klammer regulär und \eqref{Eq_Ridge_estimate} ist eindeutig lösbar \cite{montgomery2012introduction}.
Allerdings findet in der Regel \textit{keine} Variablenselektion statt, sodass die Annahme, dass $\boldsymbol{\beta}$ spärlich besetzt ist, sich nicht in $\boldsymbol{\hat{\beta}}^{Ridge}$ widerspiegelt \cite{tibshirani96regression}. Für die Ridge-Regression wird $\arg\min\limits_{\boldsymbol{\beta}} \sum_{i=1}^{n} (Y_i - \mathbf{x}_i' \boldsymbol{\beta})^2$ um die $L_2$-Norm-Nebenbedingung $\sum_{j=1}^{p}\beta_j^2 \le t$ mit $t > 0$ ergänzt. In diesem Zusammenhang wird $t$ als \textit{Restriktionsparameter} bezeichnet, der als Bedingung vorgibt, wie groß die Summe der quadrierten Regressionskoeffizienten maximal sein darf. 
%%%XXXXXXXXXXXXXXXXXXXXXXXX DAS ggf noch mal überarbeiten im Hinblick auf "`große Parameter"' XXXXXXXXXXXXXXX 
Da $t$ invers mit dem Tuningparameter $\gamma$ zusammenhängt, kann die Berechnung von $\boldsymbol{\hat{\beta}}^{Ridge}$ als Minimierungsproblem in der \textit{Lagrange-Form} (siehe Anhang \ref{App_Lagrange-Form})
%%%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\begin{align}\label{Eq_Ridge_lagrangian}
\boldsymbol{\hat{\beta}}^{Ridge}(\gamma)=\arg \displaystyle\min_{\boldsymbol{\beta}} \left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j )^2  + \gamma \sum_{j=1}^{p}\beta_j^2 \right\rbrace \qquad \text{mit} \ \gamma \ge 0
\end{align}
dargestellt werden \cite{hastie2009elements}.
Wird $\sum_{j=1}^{p} (\hat{\beta}_j^{OLS})^2 \le t$ gesetzt, entsprechen die Ridge-Schätzer den OLS-Schätzern, da in diesem Fall $\gamma=0$ ist und sich \eqref{Eq_Ridge_lagrangian} zum einfachen OLS-Minimierungsproblem reduziert \cite{tibshirani96regression}.\\
Der bzgl. $\boldsymbol{\beta}$ zu minimierende Term aus \eqref{Eq_Ridge_lagrangian} lässt sich in Abhängigkeit von $\gamma$ durch
\begin{align}\label{Eq_PRSS_Ridge}
\text{PRSS}_{Ridge}(\boldsymbol{\beta}, \gamma)&=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+\boldsymbol{\beta}'\gamma\boldsymbol{\beta} \notag \\
&=\mathbf{Y}'\mathbf{Y} -2\mathbf{Y}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\beta}'\gamma\boldsymbol{\beta}
\end{align}
darstellen. Differenzierung nach $\boldsymbol{\beta}$ ergibt
\begin{align}\label{Eq_Ableitung_Ridge}
\frac{\partial \text{PRSS}_{Ridge}(\boldsymbol{\beta}, \gamma)}{\partial  \boldsymbol{\beta}} = -2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + 2\gamma \boldsymbol{\beta}
\end{align}
und anschließendes Nullsetzen liefert den Schätzer $\boldsymbol{\hat{\beta}}^{Ridge}$ aus \eqref{Eq_Ridge_estimate}
\begin{align*}
&& 0&=-2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + 2\gamma \boldsymbol{\beta} \notag \\
&\Leftrightarrow & \mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma \boldsymbol{\beta} &= \mathbf{X}'\mathbf{Y} \notag \\
&\Leftrightarrow & (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I}) \boldsymbol{\beta} &= \mathbf{X}'\mathbf{Y} \notag\\
&\Rightarrow & \boldsymbol{\hat{\beta}}^{Ridge} &= (\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}.
\end{align*}
Der Ridge-Schätzer aus \eqref{Eq_Ridge_estimate} kann auch als Transformation des OLS-Schätzers dargestellt werden \cite{hoerl_ridge_1970}
\begin{align}\label{Eq_ridge_transformation}
\boldsymbol{\hat{\beta}}^{Ridge} =&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y} \notag\\
\stackrel{\eqref{Eq_y_Schaetzung}}{=}&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1} \mathbf{X}'\mathbf{X} (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} \notag\\
=&(\mathbf{X}'\mathbf{X} + \gamma \mathbf{I})^{-1} \mathbf{X}'\mathbf{X} \boldsymbol{\hat{\beta}}^{OLS} \notag\\
=&(\mathbf{I} + \gamma (\mathbf{X}'\mathbf{X})^{-1})^{-1}  \boldsymbol{\hat{\beta}}^{OLS}.
\end{align}
Daraus ist ersichtlich, dass $\boldsymbol{\hat{\beta}}^{Ridge} \ne \boldsymbol{\hat{\beta}}^{OLS}$ für $\gamma \ne 0$ gilt und in diesem Fall $\boldsymbol{\hat{\beta}}^{Ridge}$ nicht erwartungstreu ist \cite{seber2003linear}, weil der OLS-Schätzer erwartungstreu ist.\\ 
Es lässt sich zeigen, dass mit der Varianz aus \eqref{Eq_Cov_epsilon} für den erwarteten quadrierten Abstand für $\boldsymbol{\hat{\beta}}^{Ridge}$ zu $\boldsymbol{\beta}$ gilt (vgl. \citeA{hoerl_ridge_1970}):
\begin{align}\label{Eq_Erw_Quad_Abst_ridge}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{Ridge},\boldsymbol{\beta}) \right\rbrace=\sigma^2 \sum_{j=1}^{p} \frac{\lambda_j}{(\lambda_j + \gamma)^2} + \gamma^2 \boldsymbol{\beta}' (\mathbf{X}'\mathbf{X}+ \gamma \mathbf{I})^{-2}\boldsymbol{\beta},
\end{align}
wobei $\lambda_{max}= \lambda_1, \dots, \lambda_p= \lambda_{min}$ die Eigenwerte von $\mathbf{X}'\mathbf{X}$ sind. Der linke Summand von \eqref{Eq_Erw_Quad_Abst_ridge} beinhaltet in Anlehnung an \eqref{Eq_Lambda_Laenge_OLS} die Summe der Varianzen der $\hat{\beta}_j^{Ridge}$ und der rechte Summand den durch $\gamma$ erzeugte Bias. Anhand dieser Darstellung ist erkennbar, dass mit größer werdendem $\gamma$ die Summe der Varianzen des Schätzers abnimmt und gleichzeitig die gesamte Verzerrung ansteigt. Umgekehrt wird an dieser Risikofunktion deutlich, dass für $\gamma=0$ der Ridge-Schätzer dem OLS-Schätzer entspricht und folglich dieselbe erwartete quadratische Distanz zum wahren $\boldsymbol{\beta}$ hat. Dieser Abstand ist wie in Abschnitt \ref{Kap_Multikollinearität und $p > n$} gezeigt, deutlich von der Multikollinearität von $\mathbf{X}$ abhängig. Ziel bei der Wahl von $\gamma$ ist es deswegen, ein optimales $\gamma$ zu finden, dass $\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{Ridge},\boldsymbol{\beta}) \right\rbrace$ bzw. die einzelnen $\text{MSE}(\hat{\beta}_j^{Ridge})$ minimiert. \citeA{hoerl_ridge_1970} zeigen, dass es immer ein $\gamma$ gibt, für das $\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{Ridge},\boldsymbol{\beta}) \right\rbrace < \mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace$ gilt. In Bezug auf die Risikofunktion bedeutet das, dass die Ridge-Schätzer mit optimalem $\gamma$ für alle wahren $\boldsymbol{\beta}$ \textit{strikt besser} sind als die OLS-Schätzer \cite{gross2003linear}.\\
Die Wahl des Tuningparameters $\gamma$ muss dabei vom Anwender getroffen werden und kann, soll er nicht subjektiv festgelegt werden, durch iterative Methoden geschätzt werden. Eine Übersicht über verschiedene Methoden bzw. weiterführende Literatur zur Wahl von $\gamma$ findet sich u.a. bei \citeA{gross2003linear} und \citeA{montgomery2012introduction}.


\subsection{Lasso-Regression}\label{Kap_Lasso-Regression}
%%%WIESO KANN LASSO $p>n$ LÖSEN?????????? siehe übersichtspaper bzgl der quadrat durch betrag erweiterung?
Anders als bei der Ridge-Regression wird bei der \textit{Lasso-Regression} ein $L_1$-Norm-Strafterm in die Lagrange-Form als zusätzliches Argument eingeführt, womit sich
\begin{align}\label{Eq_Lasso_Lagrange}
\boldsymbol{\hat{\beta}}^{Lasso}(\gamma)=\arg \displaystyle\min_{\beta} \left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2  + \gamma \sum_{j=1}^{p}|\beta_j| \right\rbrace \qquad \text{mit} \ \gamma \ge 0
\end{align}
als Minimierungsproblem ergibt \cite{tibshirani96regression}. Die Betragsfunktion in \eqref{Eq_Lasso_Lagrange} hat eine \textit{gleichmäßige} Schrumpfung der Koeffizienten zur Folge, wodurch für einzelne Regressionskoeffizienten $\hat{\beta}_j =0$  und damit eine Variablenselektion erreicht werden kann. Dieser Sachverhalt lässt sich geometrisch darstellen, wenn \eqref{Eq_Lasso_Lagrange} als Optimierungsproblem mit einer Nebenbedingung ausgedrückt wird
\begin{align}\label{Eq_Lasso_Constraint}
\boldsymbol{\hat{\beta}}^{Lasso}(t)=\arg \displaystyle\min_{\beta} \ &\left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2 \right\rbrace \notag  \\
\text{s.d.} \ & \sum_{j=1}^{p}|\beta_j| \le t  \qquad \text{mit} \ t \ge 0.
\end{align}
Für den Fall von zwei Kovariablen ist die Nebenbedingung aus \eqref{Eq_Lasso_Constraint} eine gleichwinkelige Raute und die zu minimierende Residuenquadratsumme kann als Ellipsen, deren Form durch $\mathbf{X}'\mathbf{X}$ bestimmt sind, um den OLS-Schätzer im euklidischen Vektorraum $\mathbb{R}^2$ gezeichnet werden. Abbildung \ref{fig:Ridge_vs_Lasso} veranschaulicht den Schrumpfungs- und Selektionseffekt des Lasso- gegenüber dem Ridge-Schätzer. Im Vergleich zur OLS-Schätzung werden bei beiden Methoden $\hat{\beta}_1$ und $\hat{\beta}_2$ geschrumpft, allerdings kommt es nur bei der Lasso-Methode zu einer Variablenselektion, da die Ellipsen eine Ecke der Raute tangieren und der entsprechende Koeffizient $0$ gesetzt wird. Somit kann unter Einhaltung der Nebenbedingung bei der Lasso-Schätzung eine Variablenselektion erreicht werden, die bei der Ridge-Regression nicht möglich ist \cite{tibshirani96regression}.\\ 
Die geometrische Interpretation des Lasso-Schätzers gilt auch für höherdimensionale Daten, wobei die Nebenbedingung ein $p$-dimensionaler Kreuzpolytop, der um den Faktor $t$ skaliert wird, und die Zielfunktion ein $p$-dimensionaler Ellipsoid mit dem OLS-Schätzer im Zentrum ist (vgl. dazu \citeNP{Perty2009ShrinkagePolytopes}).\\
<<echo=FALSE, Ridge_vs_Lasso, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.49\\linewidth', fig.lp="fig:", fig.scap="Graphische Veranschaulichung des Lasso- und Ridge-Strafterms", fig.cap="Graphische Veranschaulichung des Lasso-Strafterms ($L_1$-Norm, Raute $|\\beta_1|+|\\beta_2|=t$) und des Ridge-Strafterms ($L_2$-Norm, Einheitskreis $ \\beta_1^2 + \\beta_2^2 = t$ ) hinsichtlich des Schrumpfungseffekts und der Variablenselektion. Die Ellipsen um den OLS-Schätzer (rot) werden solange vergrößert, bis sie die Nebenbedingung erfüllen (hellblau) und die Raute bzw. den Einheitskreis tangieren. Während es bei beiden Methoden zu einer Schrumpfung der Koeffizientenschätzer kommt, wird nur bei der Lasso-Penalisierung ein Koeffizient null gesetzt, da die Ellipse die Raute an einer Ecke tangiert.">>=
#### R-Code Lasso vs Ridge Ellipsen

library(ellipse)
#
# Plot an ellipse corresponding to a 95% probability region for a 
# bivariate normal distribution with mean scale, variances scale and 
# correlation 0.8. 
#
x <- y <- seq(-12,12,.1)
covmat <- matrix(c(1,0.5,0.5,1),nrow=2,ncol=2)


### LASSO
plot(ellipse(x=covmat,scale=c(1.88,1.88),centre=c(4,9.35),level=0.95, lwd=3),
     type = 'l',xlim=c(-5,10),ylim=c(-5,10),xlab=expression(beta[1]),
     ylab=expression(beta[2]),xaxt="n",yaxt="n", cex.lab=1, asp=1) 
abline(a=0,b=0,col="grey")
abline(v=0,col="grey")
lines(ellipse(x=covmat,scale=c(1.15,1.15),centre=c(4,9.35),level=0.95))
lines(ellipse(x=covmat,scale=c(.3,.3),centre=c(4,9.35),level=0.95))
points(4,9.35, pch=20, col="red", cex=1.7)
text(5.15,8.6, expression(hat(beta)^{OLS}), cex=1.4)

axis(1,at=0)
axis(2,at=0)
edge <- 5.4
lines(x=c(-edge,0),y=c(0,edge), lwd=1)
lines(x=c(-edge,0),y=c(0,-edge), lwd=1)
lines(x=c(0,edge),y=c(-edge,0), lwd=1)
lines(x=c(0,edge),y=c(edge,0), lwd=1)

points(0,5.4, col="cyan2", pch=20, cex=1.7)
text(1.2,6, expression(hat(beta)^{Lasso}), cex=1.4)



### RIDGE

plot(ellipse(x=covmat,scale=c(2.26,2.26),centre=c(4,9.35),level=0.95),
     type = 'l',xlim=c(-5,10),ylim=c(-5,10),xlab=expression(beta[1]),
          ylab=expression(beta[2]),xaxt="n",yaxt="n", cex.lab=1, asp=1)
abline(a=0,b=0,col="grey")
abline(v=0,col="grey")
lines(ellipse(x=covmat,scale=c(1.15,1.15),centre=c(4,9.35),level=0.95))
lines(ellipse(x=covmat,scale=c(.3,.3),centre=c(4,9.35),level=0.95))
points(4,9.35, pch=20, col="red", cex=1.7)
text(5.15,8.6, expression(hat(beta)^{OLS}), cex=1.4)

axis(1,at=0)
axis(2,at=0)
covmatc <- matrix(c(1,0,0,1),nrow=2,ncol=2)
lines(ellipse(x=covmatc,scale=c(2,2),level=0.85))
points(0.68,3.85, col="cyan2", pch=20, cex=1.7)
text(1.4,4.8, expression(hat(beta)^{Ridge}), cex=1.4)
@
Für den Fall, dass $\mathbf{X}$ orthogonal ist, d.h. $\mathbf{X}'\mathbf{X}=\mathbf{I}$ gilt, können die Lasso-Schätzer durch die OLS-Schätzer bestimmt werden \cite{tibshirani96regression}:
\begin{align}\label{Eq_Lasso_Orthogonaldesign}
\hat{\beta}_j^{Lasso}=\text{sign}(\hat{\beta}_j^{OLS})(|\hat{\beta}_j^{OLS}|-\frac{\gamma}{2})^+
\end{align}
Mit $\text{sign}(\beta_j^{OLS})$ wird das Vorzeichen des entsprechenden OLS-Schätzers bezeichnet und $(\cdot)^+$ gibt an, dass nur der positive Teil in der Klammer ausgewertet wird, d.h. wenn $|\beta_j^{OLS}|-\frac{\gamma}{2} > 0$ gilt. Somit wird $\beta_j^{Lasso} = 0$ gesetzt, wenn $\frac{\gamma}{2} \ge |\beta_j^{OLS}|$ ist. Der Beweis für \eqref{Eq_Lasso_Orthogonaldesign} findet sich in Anhang \ref{App_Lasso_Orthogonal}. Allerdings setzt dieses Design den eher selten auftretenden Fall voraus, dass die Anzahl der Variablen gleich der Anzahl an Beobachtungen ist, d.h. $p=n$ gilt.\\
Wird \eqref{Eq_Lasso_Lagrange} nach $\boldsymbol{\beta}$ abgeleitet
\begin{align*}
\frac{\partial \text{PRSS}_{Lasso}(\boldsymbol{\beta}, \gamma)}{\partial \boldsymbol{\beta}}=-2\mathbf{X}'\mathbf{Y}+2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}+\gamma\text{sign}(\boldsymbol{\beta})
\end{align*}
und bezeichnet $\text{sign}(\boldsymbol{\beta})$ einen $p$-dimensionalen Vektor, der die Vorzeichen von $\boldsymbol{\beta}$ in der Form $(\pm1,\pm1,\dots, \pm1)$ beinhaltet, kann der Lasso-Schätzer für ein festes $\gamma$ allgemein dargestellt werden als
\begin{align}
&& 0&=-2\mathbf{X}'\mathbf{Y}+2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}+\gamma\text{sign}(\boldsymbol{\beta}) \notag \\
&\Leftrightarrow &\mathbf{X}'\mathbf{X}\boldsymbol{\beta}&= \mathbf{X}'\mathbf{Y}-\frac{\gamma}{2}\text{sign}(\boldsymbol{\beta}) \notag \\
&\Rightarrow & \boldsymbol{\hat{\beta}}^{Lasso}&=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}-(\mathbf{X}'\mathbf{X})^{-1}\frac{\gamma}{2}\text{sign}(\boldsymbol{\beta}) \label{Eq_Lasso-Schätzer_Allgemein}.
\end{align}
Gleichung \eqref{Eq_Lasso-Schätzer_Allgemein} verdeutlicht, dass eine geschlossene Darstellung des Minimierungsproblems aus \eqref{Eq_Lasso_Lagrange} und damit eine analytische Lösung für die Lasso-Methode nicht möglich ist, da diese die Vorzeichen der zu schätzenden $\boldsymbol{\beta}$ beinhaltet. Allerdings handelt es sich sowohl bei $\text{RSS}(\boldsymbol{\beta})$ als auch bei $P(\boldsymbol{\beta}, \boldsymbol{\gamma})=\gamma \sum_{j=1}^{p}|\beta_j|$ um \textit{konvexe} Funktionen (siehe Anhang \ref{App_Konvexe_Funktionen}), weswegen eine Lösung für das Minimierungsproblem existiert \cite{Osborne99onthe}. \citeA{tibshirani96regression} schlägt zur Lösung einen iterativen Algorithmus für quadratische Optimierungsprobleme vor, der allerdings wegen $2^p$ möglichen Vorzeichenkombinationen in $\text{sign}(\boldsymbol{\beta})$ für ein großes $p$ ineffektiv ist. Zudem ist dieser Algorithmus nicht geeignet für den Fall $p>n$, da $\mathbf{X}'\mathbf{X}$ nicht invertierbar ist und als "`Startschätzer"' für die Regressionskoeffizienten der OLS-Schätzer benutzt wird. \citeA{Efron04leastangle} zeigen, dass eine Modifikation ihres Algorithmus für die \textit{kleinste Winkel Regression} (englisch: least angle regression; \textit{LARS}) einen Lasso-Schätzer liefert, dessen Berechnung nicht mehr Ressourcen als eine OLS-Schätzung benötigt und für $p > n$ anwendbar ist, allerdings nur maximal $n$ Schätzer ungleich null setzen kann. Der Homotopie Algorithmus von \citeA{Osborne99onthe} ist ebenfalls ein Lösungsweg für $\boldsymbol{\hat{\beta}}^{Lasso}$ für $p > n$, dessen Berechnung jedoch für große $p$ sehr aufwendig wird.\\
Neben der Herausforderung der Schätzung von $\boldsymbol{\beta}$, insbesondere für $p > n$, muss ein geeignetes $\gamma$ gewählt werden, da $\boldsymbol{\hat{\beta}}^{Lasso}$ von der Wahl dieses Parameters bzw. des Restriktionsparameters $t$ abhängt. Dazu müssen die oben erwähnten Algorithmen mit verschiedenen $\gamma$ durchgeführt und mittels einer Kennzahl verglichen werden. \citeA{leng06noteonthelasso} zeigen, dass Methoden, die die \textit{Vorhersagegenauigkeit} als Maß zur Wahl von $\gamma$ nutzen, \textit{nicht konsistente} Schätzungen für den Vektor $\boldsymbol{\beta}$ liefern, d.h. für die Wahrscheinlichkeit, dass $\boldsymbol{\hat{\beta}}^{Lasso}$ das wahre Model liefert, gilt $P(\boldsymbol{\beta}^{Lasso} = \boldsymbol{\beta}) < 1$. Gleichzeitig merken die Autoren an, dass die \textit{Genauigkeit} der einzelnen geschätzten Regressionskoeffizienten durch die Lasso-Methode gegenüber der OLS-Schätzung in Modellen verbessert werden. Gilt die sogenannte \textit{Irrepresentable Condition} (für die Definition siehe \ref{App_Irrepresentable Condition}), zeigen \citeA{Zhao2006Lasso_Consistency}, dass der Lasso-Schätzer für ein vorgegebenes $\gamma$ eine konsistente Schätzung für $\boldsymbol{\beta}$ liefert. Da diese Annahme allerdings in der Regel nicht überprüft werden kann, muss in Anwendungsfällen davon ausgegangen werden, dass die Schätzung nicht konsistent ist.\\
Ein weiteres Problem bei der Anwendung des Lasso-Schätzers sind Korrelationsstrukturen, die insbesondere bei hochdimensionalen Daten auftreten. Korrelieren zwei oder mehrere Kovariablen hoch miteinander und gilt für diese Kovariablen jeweils $\beta_j \neq 0$, wählt der Lasso-Schätzer davon zufällig nur eine Kovariable aus und setzt die restlichen Regressionskoeffizienten dieser "`Gruppe"' gleich null \cite{Bondell2008OSCAR}.\\
Um zumindest einen Teil dieser Limitierungen des Lasso-Schätzers zu beseitigen, gibt es inzwischen diverse Weiterentwicklungen des Lasso-Schätzers wie bspw. den \textit{Grouped Lasso-Schätzer} \cite{Yuan06Grouped_modelselection} oder den \textit{Adaptive Lasso-Schätzer} \cite{zou2006Adaptive_Lasso_oracle_prop}. Beide Ansätze verfolgen die Idee, den ursprünglichen Lasso-Schätzer um vorab bekannte Informationen bzw. Annahmen zu erweitern. Für den Adaptive Lasso-Schätzer wird dazu eine Gewichtung der einzelnen Regressionskoeffizienten im Strafterm vorgenommen, die die vorab festgelegte Bedeutung einzelner Kovariablen repräsentiert. Beim Grouped Lasso-Schätzer hingegen werden die Kovariablen zuvor in Gruppen zusammengefasst, die einen Zusammenhang zwischen den Kovariablen untereinander repräsentieren, und anschließend gemeinsam "`bestraft"'. 




%%% Vorteil von Lasso: durch gleichmäßige Schrumpfung werden wertvolle Variablen, also die, die unbedingt ins Modell solten, nicht so stark gegen Null geschrumpft wie die, die nicht rein sollten nud eh schon kleine Werte haben (siehe Paper von Tim Hesterberg zu LARS)

%%% Kann wegen der Sparsity-Voraussetzung Lasso maximal n Kovariablen auswählen???

%%% ACHTUNG: p>n werden nur n Variablen ausgewählt (Bühlmann findet gut an Lasso, weil eine starke Dimensionreduktion, aber im Hinblick auf den Sachverhalt, dass man möglichst alle relevanten Kovariablen haben will, ist solch eine Einschränkung eher kritisch zu sehen); wenn zwei oder mehr Kovariablen miteinander Korrelieren (oder identisch sind???) wählt Lasso nur eine ZUFÄLLIG aus

%%% Verweisen:
%Fused lasso Tibshirani 2005
%Fused lasso for unordered predictors




\subsection{Elastic Net-Regression}\label{Kap_Elastic-Net}
Um sowohl die Vorteile des Ridge- als auch des Lasso-Schätzers, d.h. Stabilisierung der Schätzungen und Anwendbarkeit für $p>n$ sowie Variablenselektion, in einer Methode zu vereinen, haben \citeA{zou_regularization_2005} den \textit{Elastic Net-Schätzer} (\textit{Enet}-Schätzer) entwickelt:
\begin{align}\label{Eq_Enet_Lagrange}
\boldsymbol{\hat{\beta}}^{Enet}(\gamma_1, \gamma_2)&=\arg \displaystyle\min_{\beta} \textnormal{PRSS}_{Enet}(\boldsymbol{\beta}, \gamma_1, \gamma_2) \notag \\
&=\arg \displaystyle\min_{\beta} \left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2  + \gamma_1 \sum_{j=1}^{p}|\beta_j| + \gamma_2 \sum_{j=1}^{p}\beta_j^2 \right\rbrace \quad \text{mit} \ \gamma_1,\gamma_2 \ge 0.
\end{align}
Der Schätzer ist eine Kombination aus der $L_1$-Norm-Bestrafung der Lasso-Regression und der $L_2$-Norm-Bestrafung der Ridge-Regression. Wird $\alpha=\gamma_2 / (\gamma_1+\gamma_2)$ für $\alpha \in [0,1]$ definiert, lässt sich \eqref{Eq_Enet_Lagrange} auch alternativ darstellen als das Optimierungsproblem
\begin{align}\label{Eq_Enet_Constraint}
\boldsymbol{\hat{\beta}}^{Enet}(\gamma_1, \gamma_2)=\arg \displaystyle\min_{\beta} &\left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2 \right\rbrace \notag \\
\text{s.d.} \ &(1-\alpha)\sum_{j=1}^{p}|\beta_j|+\alpha \sum_{j=1}^{p}\beta_j^2 \leq t \qquad \text{mit} \  t \geq 0.
\end{align}
$\alpha$ beschreibt demnach den verhältnismäßigen Einfluss des Ridge- zum Lasso-Term bei der Schätzung von $\boldsymbol{\beta}$. Solange $\alpha > 0$ gilt, handelt es sich bei der Enet-Regression um ein strinkt konvexes Optimierungsproblem.\\
Wird \eqref{Eq_Enet_Lagrange} in Matrixnotation dargestellt, ergibt sich für den \textit{naiven Elastic Net-Schätzer} (\textit{nEnet}-Schätzer) durch Ableiten und null setzen
\begin{align}\label{Eq_Naiv_Enet_Eigene}
&&\frac{\partial \textnormal{PRSS}_{Enet}(\boldsymbol{\beta}, \gamma_1, \gamma_2)}{\partial \boldsymbol{\beta}}&=-2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}+2\gamma_2 \mathbf{\boldsymbol{\beta}} + \gamma_1 \textnormal{sign}(\boldsymbol{\beta}) \notag \\
&\Rightarrow & 0 &=-2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta}+2\gamma_2 \mathbf{\boldsymbol{\beta}} + \gamma_1 \textnormal{sign}(\boldsymbol{\beta}) \notag \\
&\Leftrightarrow & \mathbf{X}'\mathbf{X}\boldsymbol{\beta}+\gamma_2 \mathbf{\boldsymbol{\beta}} &=\mathbf{X}'\mathbf{Y} - \frac{\gamma_1}{2} \textnormal{sign}(\boldsymbol{\beta}) \notag \\
&\Leftrightarrow & (\mathbf{X}'\mathbf{X}+\gamma_2 \mathbf{I}) \mathbf{\boldsymbol{\beta}} &=\mathbf{X}'\mathbf{Y} - \frac{\gamma_1}{2} \textnormal{sign}(\boldsymbol{\beta}) \notag \\
&\Rightarrow & \boldsymbol{\hat{\beta}}^{nEnet} 
&=(\mathbf{X}'\mathbf{X}+\gamma_2 \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y} - (\mathbf{X}'\mathbf{X}+\gamma_2 \mathbf{I})^{-1} \frac{\gamma_1}{2} \textnormal{sign}(\boldsymbol{\beta}).
\end{align}
Der Schätzer ist identisch mit dem nEnet-Schätzer von \citeA{zou_regularization_2005}, der durch eine Erweiterung des Datensatzes zu $\mathbf{X}^*=\frac{1}{\sqrt{1+\gamma_2}}\begin{pmatrix}
\mathbf{X}\\
\mathbf{I}\sqrt{\gamma_2}
\end{pmatrix}_{n \times (n+p)}$ der Lösung eines Lasso-Minimierungsproblems entspricht (für den Beweis sowie die ausführliche Darstellung siehe Anhang \ref{App_Beweis_Naiv_Zou_Darstellung}).\\ 
Die Bezeichnung "`naiver Elastic Net-Schätzer"' rührt daher, dass eine doppelte Penalisierung vorgenommen wird, indem erst ein optimales $\gamma_2$ wie bei der Ridge-Regression gesucht wird und anschließend die Lasso-Regression angewendet wird. Dadurch kommt es ohne geeignete Adjustierung zu einer zu starken Schrumpfung der Regressionskoeffizientenschätzer (siehe weiter unten).\\
In Analogie zum Lasso-Schätzer ergibt sich für ein orthogonales Design $\mathbf{X}'\mathbf{X}=\mathbf{I}$ der nEnet-Schätzer für die $j=1,\dots, p$ Kovariablen als (siehe Anhang \ref{App_nEnet_Orthogonal})
\begin{align}
\hat{\beta}_j^{nEnet}=\frac{(|\hat{\beta}_j^{OLS}|- \gamma_1 /2)^+}{1+\gamma_2}\text{sign}(\hat{\beta}_j^{OLS}).
\end{align}
Unabhängig davon, ob die RSS eine \textit{konvexe} Funktion (für den Fall $\text{rang}(\mathbf{X})< p$) oder eine \textit{strikt konvexe} Funktion (für den Fall $\text{rang}(\mathbf{X})= p$) ist, handelt es sich bei der PRSS der nEnet-Methode um eine strikt konvexe Funktion. Grund hierfür ist der Strafterm $P$, der durch die $L_2$-Norm selbst strikt konvex ist. Dadurch hat der nEnet-Schätzer eine eindeutige Lösung und Kovariablen mit identischen beobachteten Ausprägungen erhalten die selben Schätzer (für den Beweis siehe Anhang \ref{App_GroupEffect_Strict_Conv}). Darüber hinaus hat der nEnet-Schätzer einen \textit{Gruppierungseffekt}, d.h. Kovariablen, die hoch miteinander korrelieren erhalten ähnliche Schätzwerte: \begin{satz}\label{Satz_obere_Grenze_Diff_nEnet}
Es seien die Kovariablen in $\mathbf{X}$ standardisiert, die Responsevariable $\mathbf{Y}$ zentriert und die Tuningparameter $\gamma_1$ bzw. $\gamma_2$ fest. Darüber hinaus gelte $\hat{\beta}^{nEnt}_k(\gamma_1,\gamma_2) \cdot \hat{\beta}^{nEnt}_l(\gamma_1,\gamma_2) >0$ für die nEnet-Schätzer der Kovariablen $k$ und $l$ mit $k,l \in \{1,\dots,p\}$. Bezeichne $||\cdot||_q$ die $L_q$-Norm und wird die einheitenlose Differenz zwischen $\hat{\beta}^{nEnt}_k(\gamma_1,\gamma_2)$ und $\hat{\beta}^{nEnt}_l(\gamma_1,\gamma_2)$ definiert als
\begin{align*}
D_{\gamma_1,\gamma_2}(k,l)=\frac{1}{||\mathbf{Y}||_1}||\hat{\beta}_k^{nEnet}(\gamma_1,\gamma_2)-\hat{\beta}_l^{nEnet}(\gamma_1,\gamma_2)||_2,
\end{align*}
so gilt
\begin{align*}
D_{\gamma_1,\gamma_2}(k,l) \leq \frac{1}{\gamma_2}\sqrt{2(1-\rho_{k,l})}.
\end{align*}
wobei $\rho_{k,l}$ der Stichprobenkorrelation $\rho_{k,l}=\mathbf{x}_k{'}\mathbf{x}_l$ entspricht.
\end{satz}
Der nachfolgende Beweis ist eine korrigierte und ausführliche Ausarbeitung des Beweises von \citeA{zou_regularization_2005}.
\begin{proof}
Da $\boldsymbol{\hat{\beta}}^{nEnet}$ die Gleichung
\begin{align}\label{Eq_nEnt_Ableitung_beta_j}
\frac{\partial\textnormal{PRSS}_{nEnet}(\boldsymbol{\beta},\gamma_1,\gamma_2)}{\partial \beta_j}=0 \qquad \textnormal{für} \ \hat{\beta}^{nEnet}_j \neq 0 \qquad \textnormal{mit} \ j=1,\dots,p
\end{align}
erfüllt, ergibt sich aus \eqref{Eq_nEnt_Ableitung_beta_j} für die Kovariable $k$
\begin{align}\label{Eq_Diff_nEnet_Kovariable_K}
-2\mathbf{x}_k{'} \left\lbrace \mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nEnet}(\gamma_1,\gamma_2) \right\rbrace + \gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_k^{nEnet}(\gamma_1,\gamma_2)\right\rbrace + 2\gamma_2\hat{\beta}_k^{nEnet}(\gamma_1,\gamma_2)=0
\end{align}
und für die Kovariable $l$
\begin{align}\label{Eq_Diff_nEnet_Kovariable_L}
-2\mathbf{x}_l{'} \left\lbrace \mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nEnet}(\gamma_1,\gamma_2) \right\rbrace + \gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_l^{nEnet}(\gamma_1,\gamma_2)\right\rbrace + 2\gamma_2\hat{\beta}_l^{nEnet}(\gamma_1,\gamma_2)=0
\end{align}
mit $k,l = 1, \dots, p$.\\
Wegen der Annahme $\hat{\beta}^{nEnt}_k(\gamma_1,\gamma_2) \hat{\beta}^{nEnt}_l(\gamma_1,\gamma_2) >0$ muss auch\\ $\textnormal{sign}\left\lbrace\hat{\beta}^{nEnt}_k(\gamma_1,\gamma_2)\right\rbrace=\textnormal{sign}\left\lbrace\hat{\beta}^{nEnt}_l(\gamma_1,\gamma_2)\right\rbrace$ gelten, woraus für die Differenz zwischen \eqref{Eq_Diff_nEnet_Kovariable_K} und \eqref{Eq_Diff_nEnet_Kovariable_L} folgt:
\begin{align} 
&& 2(\mathbf{x}_l{'} - \mathbf{x}_k{'})\left\lbrace\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nEnet}(\gamma_1,\gamma_2)\right\rbrace+2\gamma_2\hat{\beta}_k^{nEnet}(\gamma_1,\gamma_2) - 2\gamma_2\hat{\beta}_l^{nEnet}(\gamma_1,\gamma_2)&=0 \notag \\
&\Leftrightarrow& (\mathbf{x}_l{'} - \mathbf{x}_k{'})\left\lbrace\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nEnet}(\gamma_1,\gamma_2)\right\rbrace+\gamma_2\left\lbrace\hat{\beta}_k^{nEnet}(\gamma_1,\gamma_2) - \hat{\beta}_l^{nEnet}(\gamma_1,\gamma_2)\right\rbrace&=0 \notag \\
&\Leftrightarrow& \hat{\beta}_k^{nEnet}(\gamma_1,\gamma_2) - \hat{\beta}_l^{nEnet}(\gamma_1,\gamma_2) = \frac{1}{\gamma_2} (\mathbf{x}_k{'} - \mathbf{x}_l{'}) \boldsymbol{\hat{\varepsilon}}^{nEnet} \label{Eq_Diff_nEnet_Schätzer_unnormiert},
\end{align}
wobei $\boldsymbol{\hat{\varepsilon}}^{nEnet}$ dem Residuenvektor  $\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nEnet}$ entspricht.\\
Da $\boldsymbol{\hat{\beta}}^{nEnet}$ die RSS aus \eqref{Eq_Enet_Constraint} unter Berücksichtigung der Enet-Nebenbedingung minimiert, muss 
\begin{align*}
&& \textnormal{PRSS}_{nEnet}(\boldsymbol{\hat{\beta}}^{nEnet}, \gamma_1,\gamma_2) &\leq \textnormal{PRSS}_{nEnet}(\boldsymbol{\beta}=0, \gamma_1,\gamma_2) \\
&\Leftrightarrow& ||\boldsymbol{\hat{\varepsilon}}^{nEnet}||^2_2 + \gamma_1 \sum_{1}^{p}|\beta_j^{nEnet}|+\gamma_2 \sum_{1}^{p}(\beta_j^{nEnet})^2 &\leq ||\mathbf{Y}-\mathbf{X}\mathbf{0}||^2_2=||\mathbf{Y}||^2_2
\end{align*}
und folglich, wenn $||\cdot||_2$ die $L_2$-Norm bezeichnet,
\begin{align}\label{Eq_Verhaeltnis_Enet_Resid_vs_Y2}
&& ||\boldsymbol{\hat{\varepsilon}}^{nEnet}||_2 &\leq ||\mathbf{Y}||_2 \notag \\
&\Rightarrow& \frac{||\boldsymbol{\hat{\varepsilon}}^{nEnet}||_2}{||\mathbf{Y}||_2} &\leq 1
\end{align}
gelten, weil in $\textnormal{PRSS}_{nEnet}(\boldsymbol{\hat{\beta}}^{nEnet}, \gamma_1,\gamma_2)$ alle Summanden $\geq 0$ sein müssen.\\
Wegen $\rho_{k,l} = \mathbf{x}_k{'}\mathbf{x}_l$ gilt auch
\begin{align}\label{Eq_Betrag_Diff_xl_xk}
&& ||\mathbf{x}_k - \mathbf{x}_l||^2_2&=\mathbf{x}_k{'}\mathbf{x}_k - 2\mathbf{x}_k{'}\mathbf{x}_l +\mathbf{x}_l{'}\mathbf{x}_l \notag \\
&\Leftrightarrow& &=1-2\rho_{k,l}+1 \notag \\
&\Leftrightarrow& &=2(1-\rho_{k,l}) \notag \\
&\Rightarrow& ||\mathbf{x}_k - \mathbf{x}_l||_2 &= \sqrt{2(1-\rho_{k,l})}.
\end{align}
Da für das Verhältnis der $L_1$-Norm zur $L_2$-Norm eines Vektors $\mathbf{a}$
\begin{align}\label{Eq_L1_Norm_greater_L2_Norm} 
&& ||\mathbf{a}||_2 &= \sqrt{\sum_{j=1}^{p}a_j^2} \notag \\ 
&& & \leq \sqrt{\sum_{j=1}^{p-1}a_j^2}+\sqrt{a_p^2} \notag \\ 
&& &\leq \sqrt{\sum_{j=1}^{p-2}a_j^2}+\sqrt{a_{p-1}^2}+\sqrt{a_p^2} \notag \\
&& &\leq \dots \notag \\ 
&& &\leq \sum_{j=1}^{p}\sqrt{a_j^2} =||\mathbf{a}||_1 \notag \\
&\Rightarrow& \frac{1}{||\mathbf{a}||_2} &\geq \frac{1}{||\mathbf{a}||_1}
\end{align}
und nach der \textit{Cauchy-Schwarz-Ungleichung} für zwei Vektoren $\mathbf{a}$ und $\mathbf{b}$
\begin{align}\label{Eq_Cauchy-Schwarz-Ungleichung}
||\mathbf{a}'\mathbf{b}||_2 \leq ||\mathbf{a}||_2 \cdot ||\mathbf{b}||_2
\end{align}
gilt, folgt aus
\eqref{Eq_Diff_nEnet_Schätzer_unnormiert} für die obere Grenze der Differenz $D_{\gamma_1,\gamma_2}(k,l)$:
\begin{align*}
\frac{1}{||\mathbf{Y}||_1}||\hat{\beta}_k^{nEnet}(\gamma_1,\gamma_2)-\hat{\beta}_l^{nEnet}(\gamma_1,\gamma_2)||_2 &= \frac{1}{||\mathbf{Y}||_1}|| \frac{1}{\gamma_2} (\mathbf{x}_k{'} - \mathbf{x}_l{'}) \boldsymbol{\hat{\varepsilon}}^{nEnet} ||_2\\
&\stackrel{\eqref{Eq_Cauchy-Schwarz-Ungleichung}}{\leq} \frac{1}{\gamma_2} {\frac{1}{||\mathbf{Y}||_1}} ||\boldsymbol{\hat{\varepsilon}}^{nEnet}||_2 \cdot ||\mathbf{x}_k - \mathbf{x}_l||_2 \\
&\stackrel{\eqref{Eq_L1_Norm_greater_L2_Norm}}{\leq} \frac{1}{\gamma_2} {\frac{||\boldsymbol{\hat{\varepsilon}}^{nEnet}||_2}{||\mathbf{Y}||_2}}
||\mathbf{x}_k - \mathbf{x}_l||_2 \\
&\stackrel{\eqref{Eq_Verhaeltnis_Enet_Resid_vs_Y2}}{\leq} \frac{1}{\gamma_2}
||\mathbf{x}_k - \mathbf{x}_l||_2 \\
&\stackrel{\eqref{Eq_Betrag_Diff_xl_xk}}{=} \frac{1}{\gamma_2}\sqrt{2(1-\rho_{k,l})}.
\end{align*}
\end{proof} 
Satz \ref{Satz_obere_Grenze_Diff_nEnet} verdeutlicht, dass bei einer hohen Korrelation zwischen $x_k$ und $x_l$ die Differenz der entsprechenden Regressionskoeffizientenschätzer nahe 0 ist, da $\rho_{k,l} \rightarrow 1$. Diese Eigenschaft ist besonders wünschenswert, weil sich damit eine inhaltliche Ähnlichkeit, die sich in den beobachteten Kovariablenausprägungen äußert, in den geschätzten Regressionskoeffizienten darstellt. Zusätzlich ist anhand von Satz \ref{Satz_obere_Grenze_Diff_nEnet} erkennbar, dass im Fall einer perfekte Korrelation, d.h. $\rho_{k,l}=1$, die Schätzer wegen $D_{\gamma_1,\gamma_2}(k,l)=0$ identisch sein müssen. %Die Autoren der Enet-Methode weisen darauf hin, dass für den Fall $\rho_{k,l}=-1$ einer der beiden Kovariablenvektoren mit $-1$ multipliziert werden soll, um die Darstellung aus \ref{Eq_obere_Grenze_Diff_nEnet} für die obere Grenze des Unterschieds nutzen zu können.
\\
Da durch die weiter oben angesprochene doppelte Schrumpfung die geschätzten Regressionskoeffizienten unnötig stark --- im Verhältnis zur Verringerung der Varianz --- verzerrt werden, d.h. der MSE der einzelnen $\hat{\beta}$ ansteigt, schlagen \citeA{zou_regularization_2005} eine Skalierung des nEnet-Schätzers vor, um den eigentlichen Enet-Schätzer zu erhalten:
\begin{align}\label{Eq_Enet-Schaetzer}
\boldsymbol{\hat{\beta}}^{Enet}=(1+\gamma_2)\boldsymbol{\hat{\beta}}^{nEnet}.
\end{align}
Die Wahl des Skalierungsfaktors $(1+\gamma_2)$ kann neben  empirischen begründeten Verbesserungen zum Lasso- bzw. Ridge-Schätzer \cite{zou_regularization_2005} mit einer Dekomposition des Ridge-Schätzers begründet werden. Wegen der Standardisierung der Kovariablen gilt $\mathbf{X}'\mathbf{X}=\boldsymbol{\Sigma}$. Damit lässt sich der Ridge-Schätzer aus \eqref{Eq_Ridge_estimate}, der als Summand in \eqref{Eq_Naiv_Enet_Eigene} vorkommt, darstellen als:
\begin{align}\label{Eq_Decorr_nEnet}
&& \boldsymbol{\hat{\beta}}^{Ridge}&=(\boldsymbol{\Sigma}+\gamma_2\mathbf{I})^{-1}\mathbf{X}'\mathbf{Y} \notag \\
&\Leftrightarrow& &=\begin{pmatrix}
1+\gamma_2 & \rho_{1,2}   & \dots & \rho_{1,p} \\
\rho_{2,1} & 1 + \gamma_2 &       &            \\
\vdots     &              & \ddots& \rho_{p-1,p}\\
\rho_{p,1} &              & \rho_{p, p-1}& 1 + \gamma_2
\end{pmatrix}^{-1}\mathbf{X}'\mathbf{Y} \notag \\
&\Leftrightarrow& &=\left[ (1+\gamma_2) \begin{pmatrix}
1 & \frac{\rho_{1,2}}{1+\gamma_2}&\dots&\frac{\rho_{1,p}}{1+\gamma_2} \\
\frac{\rho_{2,1}}{1+\gamma_2} & 1 &       &            \\
\vdots     &              & \ddots& \frac{\rho_{p-1,p}}{1+\gamma_2}\\
\frac{\rho_{p,1}}{1+\gamma_2} &              & \frac{\rho_{p, p-1}}{1+\gamma_2}& 1
\end{pmatrix} \right]^{-1}\mathbf{X}'\mathbf{Y} \notag \\
&\Leftrightarrow& &=\frac{1}{1+\gamma_2} \begin{pmatrix}
1 & \frac{\rho_{1,2}}{1+\gamma_2}&\dots&\frac{\rho_{1,p}}{1+\gamma_2} \\
\frac{\rho_{2,1}}{1+\gamma_2} & 1 &       &            \\
\vdots     &              & \ddots& \frac{\rho_{p-1,p}}{1+\gamma_2}\\
\frac{\rho_{p,1}}{1+\gamma_2} &              & \frac{\rho_{p, p-1}}{1+\gamma_2}& 1
\end{pmatrix}^{-1}\mathbf{X}'\mathbf{Y}.
\end{align}
Wegen $\gamma_2 \geq 0$ muss $\frac{\rho_{k,l}}{1+\gamma_2} \leq \rho_{k,l}$ gelten, weswegen der Ridge-Operator $\gamma_2\mathbf{I}$ in \eqref{Eq_Decorr_nEnet} als \textit{Dekorrelation} für den Ausdruck $(\mathbf{X}'\mathbf{X}+\gamma_2\mathbf{I})$ sowohl für den Ridge- als auch den Lasso-Summanden in \eqref{Eq_Naiv_Enet_Eigene} interpretiert werden kann. Dekorrelation bedeutet in diesem Zusammenhang, dass die Korrelationsstruktur zwischen den einzelnen Kovariablen "`aufgeweicht"' wird, d.h. beobachtete Ähnlichkeiten zwischen den Kovariablen werden als künstlich verringerte Korrelationen in der Kovarianzmatrix ausgedrückt.
Unter Berücksichtigung von \eqref{Eq_Decorr_nEnet} und mit
\begin{align*}
\boldsymbol{\Sigma}^{Decor}=\begin{pmatrix}
1 & \frac{\rho_{1,2}}{1+\gamma_2}&\dots&\frac{\rho_{1,p}}{1+\gamma_2} \\
\frac{\rho_{2,1}}{1+\gamma_2} & 1 &       &            \\
\vdots     &              & \ddots& \frac{\rho_{p-1,p}}{1+\gamma_2}\\
\frac{\rho_{p,1}}{1+\gamma_2} &              & \frac{\rho_{p, p-1}}{1+\gamma_2}& 1
\end{pmatrix}^{-1}
\end{align*}
lässt sich \eqref{Eq_Naiv_Enet_Eigene} darstellen als
\begin{align*}
\boldsymbol{\hat{\beta}}^{nEnet}=
\frac{1}{1+\gamma_2} \left\lbrace
\boldsymbol{\Sigma}^{Decor}\mathbf{X}'\mathbf{Y} - 
\boldsymbol{\Sigma}^{Decor}
\frac{\gamma_1}{2} \textnormal{sign}(\boldsymbol{\beta})
\right\rbrace,
\end{align*}
was die \textit{zusätzlich} zum Lasso-Operator bewirkte Schrumpfung verdeutlicht. Ist, wie eingangs in diesem Abschnitt beschrieben, gewünscht, dass die Vorteile von Ridge- und Lasso-Schätzer kombiniert werden, benötigt man den zusätzlichen Schrumpfungsfaktor des Ridge-Operators nicht, da bereits eine Schrumpfung und Selektion durch den Lasso-Operator bewirkt wird, die Dekorrelation des Ridge-Operators hingegen eine Stabilisierung der Schätzer gewährleistet.\\
Durch den $L_1$-Strafterm-Teil ist eine analytische Lösung des Minimierungsproblems aus \eqref{Eq_Enet_Lagrange} nicht möglich. Da, wie in Anhang \ref{App_Beweis_Naiv_Zou_Darstellung} gezeigt, das nEnet-Problem sich als Lasso-Problem darstellen lässt, kann direkt der LARS-Algorithmus verwendet werden. Durch die Erweiterung des Datensatzes auf $n+p$ Zeilen können dadurch auch mehr als $n$ Kovariablen ungleich null bei der Schätzung gesetzt werden. Allerdings erweist sich dieser Algorithmus bei hochdimensionalen Daten als relativ langsam. \citeA{zou_regularization_2005} schlagen deswegen den \textit{LARS-EN}-Algorithmus vor, der eine Erweiterung des \textit{LARS}-Algorithmus zur Lösung des Lasso-Problems (siehe \citeNP{Efron04leastangle}) ist. Für den LARS-EN-Algorithmus wird berücksichtigt, dass der erweiterte Datensatz $\mathbf{X}^*$ 
zum großen Teil spärlich besetzt ist und durch die \textit{Cholesky-Zerlegung} eine effizientere Berechnung möglich ist.\\
Da im Gegensatz zur Ridge- und Lasso-Methode \textit{zwei} Tuningparameter, $\gamma_1$ und $\gamma_2$, vom Anwender gewählt werden müssen, schlagen \citeA{zou_regularization_2005} eine $k$-fache Kreuzvalidierung auf der sich durch die beiden Tuningparameter ergeben zweidimensionalen Oberfläche vor. Dazu werden zunächst mehrere Werte für $\gamma_2$ vorgegeben und für jeden dieser Werte durch eine $k$-fache Kreuzvalidierung ein optimales $\gamma_1$ bestimmt. Anschließend wird die Kombination aus $\gamma_1$ und $\gamma_2$ gewählt, die den kleinsten Kreuzvalidierungswert liefert.\\
Ähnlich zum Lasso-Schätzer lässt sich eine Irrepresentable-Condition formulieren, durch die der Enet-Schätzer das wahre Modell auswählen kann (siehe dazu \citeNP{jia_2008Enet_Consistency}). Da diese Bedingung schwächer als die Irrepresentable-Condition der Lasso-Methode ist, kann der Enet-Schätzer noch in Fällen das wahre Modell liefern, in denen die Lasso-Methode versagt. Allerdings kann auch die Enet-Regression kein korrektes Modell mehr liefern, wenn ihre Irrepresentable-Condition verletzt ist.

% bei Lasso muss die Langrangefunktion keine strikt konvexe Funktion sein, da für n<p die Matrix X'X (also die Hessian) keinen vollen Rank besitzt und positiv semidefinit ist (alle eigenwerte sind größergleich 0), d.h. die RSS ist konvex und der Penalisierungsterm ist auch konvex (L1-Norm); summe zweier konvexen Funktionen ist wieder eine konvexe Funktion, damit aber ggf. keine eindeutigen lösungen;
%bei elastic net ist RSS zwar evtl auch nur konvex und der Lasso teil auch, aber der Ridge-Teil ist wegen L2-Norm strikt konvex und summe aus konvex und strikt konvex ist wieder strikt konvex 

%Anhand von \eqref{Eq_Naiv_Enet_Eigene} lässt sich sehen, dass Gamma1, Gamma2 Lasso Ridge STabiliserung Invertierbarkeit BLA



\subsection{Netzwerk-basierte penalisierte Regressionen}\label{Kap_Netzwerkbasierte_Penalisierung}
Netzwerkbasierte penalisierte Regressionen unterscheiden sich zu den Ansätzen aus Kapitel \ref{Kap_Ridge-Regression} bis \ref{Kap_Elastic-Net} darin, dass \textit{vorab bekannte} Informationen über den Zusammenhang von Kovariablen untereinander in den Strafterm $P(\boldsymbol{\beta},\boldsymbol{\gamma})$ aufgenommen werden. Beispielsweise liegen in der Genetik häufig Informationen über die Zusammenhänge zwischen einzelnen Genen oder Genprodukten bzw. zwischen Genen und Proteinen \textit{a priori} vor, d.h. es ist bekannt, inwieweit sie sich gegenseitig beeinflussen. Diese Abhängigkeiten können als Netzwerke beschrieben werden, die sich wiederum als Graphen darstellen lassen. In dem entsprechenden Graphen repräsentieren Knoten die Kovariablen, d.h. es gilt $x_1, \dots, x_p = v_1, \dots,v_{N_V}$, deren ggf. vorhandene Abhängigkeit durch eine Kante repräsentiert wird.

\paragraph{Grace}
Bei der ursprünglichen netzwerkbasierten penalisierten Regression \textit{Grace} (graph-bedingte Regression; englisch: graph constrained regression) haben \citeA{li_network-constrained_2008} den Strafterm als
\begin{align}\label{Eq_P-Term_Li_2008}
P_{Grace}(\boldsymbol{\beta},\boldsymbol{\gamma})=\gamma_1||\boldsymbol{\beta}||_1 + \gamma_2\boldsymbol{\beta}'\textbf{\~L}\boldsymbol{\beta}
\end{align}
definiert, wobei $\gamma_1||\boldsymbol{\beta}||_1$ dem Lasso Strafterm entspricht und $\textbf{\~L}$ die \textit{normalisierte Laplace-Matrix} eines ungerichteten Graphen $G=(V,E)$ bezeichnet. $\textbf{\~L}=(\tilde{l}_{u,v})_{N_V \times N_V}$ für $u,v \in V$ ist eine \textit{positiv semidefinite} Matrix, die definiert ist als
\begin{align*}
\tilde{l}_{u,v} = \begin{cases}
1-\frac{w(\{u,v\})}{d(u)}, \qquad &\text{wenn} \ u=v \ \text{und} \ d(u) \neq 0,\\
-\frac{w(\{u,v\})}{\sqrt{d(u)d(v)}},  \qquad &\text{wenn} \ a_{u,v} = 1,\\
 0, \qquad &\text{sonst.}\\
\end{cases}
\end{align*}
Der Knotengrad sei in diesem Fall $d(u)=\sum_{\{u,v\}}w(\{u,v\})$ mit den Kantengewichten $w(\{u,v\})$, die die Unsicherheit oder Stärke einer Kante $e=\{u,v\}$ zwischen zwei Knoten $u$ und $v$ angeben.\\
Da $\textbf{\~L}$ positiv semidefinit ist, gilt $\textbf{\~L}=\textbf{S}\textbf{S}'$ \cite{chung1996spectralgraph}, wobei $\textbf{S}$ eine $N_V \times N_E$-Matrix ist, deren Spalten bzw. Zeilen den Kanten bzw. Knoten von $G$ entsprechen. Sind zwei Knoten $u$ und $v$ adjazent, hat $\textbf{S}$ in der entsprechenden Spalte für die Kante $\{u,v\}$ in der Zeile $u$ den Eintrag $\sqrt{w(\{u,v\})}/d(u)$ und in Zeile $v$ den Eintrag $-\sqrt{w(\{u,v\})}/d(v)$. Die restlichen Zeilen sind mit nullen besetzt. Somit ergibt sich als die PRSS für Grace:
\begin{align}\label{Eq_PRSS_Grace}
\text{PRSS}_{Grace}(\boldsymbol{\beta},\gamma_1,\gamma_2)&=||\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}||_2^2+\gamma_1||\boldsymbol{\beta}||_1 +\gamma_2\boldsymbol{\beta}'\textbf{S}\textbf{S}'\boldsymbol{\beta} \notag \\
&=||\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}||_2^2+\gamma_1||\boldsymbol{\beta}||_1 \notag \\
&\phantom{{}=1} +\gamma_2 \left\lbrace \sum_{\{u,v\}} \left(\frac{\beta_u}{\sqrt{d(u)}}-\frac{\beta_v}{\sqrt{d(v)}}\right)^2 w(\{u,v\})\right\rbrace, 
\end{align}
die für die Schätzung von $\boldsymbol{\hat{\beta}}^{Grace}$ bezüglich $\boldsymbol{\beta}$ minimiert werden muss:
\begin{align}\label{Eq_Grace_Constrained}
\boldsymbol{\hat{\beta}}^{Grace} &= \arg \displaystyle\min_{\beta} \text{PRSS}_{Grace}(\boldsymbol{\beta},\gamma_1,\gamma_2).
\end{align}
Wegen des Lasso-Operators kann die Grace-Regression wie die Enet-Regression eine Variablenselektion durchführen und da $\gamma_2\boldsymbol{\beta}'\textbf{S}\textbf{S}'\boldsymbol{\beta}$ eine strikt konvexe Funktion ist, handelt es sich in \eqref{Eq_Grace_Constrained} für $\gamma_2\boldsymbol{\beta}'\textbf{S}\textbf{S}'\boldsymbol{\beta} > 0$ um ein strikt konvexes Optimierungsproblem und es kommt zu einem Gruppierungseffekt bei der Schätzung (siehe Anhang \ref{App_GroupEffect_Strict_Conv}).\\
Der Strafterm aus \eqref{Eq_PRSS_Grace} weist eine Ähnlichkeit zu dem Fused-Lasso-Strafterm
\begin{align*}
\text{PRSS}_{fLasso}(\boldsymbol{\beta}, \gamma_1, \gamma_2)=||\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}||_2^2+\gamma_1||\boldsymbol{\beta}||_1 + \gamma_2 \sum_{j=2}^{p}|\beta_j - \beta_{j-1}|
\end{align*}
auf \cite{Tibshirani05fused}, für den die Kovariablen zuvor in eine inhaltlich begründete Reihenfolge gebracht werden. Bei der Fused-Lasso-Regression wird angenommen, dass benachbarte Koeffizienten identische Steigungskoeffizienten haben, und für den Fall von unterschiedlichen Steigungskoeffizienten wird der Strafterm erhöht. Dadurch kommt es zu einer Glättung des geschätzten Parameters $\hat{\beta}_j$ über seine Nachbarn $\hat{\beta}_{j-1}$ und $\hat{\beta}_{j+1}$. Die Grace-Regression beinhaltet dagegen eine abgeschwächte Annahme bzgl. der Regressionskoeffizienten von benachbarten Variablen. Diese können auch unterschiedlich große Steigungskoeffizienten haben, ohne dass der Strafterm ansteigt, wenn entsprechend die Knotengrade der jeweiligen Knoten unterschiedlich sind. Inhaltlich lässt sich der Strafterm so interpretieren, dass mit den Knotengraden bei der Differenz zwischen zwei Steigungskoeffizienten berücksichtigt wird, dass jeder Knoten selbst wiederum in einem Zusammenhang mit mehreren anderen Knoten stehen kann und die Abweichung der beiden Regressionskoeffizienten voneinander durch die Nähe zu weiteren Kovariablen hervorgerufen wird. Dadurch können Variablen, die mit vielen anderen Variablen "`verbunden"' sind, bei der Schätzung größere Regressionskoeffizienten als ihre Nachbarn erhalten, die weniger Kanten haben. Wird der Knotengrad als Maß für die Bedeutung einer Variable interpretiert, kann sich die Bedeutung von Kovariablen in höheren geschätzten Steigungskoeffizienten als der ggf. weniger bedeutsamen Nachbarn widerspiegeln.\\
Wird \eqref{Eq_PRSS_Grace} abgeleitet und null gesetzt, ergibt sich der naive Grace-Schätzer
\begin{align}\label{Eq_Grace_PRSS_Ableitung}
&&\frac{\partial \textnormal{PRSS}_{Grace}(\boldsymbol{\beta}, \gamma_1, \gamma_2)}{\partial \boldsymbol{\beta}}&=-2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma_1 \textnormal{sign}(\boldsymbol{\beta}) + 2\gamma_2 \mathbf{S}\mathbf{S}'\mathbf{\boldsymbol{\beta}} \notag \\
&\Rightarrow& 0 &= -2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma_1 \textnormal{sign}(\boldsymbol{\beta}) + 2\gamma_2 \mathbf{S}\mathbf{S}'\mathbf{\boldsymbol{\beta}} \notag \\
&\Rightarrow& \boldsymbol{\hat{\beta}}^{nGrace}&=(\mathbf{X}'\mathbf{X}+\gamma_2\mathbf{S}\mathbf{S}')^{-1}\mathbf{X}'\mathbf{Y} - (\mathbf{X}'\mathbf{X}+\gamma_2\mathbf{S}\mathbf{S}')^{-1}\frac{\gamma_1}{2}\text{sign}(\boldsymbol{\beta}).
\end{align}
Anhand dieser Darstellung wird sichtbar, dass sich für $\mathbf{S}\mathbf{S}'=\mathbf{I}$ der nEnet-Schätzer aus \eqref{Eq_Naiv_Enet_Eigene} und für $\mathbf{S}\mathbf{S}'=\mathbf{0}_{p \times p}$ der Lasso-Schätzer aus \eqref{Eq_Lasso-Schätzer_Allgemein} ergibt. In Analogie zur nEnet-Regression lässt sich durch eine Erweiterung des Datensatzes zu $\mathbf{X}^*=\frac{1}{\sqrt{1+\gamma_2}}\begin{pmatrix}
\mathbf{X}\\
\sqrt{\gamma_2}\mathbf{S}'
\end{pmatrix}$ das Minimierungsproblem aus \eqref{Eq_Grace_Constrained} als Lasso-Problem darstellen. Als Lösung ergibt sich wie bei der nEnet-Regression $\boldsymbol{\hat{\beta}}^{nGrace}=\frac{1}{\sqrt{1+\gamma_2}}\boldsymbol{\hat{\beta}}^*$ (vgl. dazu \ref{App_Beweis_Naiv_Zou_Darstellung}). Durch den erweiterten Datensatz auf $n+p$ Zeilen kann die Lösung ggf. alle $p$ Regressionskoeffizienten ungleich null setzen. Wegen der ebenfalls doppelten Schrumpfung durch den $L_1$- und $L_2$-Strafterm, schlagen \citeA{li_network-constrained_2008} wie bei dem Enet-Schätzer $(1+\gamma_2)$ als Korrekturfaktor vor. Damit ergibt sich $\boldsymbol{\hat{\beta}}^{Grace}=\sqrt{1+\gamma_2}\boldsymbol{\hat{\beta}}^*$ für den Grace-Schätzer.
Für die Wahl von $\gamma_1$ und $\gamma_2$ empfehlen die Autoren das selbe Vorgehen wie bei der Enet-Regression, d.h. eine Kreuzvalidierung über beide Tuningparameter (siehe dazu Abschnitt \ref{Kap_Elastic-Net}).\\
In Analogie zum Enet-Schätzer lässt sich für zwei mittels Grace geschätzte Regressionskoeffizienten $\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2)$ und $\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)$ eine obere Grenze
\begin{align}\label{Eq_Obere_Grenze_nGrace}
D_{\gamma_1,\gamma_2}(k,l) \leq \frac{1}{2\gamma_2}\sqrt{2(1-\rho_{k,l})}
\end{align}
für die Differenz 
\begin{align}
D_{\gamma_1,\gamma_2}(k,l)=\frac{1}{||\mathbf{Y}||_1}||\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2)- \hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)||_2
\end{align}
der beiden Regressionskoeffizienten angeben (für den Beweis siehe Anhang \ref{App_obere_Grenze_Diff_nGrace}). Bedingung für diese Darstellung ist, dass nur die beiden Knoten miteinander verbunden sind, d.h. $d(k)=d(l)=w(\{k,l\})$, und $\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2) \cdot \hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2) > 0$ gilt. Im Hinblick auf die Ähnlichkeit von hoch miteinander korrelierenden Kovariablen wird anhand von \eqref{Eq_Obere_Grenze_nGrace} deutlich, dass durch den Einbezug der vorab bekannten \textit{inhaltlichen} Information über den Zusammenhang von $k$ und $l$ die entsprechenden Schätzer noch besser aneinander angeglichen werden als bei der Enet-Schätzung.\\
Wird $p$ fest gehalten und seien die Tuningparameter als $\frac{\gamma_{1,n}}{\sqrt{n}} \rightarrow \gamma_{1,0} \geq 0$ sowie $\frac{\gamma_{2,n}}{\sqrt{n}} \rightarrow \gamma_{2,0} \geq 0$ von $n$ abhängige Funktionen definiert, gilt mit wachsender Stichprobengröße $n \rightarrow \infty$ folgende asymptotische Eigenschaft des Grace-Schätzers (für den Beweis siehe Anhang \ref{App_Asymp_Grace_Proof}):
\begin{align*}
\sqrt{n}(\boldsymbol{\hat{\beta}}_{(n)}^{Grace} - \boldsymbol{\beta}) \xrightarrow{d} \arg \min (L)
\end{align*}
mit
\begin{align}\label{Eq_Vn_asymptotic}
L(\mathbf{u})&=-2\mathbf{u}'\mathbf{W} + \mathbf{u}'\mathbf{C}\mathbf{u} \notag \\
&\phantom{{}=1} + \gamma_{1,0}\sum_{j=1}^{p}\left\lbrace u_j \textnormal{sign}(\beta_j)I(\beta_j \neq 0) + |u_j|I(\beta_j=0) \right\rbrace \notag \\
&\phantom{{}=1} + 2\gamma_{2,0} \sum_{\{k,l\}} \left\lbrace \left( \frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}\right)
\left( \frac{u_k}{\sqrt{d(k)}} - \frac{u_l}{\sqrt{d(l)}}\right)w(\{k,l\})
\right\rbrace.
\end{align}
Dabei gelte
\begin{align*}
\mathbf{u}&=(u_1, \dots , u_p)' \\
\mathbf{C}&=\lim\limits_{n \rightarrow \infty} \left( \frac{1}{n} \sum_{i=1}^{n} \mathbf{x}_i \mathbf{x}_i^{'}\right) \qquad \text{mit} \ \mathbf{x}_i=(x_{i,1}, \dots , x_{i,p})' \\
\mathbf{W}&\sim N(0, \sigma^2 \mathbf{C})
\end{align*}
und $I(\cdot)$ sei die Indikatorfunktion.\\
Anhand von \eqref{Eq_Vn_asymptotic} wird deutlich, dass $\sqrt{n}(\boldsymbol{\hat{\beta}}_{(n)}^{Grace} - \boldsymbol{\beta})$ nur dann nach der durch die netzwerk-basierte Penalisation bestimmten Verteilung konvergiert, wenn $\gamma_{1,n}$ und $\gamma_{2,n}$ mit steigendem $n$ schneller wachsen als $\sqrt{n}$, d.h. es gilt $\gamma_{1,n}, \gamma_{2,n} \in \mathcal{O}(\sqrt{n})$. Für den Fall, dass die Tuningparameter mit steigendem $n$ langsamer wachsen als $\sqrt{n}$, d.h. $\gamma_{1,n}, \gamma_{2,n} \in o(\sqrt{n})$, konvergiert $\sqrt{n}(\boldsymbol{\hat{\beta}}_{(n)}^{Grace}-\boldsymbol{\beta})$ nach Verteilung gegen die Normalverteilung $N(\mathbf{0}, \sigma^2 \mathbf{C}^{-1})$, da die Penalisierungsterme in \eqref{Eq_Vn_asymptotic} wegfallen (vlg. dazu \citeNP{fahrmeir2009regression}).

\paragraph{aGrace}
Ein wesentlicher Nachteil an der Grace-Regression ist die zusätzliche Abhängigkeit der Höhe des Strafterms vom Vorzeichen der Regressionskoeffizienten. Für den Fall, dass die Regressionskoeffizienten von zwei im Netzwerk adjazente Kovariablen $u$ und $v$ gegensätzliche Vorzeichen haben, gleichzeitig aber $|\beta_u|=|\beta_v|$ gilt, kommt es in \eqref{Eq_PRSS_Grace} dennoch zu einer Erhöhung des Starfterms und damit zu einer unnötig hohen Schrumpfung der geschätzten Regressionskoeffizienten.\\
Um diesem Effekt entgegenzuwirken, schlagen \citeA{li_variable_2010} eine modifizierte normierte Laplace-Matrix $\textbf{\~L}^*=(\tilde{l}^*_{u,v})_{N_V \times N_V}$ vor:
\begin{align*}
\tilde{l}^*_{u,v} = \begin{cases}
1-\frac{w(\{u,v\})}{d(u)}, \qquad &\text{wenn} \ u=v \ \text{und} \ d(u) \neq 0,\\
-\frac{\text{sign}(\tilde{\beta}_u)\text{sign}(\tilde{\beta}_v)w(\{u,v\})}{\sqrt{d(u)d(v)}},  \qquad &\text{wenn} \ a_{u,v} = 1,\\
 0, \qquad &\text{sonst,}\\
\end{cases}
\end{align*}
womit sich die PRSS ergibt als:
\begin{align}
\text{PRSS}_{aGrace}(\boldsymbol{\beta},\gamma_1,\gamma_2)&=||\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}||_2^2+\gamma_1||\boldsymbol{\beta}||_1 +\gamma_2\boldsymbol{\beta}'\textbf{\~L}^*\boldsymbol{\beta} \notag \\
&=||\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}||_2^2+\gamma_1||\boldsymbol{\beta}||_1 \notag \\
&\phantom{{}=1} +\gamma_2 \left\lbrace \sum_{\{u,v\}} \left(\frac{\text{sign}(\tilde{\beta}_u)\beta_u}{\sqrt{d(u)}}-\frac{\text{sign}(\tilde{\beta}_v)\beta_v}{\sqrt{d(v)}}\right)^2 w(\{u,v\})\right\rbrace.
\end{align}
Die Vorzeichen
$\text{sign}(\tilde{\beta}_u)$ und  $\text{sign}(\tilde{\beta}_v)$ der Kovariablen werden dabei mittels OLS-Regression für $p<n$ bzw. Enet-Regression für $p \geq n$ geschätzt. Dadurch wird allerdings jeder Fehler bei der Vorzeichenschätzung, der bei nicht erfüllter Irrepresentable-Condition auftreten kann (siehe \ref{Kap_Lasso-Regression} und \ref{Kap_Elastic-Net}), direkt in den Strafterm übernommen und der Starfterm der aGrace-Methode fälschlich erhöht.

\paragraph{2Step}
\citeA{pan_incorporating_2010} schlagen einen Netzwerk-basierten Nebenbedingungsterm
\begin{align*}
&\arg \displaystyle\min_{\boldsymbol{\beta}}  \sum_{i=1}^{n} \left( Y_i - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2 \notag \\  
\text{s.d.} \ &\sum_{\{u,v\}} \left[ \left(\frac{|\beta_u|}{c_u} \right)^\xi + \left(\frac{|\beta_v|}{c_v} \right)^\xi \right]^{\frac{1}{\xi}} \leq t,
\end{align*}
mit dem Norm-Parameter $\xi$, einem Restriktionsparameter $t$ und den Knotengewichten $c_u$ vor. Mit $\xi=\infty$ und wegen $||\cdot||_\infty := \max (|x_1|, \dots, |x_p|)$ gilt 
\begin{align}\label{Eq_Net_Pen_Upsilon_Norm_INFTY}
&\arg \displaystyle\min_{\boldsymbol{\beta}}  \sum_{i=1}^{n} \left( Y_i - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2 \notag \\  
\text{s.d.} \ &\sum_{\{u,v\}} \max \left( \frac{|\beta_u|}{c_u}, \frac{|\beta_v|}{c_v} \right) \leq t,
\end{align}
womit \textit{gleichzeitig} ein Gruppierungseffekt und eine Variablenselektion vorgenommen werden. Dabei handelt es sich um die beabsichtigten Eigenschaften des Schätzers und die Autoren erhalten in ihrer Simulationsstudie verglichen mit Werten $\xi < \infty$ die besten Schätzungen hinsichtlich der richtig-positiv-Rate. Allerdings erweisen sich die einzelnen Schätzer in den Simulationen im Vergleich zu anderen Methoden wie dem Enet als stärker verzerrt. Eine intuitive Begründung hierfür ist der Umstand, dass \eqref{Eq_Net_Pen_Upsilon_Norm_INFTY} mit dem Bestrafungsterm keine Unterscheidung zwischen Selektion und Glättung über zusammengehörige Parameter vornehmen kann. Dadurch können geschätzte Regressionskoeffizienten auch dann selektiert werden, wenn sie in Wahrheit nur angeglichen werden müssen. Umgekehrt werden sie ggf. bloß geglättet, obwohl sie tatsächlich null sind.\\
Um die Selektion und Gruppierung getrennt durchzuführen, schlagen
\citeA{luo_two-step_2012} basierend auf \eqref{Eq_Net_Pen_Upsilon_Norm_INFTY} und einer modifizierten aGrace-Nebenbedingung
\begin{align*}
\sum_{\{u,v\}} \left| \frac{\text{sign}(\tilde{\beta}_u)\beta_u}{\sqrt{d(u)}}-\frac{\text{sign}(\tilde{\beta}_v)\beta_v}{\sqrt{d(v)}}\right|
\end{align*}
eine zweistufige Penalisierung vor. Dabei wird im ersten Schritt unter Verwendung von \eqref{Eq_Net_Pen_Upsilon_Norm_INFTY} $\boldsymbol{\beta}$ als $\boldsymbol{\hat{\beta}}^{(1)}$ durch eine zehnfache Kreuzvalidierung über $t$ geschätzt. Auf Grundlage dieser Schätzung werden alle Kanten $e=\{u,v\}$ aus $G=(V,E)$ aus dem Graphen entfernt, für deren zugehörige Regressionskoeffizienten $\hat{\beta}_u^{(1)}=\hat{\beta}_v^{(1)}=0$ gilt. Dadurch wird ein in der Größe reduzierter Graph $G^{neu}=(V,E^{neu})$ geschätzt und $\forall e \in E^{neu}$
\begin{align*}
s^{(0)}_{\{u,v\}}=\begin{cases}
1, &\qquad \text{falls} \ \hat{\beta}_u \cdot \hat{\beta}_v > 0 \\
-1, &\qquad \text{sonst},
\end{cases}
\end{align*} 
eine Vorzeichenvariable $s^{(0)}$ bestimmt. Im zweiten Schritt wird eine erneute Regressionskoeffizientenschätzung mit der modifizierten aGrace-Nebenbedingung über den Restriktionsparameter $r$ mit einer zehnfachen Kreuzvalidierung durchgeführt:
\begin{align}\label{Eq_2nd_step_2step}
&\arg \displaystyle\min_{\boldsymbol{\beta}} \sum_{i=1}^{n} \left( Y_i - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2 \notag \\  
\text{s.d.} \ &\sum_{\{u,v\} \in E^{neu}} \left| \frac{\beta_u}{c_u} - s^{(0)}_{\{u,v\}} \frac{\beta_v}{c_v} \right| \leq r,
\end{align}
mit  $\beta_j=0$, falls $j \in e \in E\setminus E^{neu}$. Die Autoren der 2Step-Methode geben allerdings nicht an, inwieweit sich die fälschliche Entfernung von Kanten aus dem Graphen im ersten Schritt auf die Genauigkeit bzw. Variablenselektion im zweiten Schritt auswirkt. Insbesondere die Bedingung, dass alle Koeffizienten $\beta_j=0$ gesetzt werden, falls $j \in e \in E\setminus E^{neu}$ gilt, könnte zu streng sein, weil demnach "`isolierte"' Kovariablen keinen Einfluss auf die Responsevariable haben.

\paragraph{$\text{NBPR}_c$} Die Annahme der Grace-, aGrace- sowie 2Step-Regression, dass mit einer Knotengewichtung $c$ für zwei adjazente Kovariablen $u$ und $v$ die Gleichheit $\beta_u/c_u=\beta_v/c_v$ bzw. $|\beta_u|/c_u=|\beta_v|/c_v$  gilt und andernfalls bei der Schätzung eine Erhöhung des Starfterms vorgenommen werden muss, ist dahingehend idealisiert, dass allen Kovariablen unter Berücksichtigung ihrer Bedeutung im Netzwerk derselbe Einfluss auf die Responsevariable unterstellt wird. Beispielsweise gibt es keine begründete Annahme, warum in biologischen Netzwerken zwei benachbarte und gleich zentrale Gene denselben Einfluss auf die Responsevariable haben sollen. \citeA{kim_network-based_2013} schlagen daher die abgeschwächte Annahme $I(\beta_u \neq 0)=I(\beta_v \neq 0)$ vor, bei der lediglich unterstellt wird, dass, wenn Kovariable $u$ einen Einfluss hat, auch ihre im Netzwerk benachbarte Kovariable $v$ einen Einfluss hat. Als \textit{idealen} Strafterm definieren die Autoren 
\begin{align}\label{Eq_NBPR_idealer_Strafterm}
P_{iNBPR}(\boldsymbol{\beta}, \gamma_1, \gamma_2)=\gamma_1 \sum_{j=1}^{p}I(|\beta_j|\neq 0) + \gamma_2 \sum_{\{u,v\}}\left| I\left( \frac{|\beta_u|}{c_u} \neq 0 \right) - I\left( \frac{|\beta_v|}{c_v} \neq 0 \right) \right|.
\end{align}
Da es sich bei der Indikatorfunktion $I(\cdot)$ um eine nicht-stetige Funktion handelt, ist die Minimierung von \eqref{Eq_NBPR_idealer_Strafterm} nicht in Polynomialzeit, d.h. $\mathcal{O}(p^r)$ mit konstantem Faktor $r$, lösbar. Allein die Anzahl an möglichen Lösungen für den ersten Term wächst exponentiell mit $p$, so dass $2^p$ mögliche Lösungen berechnet werden müssen und damit bei hochdimensionalen Daten \textit{praktisch} das Minimierungsproblem nicht lösbar ist.\\
Als Alternative für eine Indikatorfunktion $I(|z| \neq 0)$ mit einer Variable $z$ kann die stetige Funktion $J_\tau(|z|)=\min (\frac{|z|}{\tau}, 1)$ mit Tuningparameter $\tau > 0$ genutzt werden, da diese für $\tau \rightarrow 0$ die Indikatorfunktion approximiert. $P_{NBPR}(\boldsymbol{\beta}, \gamma_1, \gamma_2)$ lässt sich damit schreiben als
\begin{align}\label{Eq_NBPR_tau_Strafterm}
P_{NBPR_c}(\boldsymbol{\beta}, \gamma_1, \gamma_2, \tau)=\gamma_1 \sum_{j=1}^{p}J_\tau(|\beta_j|) + \gamma_2 \sum_{\{u,v\}}\left| J_\tau\left( \frac{|\beta_u|}{c_u}\right) - J_\tau\left( \frac{|\beta_v|}{c_v}\right) \right|.
\end{align}
Während die Knotengewichtung $c$ in \eqref{Eq_NBPR_idealer_Strafterm} vernachlässigt werden kann, muss sie in \eqref{Eq_NBPR_tau_Strafterm} berücksichtigt werden, weil darüber eine Relativierung der Regressionskoeffizientengröße durch deren Bedeutung im Netzwerk erreicht und die Auswertung von $J_\tau$ bestimmt wird. Durch die Verwendung der nicht-konvexen Funktion $J_\tau$ ist auch  \eqref{Eq_NBPR_tau_Strafterm} eine nicht-konvexe Funktion, für die es demnach kein eindeutiges Minimum gibt. Die Autoren verwenden deswegen einen \textit{difference convex programming-algorithm} (\textit{DCPA}), bei dem eine nicht-konvexe Funktion sich als Differenz zweier konvexer Funktionen darstellen lässt. Die DCPA-Darstellung von $J_\tau(|z|)=\min (\frac{|z|}{\tau}, 1)$ ist die um den Faktor $\frac{1}{\tau}$ multiplizierte Differenz der konvexen Funktionen $|z|$ und $\max(|z|-\tau,0)$. Mit $|z|=|\beta_j|$ können für die einzelnen Summanden $P_{1,NBPR_c}^{(j)}$ des ersten Teils des Strafterms folgende Fälle unterschieden werden:
\begin{align*}
P_{1,NBPR_c}^{(j)}=\frac{1}{\tau}\left(|\beta_j|-\max(|\beta_j|-\tau, 0)\right)=\begin{cases}
1, &\qquad \text{für} \ \tau \leq |\beta_j| \\
\frac{|\beta_j|}{\tau}, &\qquad \text{für} \ \tau > |\beta_j|.
\end{cases}
\end{align*}
Gilt $\tau \rightarrow 0$ nähert sich $P_{1,NBPR_c}^{(j)}$ den einzelnen Indikatorfunktionen des ersten Terms des Strafterms aus \eqref{Eq_NBPR_idealer_Strafterm} an.\\
Mit $c>0$ ergibt sich für den zweiten Teil aus \eqref{Eq_NBPR_tau_Strafterm} unter Verwendung des DCPA für die einzelnen Summanden $P_{2,NBPR_c}^{(\{u,v\})}$:
\begin{align}
P_{2,NBPR_c}^{(\{u,v\})}&=\left| \frac{1}{\tau}\left(\frac{|\beta_u|}{c_u}-\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)\right) -  \frac{1}{\tau}\left(\frac{|\beta_v|}{c_v}-\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right)\right) \right| \notag \\
&=\frac{1}{\tau}\left|
\left(\frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right)\right) -  \left(\frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)\right)
\right| \notag \\
&= \frac{1}{\tau} \left\lbrace 
2\max \left[\frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right), \frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)\right] \right. \notag \\
&\phantom{{}=1} \left. 
- \left[ \frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right) +  \frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)
\right] \right\rbrace
\end{align}
und damit als zu unterscheidende Fälle:
\begin{align}
P_{2,NBPR_c}^{(\{u,v\})}=\begin{cases}
\frac{1}{\tau}\left(\frac{|\beta_u|}{c_u}-\frac{|\beta_v|}{c_v}\right), 
&\qquad \text{für} \ \tau \geq \frac{|\beta_u|}{c_u} > \frac{|\beta_v|}{c_v} \notag \\
\frac{1}{\tau}\left(\frac{|\beta_v|}{c_v}-\frac{|\beta_u|}{c_u}\right), 
&\qquad \text{für} \ \tau \geq \frac{|\beta_v|}{c_v} > \frac{|\beta_u|}{c_u} \notag \\
0, 
&\qquad \text{für} \ \tau \geq \frac{|\beta_v|}{c_v} = \frac{|\beta_u|}{c_u} \notag \\
0, 
&\qquad \text{für} \ \tau < \frac{|\beta_v|}{c_v} = \frac{|\beta_u|}{c_u} \notag \\
1-\frac{|\beta_v|}{\tau \cdot c_v}, 
&\qquad \text{für} \ \frac{|\beta_u|}{c_u} > \tau > \frac{|\beta_v|}{c_v} \notag \\
1-\frac{|\beta_u|}{\tau \cdot c_u}, 
&\qquad \text{für} \ \frac{|\beta_v|}{c_v} > \tau > \frac{|\beta_u|}{c_u}.
\end{cases}
\end{align}
Die Fallunterscheidung macht deutlich, dass sich für $\tau \rightarrow 0$ die Differenz des zweiten Strafterms in \eqref{Eq_NBPR_tau_Strafterm} der Differenz der Indikatorfunktionen in \eqref{Eq_NBPR_idealer_Strafterm} annähert.\\
Der gesamte Strafterm lässt sich damit als Differenz zweier konvexer Funktionen darstellen:
\begin{align}\label{Eq_NBPR_Diff_Konvexe_Funktionen}
P_{NBPR_c}(\boldsymbol{\beta},\gamma_1, \gamma_2, \tau)
&=\frac{\gamma_1}{\tau} \sum_{j=1}^{p}\left\lbrace |\beta_j|-\max(|\beta_j|-\tau, 0)\right\rbrace  \notag \\
	&\phantom{{}=0} 
	+ \frac{\gamma_2}{\tau} \sum_{\{u,v\}} \left\lbrace 
	2\max \left[
	\frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right), \frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)
	\right] 
	\right. \notag \\
	&\phantom{{}=11} 
	\left. 
	- \left[ \frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right) +  \frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right) \right] \right\rbrace \notag \\
&=\frac{1}{\tau} 
\left\lbrace 
\gamma_1 \sum_{j=1}^{p}|\beta_j| \right. \notag \\ 
	&\phantom{{}=11} \left.
	+ \gamma_2 \sum_{\{u,v\}} 2\max \left[\frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right), \frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)\right] \right\rbrace \notag \\
	&\phantom{{}=0} 
	- \frac{1}{\tau}
	\left\lbrace \gamma_1 \sum_{j=1}^{p}\max(|\beta_j|-\tau, 0) \right. \notag \\
	&\phantom{{}=11} \left.
	+ \gamma_2 \sum_{\{u,v\}} \left[ \frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right) +  \frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right) \right]
	\right\rbrace
\end{align}
Wird die zweite konvexe Funktion aus \eqref{Eq_NBPR_Diff_Konvexe_Funktionen} durch einen zuvor erhaltenen Schätzer $\hat{\beta}^{(t-1)}$ angenähert, sodass für den ersten Teil
\begin{align*}
\max(|\hat{\beta}_j^{(t-1)}|-\tau, 0)
\end{align*}
und für den zweiten Teil
\begin{align*}
\frac{\beta_u}{c_u}\text{sign}(\hat{\beta}_u^{(t-1)})+\max\left(\frac{|\hat{\beta}_v^{(t-1)}|}{c_v}-\tau, 0\right) +  \frac{\beta_v}{c_v}\text{sign}(\hat{\beta}_v^{(t-1)})+\max\left(\frac{|\hat{\beta}_u^{(t-1)}|}{c_u}-\tau, 0\right)
\end{align*}
und damit für $\tau \rightarrow 0$ 
\begin{align*}
&\frac{\gamma_1}{\tau} \sum_{j=1}^{p}|\beta_j| - \frac{\gamma_1}{\tau} \sum_{j=1}^{p}\max(|\hat{\beta}_j^{(t-1)}|-\tau, 0) \\ 
\overset{a}{\sim} & \frac{\gamma_1}{\tau} \sum_{j=1}^{p}|\beta_j| - \sum_{j=1}^{p}\left(|\beta_j|\cdot I(|\hat{\beta}_j^{(t-1)}|>\tau)\right)\\
=&\frac{\gamma_1}{\tau} \sum_{j=1}^{p}\left(|\beta_j|\cdot I(|\hat{\beta}_j^{(t-1)}| \leq \tau)\right)
\end{align*}
sowie
\begin{align*}
&\frac{\gamma_2}{\tau} \sum_{\{u,v\}} \left\lbrace
\frac{\beta_u}{c_u}\text{sign}(\hat{\beta}_u^{(t-1)})+\max\left(\frac{|\hat{\beta}_v^{(t-1)}|}{c_v}-\tau, 0\right) \right. \\
&\phantom{{}=111} \left. +  \frac{\beta_v}{c_v}\text{sign}(\hat{\beta}_v^{(t-1)})+\max\left(\frac{|\hat{\beta}_u^{(t-1)}|}{c_u}-\tau, 0\right)
\right\rbrace \\
\overset{a}{\sim}&\frac{\gamma_2}{\tau} \sum_{\{u,v\}} \left\lbrace
\frac{\beta_u}{c_u}\text{sign}(\hat{\beta}_u^{(t-1)}) \left[1+I\left(\frac{|\hat{\beta}_u^{(t-1)}|}{c_u}>\tau \right) \right] \right. \\
&\phantom{{}=111} \left. + 
\frac{\beta_v}{c_v}\text{sign}(\hat{\beta}_v^{(t-1)})\left[1+I\left(\frac{|\hat{\beta}_v^{(t-1)}|}{c_v}>\tau \right) \right]
\right\rbrace
\end{align*}
gilt, ergibt sich als PRSS für den Schätzdurchlauf $t$:
\begin{align}\label{Eq_NBPR_round_t}
PRSS_{NBPR_c}^{(t)}(\boldsymbol{\beta}, \gamma_1, \gamma_2,\tau)&=\sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j )^2 + \frac{\gamma_1}{\tau} \sum_{j=1}^{p}\left(|\beta_j|\cdot I(|\hat{\beta}_j^{(t-1)}| \leq \tau)\right) \notag \\
&\phantom{{}=0} + \gamma_2 \sum_{\{u,v\}} 2\max \left[\frac{|\beta_u|}{c_u}+\max\left(\frac{|\beta_v|}{c_v}-\tau, 0\right), \frac{|\beta_v|}{c_v}+\max\left(\frac{|\beta_u|}{c_u}-\tau, 0\right)\right] \notag \\
&\phantom{{}=0} - \frac{\gamma_2}{\tau} \sum_{\{u,v\}} \left\lbrace
\frac{\beta_u}{c_u}\text{sign}(\hat{\beta}_u^{(t-1)}) \left[1+I\left(\frac{|\hat{\beta}_u^{(t-1)}|}{c_u}>\tau \right) \right] \right. \notag \\
&\phantom{{}=1111} \left. + 
\frac{\beta_v}{c_v}\text{sign}(\hat{\beta}_v^{(t-1)})\left[1+I\left(\frac{|\hat{\beta}_v^{(t-1)}|}{c_v}>\tau \right) \right]
\right\rbrace.
\end{align}
Für fest vorgegebene Tuningparameter $\gamma_1$, $\gamma_2$, $\tau$, einer Toleranzgrenze $\epsilon$ sowie dem Knotenzentralitätsmaß $c$ ergibt sich als Lösungsalgorithmus zur Bestimmung von $\boldsymbol{\hat{\beta}}^{NBPR_c}$:
\begin{algorithm}[H]\label{Alg_NBPR}
$1.$ Minimiere $PRSS_{NBPR_c}^{(t)}(\boldsymbol{\beta})$ für $t=1$ und einem vorgegeben Startschätzer $\boldsymbol{\hat{\beta}}^{(0)}$ \\ 
$2.$ Berechne $\boldsymbol{\hat{\beta}}^{(t)}$, das $PRSS_{NBPR_c}^{(t)}(\boldsymbol{\beta})$ für Iteration $t$ mit $t > 1$ minimiert.\\
$3.$ Stoppe, falls $PRSS_{NBPR_c}^{(t-1)}(\boldsymbol{\hat{\beta}}) - PRSS_{NBPR_c}^{(t)}(\boldsymbol{\hat{\beta}}) \leq \epsilon$, sonst gehe zu Punkt 2.
\end{algorithm}
Zur Bestimmung der Tuningparameter $\gamma_1$, $\gamma_2$ und $\tau$ schlagen die Autoren der $\text{NBPR}_c$-Methode eine Suche über verschiedene Kombinationen aus zuvor festgelegten Tuningparametern vor. Dabei wird diejenige Kombination gewählt, für die $\sum_{i=1}^{n}(\hat{Y}_i-Y_i)^2$ am geringsten ist (siehe dazu auch Abschnitt \ref{Schätzung mittels NBPR}).


\chapter{Simulationsstudie}\label{Kap_Simulationsstudie}
Die Regressionskoeffizientenschätzer basierend auf der $\text{NBPR}_c$-Methode werden im nachfolgenden Kapitel mit der Enet-Methode hinsichtlich der richtig-positiven-Rate (RP-Rate) bzw. falsch-positiven-Rate (FP-Rate) sowie bezüglich dem als Modellfehler definierten Maß 
\begin{align*}
\text{MF}=(\boldsymbol{\hat{\beta}}- \boldsymbol{\beta})'\mathbb{C}\text{ov}(\mathbf{X})(\boldsymbol{\hat{\beta}}- \boldsymbol{\beta})
\end{align*}
und dem Vorhersagefehler
\begin{align*}
\text{PE}=(\mathbf{\hat{Y}}- \mathbf{Y})'(\mathbf{\hat{Y}}- \mathbf{Y})
\end{align*}
verglichen.\\
Dazu wird im Rahmen dieser Arbeit eine Simulationsstudie mit $100$ pseudozufällig generierten Datensätzen durchgeführt. Neben dem Vergleich zur Enet-Methode, die der Standard-Ansatz für hochdimensionale Daten ist, wird die Güte der Schätzer der $\text{NBPR}_c$-Regression auch hinsichtlich unterschiedlicher Zentralitätsmaße bei der Gewichtung der einzelnen Regressionskoeffizienten des $\text{NBPR}_c$-Strafterms untereinander verglichen.\\ 
Im Folgenden wird zunächst die Methode zur Datensimulation (Abschnitt \ref{Kap_Datensimulation}) und im anschließenden Abschnitt \ref{Kap_Schaetzung_beta} das Vorgehen zur Schätzung der Regressionskoeffizienten der simulierten Daten erläutert. Im abschließenden Abschnitt \ref{Kap_Ergebnisse_Schaetzung} werden die Schätzergebnisse der Enet-Regression sowie von sechs verschiedenen Gewichtungen für die $\text{NBPR}_c$-Methode bzw. einer Version ohne Gewichtung hinsichtlich der oben erwähnten Gütekriterien vorgestellt und miteinander verglichen.

\section{Datensimulation}\label{Kap_Datensimulation}

Für die unter \ref{Kap_Netzwerkbasierte_Penalisierung} vorgestellten Methoden wurde von den jeweiligen Autoren stets eine Datensimulationsschema genutzt, das \citeA{li_network-constrained_2008} für ihre Grace-Regression verwendet haben. Dabei wurde als der den Daten zugrunde liegende Graph ein unzusammenhängender Graph gewählt. Dessen Subgraphen bestanden jeweils aus einem zentralen Knoten, der mit zehn anderen Knoten verbunden ist, die untereinander wiederum keine Kanten aufweisen. Die Knoten- und damit Kovariablenanzahl lag bei den Simulationen für $p \gg n$ jeweils zwischen $1100$ und $2200$ (vgl. \citeNP{li_network-constrained_2008} und \citeNP{kim_network-based_2013}). Anschließend wurden bei zwei bis vier Subgraphen alle Regressionskoeffizienten ungleich null gesetzt, sodass nur die dazu entsprechenden Kovariablen in das wahre Modell gehörten. Die restlichen Regressionskoeffizienten des Graphen wurden gleich null gesetzt und gehörten damit nicht in das wahre Modell. Bei den Simulation wurden jede Kovariable $X_j$ als normal verteilte Pseudozufallsvariable $X \sim \text{N}(0,1)$ und der Fehler $\varepsilon$ als $\varepsilon \sim \text{N}(0,\sigma^2)$ mit $\sigma^2=\sum_{j=1}^{p}\beta_j/b$ und $b \in \mathbb{N}_+$ generiert. Anschließend wurde damit die Responsevariable als $\text{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}$ erzeugt.\\
Nach Auffassung des Autors dieser Arbeit sind die sternförmigen Subgraphen in dem oben beschriebenen Simulationsablauf im Verhältnis zu wahren biologischen Netzwerken, d.h. den Pathways, idealisiert bzw. zu sehr vereinfacht. Abbildung \ref{Fig:KEGG_Cancer_PW} zeigt exemplarisch aus der KEGG-Datenbank eine Übersicht über alle biologischen Pathways im Zusammenhang mit Krebserkrankungen \cite{Kanehisa2000KEGG}. Anhand des daraus erstellten ungerichteten Graphen in Abbildung \ref{fig:Cancer_PW_Graph} wird deutlich, dass die sternförmige Subgraphen keine geeignete Wahl sind, um biologische Netzwerke zu simulieren.\\
Deswegen wird für diese Arbeit ein Simulationsschema erstellt, das ungerichtete Graphen liefert, die eine größere Ähnlichkeit zu biologischen Pathways aufweisen als die Graphen nach dem Schema von \citeA{li_network-constrained_2008}. Dazu werden zunächst alle $290$ verfügbaren Pathways für den Homo Sapiens der KEGG-Datenbank (Stand 01. November, 2014) in ungerichtete Graphen umgewandelt. Davon werden $20$ Graphen, die keine Kanten haben, und zwei Graphen, die nur aus zwei Knoten bestehen, entfernt. Anschließend wird für jeden Graphen der durchschnittliche Knotengrad, der Medianknotengrad, die Anzahl an Kanten sowie das Verhältnis der vorhandenen Kanten zu den theoretisch möglichen $N_V(N_V-1)/2$ Kanten bestimmt und gegen die Ordnung der Graphen, d.h. deren Knotenanzahlen $N_V$, in Streudiagrammen abgetragen (siehe Abbildung \ref{fig:Plots_KEGG_Kennzahlen}).
Auf Grundlage einer visuellen Inspektion der Streudiagramme werden die Graphen in fünf Kategorien $O_o$ bzgl. ihrer Ordnung einsortiert, wodurch die biologischen Netzwerke im Hinblick auf das Verhältnis von $N_V$ zur Kantenzahl $N_E$ ausreichend detailliert klassifiziert sind:
\begin{align*}
O_o =\begin{cases}
O_1 \qquad \text{für} \ G(V,E) \ \text{mit} \ N_V<25,\\
O_2 \qquad \text{für} \ G(V,E) \ \text{mit} \ 25 \leq N_V<50,\\
O_3 \qquad \text{für} \ G(V,E) \ \text{mit} \ 50 \leq N_V<100,\\
O_4 \qquad \text{für} \ G(V,E) \ \text{mit} \ 100 \leq N_V<200,\\
O_5 \qquad \text{für} \ G(V,E) \ \text{mit} \ 200 \leq N_V.
\end{cases}
\end{align*}
<<echo=FALSE, message=FALSE, Plots_KEGG_Kennzahlen, fig.pos="H", fig.show='hold', fig.align='center', out.width='\\linewidth', fig.lp="fig:", fig.scap="Streudiagramme von deskriptiven Kennzahlen der KEGG-Pathways", fig.cap="Streudiagramme von verschiedenen Kennzahlen der ungerichteten Graphen, die aus den $268$ KEGG-Pathways generiert worden sind.">>=
load("~/Studium/BIOMETRIE/Masterarbeit/R-Code/R-Objekte/KEGG_descriptive.RData")
par(mfrow=c(2,2))

plot(KEGG.descript$n.vertices, KEGG.descript$n.edges,
     main="a",
     ylab="Anzahl Kanten",
     xlab="Anzahl Knoten")

plot(KEGG.descript$n.vertices, KEGG.descript$rel.edge, 
     main="b",
     ylab="Relativer Anteil an Kanten",
     xlab="Anzahl Knoten")


plot(KEGG.descript$n.vertices, KEGG.descript$mean.degree,
	 main="c",
     ylab="Arith. Mittel des Knotengrads",
     xlab="Anzahl Knoten")

plot(KEGG.descript$n.vertices, KEGG.descript$median.degree,
	 main="d",
     ylab="Median des Knotengrads",
     xlab="Anzahl Knoten")
@
Unter Berücksichtigung dieser Ordnungskategorien werden für alle KEGG-Graphen, die in eine Kategorie fallen, die arithmetischen Mittel bzw. die Mediane für den Knotengrad, den relativen Anteil an allen möglichen Kanten sowie über $N_E$ berechnet (siehe Tabelle \ref{Tab_Uebersicht_Simu_Graphen}). Um eine möglichst realistische Simulation von biologischen Pathways zu erreichen, werden mit Hilfe des R-Pakets \texttt{qpgraph} \cite{castelo2006Rpackage_qpgraph,castelo2009Rpackage_qpgraph,castelo2014Rpackage_qpgraph} Zufallsgraphen nach dem  Erd{\H{o}}s-R{\'e}nyi-Modell modelliert, d.h. für einen Graphen gilt $G(V,E)$ mit $|E|=\lfloor \frac{N_V(N_V-1)}{2} p+ 0.5 \rfloor$, wobei $p$ der Wahrscheinlichkeit entspricht, dass zwischen zwei Knoten $u$ und $v$ eines Graphen eine Kante existiert. Für die Simulation eines Graphen, der in die Ordnungskategorie $O_o$ fällt, wird als $p$ der Median des relative Anteils von Kanten an möglichen Kanten von den biologischen Pathways der entsprechenden Ordnung gewählt. Unter Berücksichtigung der Streudiagramme in Abbildung \ref{fig:Plots_KEGG_Kennzahlen} erscheint der Median als günstigere Wahl, da er nicht so stark wie das arithmetische Mittel von Ausreißern bzgl. des relativen Anteils an Kanten beeinflusst wird. Der Vergleich der arithmetischen Mittelwerte und Mediane in Tabelle \ref{Tab_Uebersicht_Simu_Graphen} unterstützt dieses Vorgehen, da in Bezug auf den Median die biologischen Graphen im Mittel deutlich weniger dicht sind als in Bezug auf das arithmetische Mittel. Auch der beispielhaft simulierte Graph mit $75$ Knoten in Abbildung \ref{fig:Beispielgraph_Simu} unterstreicht die Entscheidung für den Median. Dadurch variieren die einzelnen Knoten hinsichtlich ihres Knotengrades in diesem Fall von $0$ (türkis) bzw. $1$ (gelb) bis zu $8$(rot), was als eine Analogie zur Knotengradvariabilität bei biologischen Pathways interpretiert werden kann.
\begin{table}[H]
\centering
\begin{tabular}{clccclccc}
\multicolumn{1}{l}{}        &                       & \multicolumn{3}{c}{\textbf{Arithmetische Mittelwerte}}                                                                 &                       & \multicolumn{3}{c}{\textbf{Mediane}}                                                                            \\ \cline{3-5} \cline{7-9} 
                            & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$d(G)$} & \multicolumn{1}{c|}{$p= \frac{N_E}{N_V(N_V-1)/2}$} & \multicolumn{1}{c|}{$ N_E$}   & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{$d(G)$} & \multicolumn{1}{l|}{$p= \frac{N_E}{N_V(N_V-1)/2}$} & \multicolumn{1}{l|}{$N_E $}  \\ \cline{1-1} \cline{3-5} \cline{7-9} 
\multicolumn{1}{|c|}{$O_1$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$2.141$}      & \multicolumn{1}{c|}{$0.2076$}                      & \multicolumn{1}{c|}{$15.97$}  & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$1$}    & \multicolumn{1}{c|}{$0.1593$}                      & \multicolumn{1}{c|}{$7.5$}   \\ \cline{1-1} \cline{3-5} \cline{7-9} 
\multicolumn{1}{|c|}{$O_2$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$3.781$}      & \multicolumn{1}{c|}{$0.1135$}                      & \multicolumn{1}{c|}{$68.39$}  & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$1$}    & \multicolumn{1}{c|}{$0.0654$}                      & \multicolumn{1}{c|}{$42$}    \\ \cline{1-1} \cline{3-5} \cline{7-9} 
\multicolumn{1}{|c|}{$O_3$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$5.611$}      & \multicolumn{1}{c|}{$0.0826$}                      & \multicolumn{1}{c|}{$200.64$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$3$}    & \multicolumn{1}{c|}{$0.0566$}                      & \multicolumn{1}{c|}{$134$}   \\ \cline{1-1} \cline{3-5} \cline{7-9} 
\multicolumn{1}{|c|}{$O_4$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$6.656$}      & \multicolumn{1}{c|}{$0.0495$}                      & \multicolumn{1}{c|}{$467.95$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$3$}    & \multicolumn{1}{c|}{$0.0388$}                      & \multicolumn{1}{c|}{$337.5$} \\ \cline{1-1} \cline{3-5} \cline{7-9} 
\multicolumn{1}{|c|}{$O_5$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$6.819$}      & \multicolumn{1}{c|}{$0.0283$}                      & \multicolumn{1}{c|}{$869.65$} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$4$}    & \multicolumn{1}{c|}{$0.0214$}                      & \multicolumn{1}{c|}{$706$}   \\ \cline{1-1} \cline{3-5} \cline{7-9} 
\end{tabular}
\caption[Zusammenfassung über simulierte Graphen]{Übersicht über die KEGG-Graphen bzgl. des durchschnittlichen Knotengrads $d(v)$, des Anteils an vorhandenen zu theoretisch möglichen Kanten $p=\frac{N_E}{N_V(N_V-1)/2}$ sowie der Anzahl an Kanten $N_E$ für die einzelnen Ordnungskategorien. }\label{Tab_Uebersicht_Simu_Graphen}
\end{table}
Nach dem oben entwickelten Schema werden für die Simulationsstudie insgesamt $100$ Datensätze mit jeweils $n=300$ Beobachtungen und $p=1000$ Kovariablen simuliert. Als den Datensätzen zugrunde liegende Graphen werden mit der R-Funktion \texttt{erGraphParam} jeweils drei Graphen mit $20$, $30$ bzw. $50$ Knoten, zwei Graphen mit $75$ Knoten sowie jeweils ein Graph mit $25$, $125$, $175$ und $225$ Knoten pseudozufällig erstellt.\\
Auf Grundlage der dabei mitsimulierten Kovarianzmatrizen werden mit der Funktion \texttt{rmvnorm} multivariat normalverteilte Kovariablen simuliert, mit deren Hilfe wiederum die Responsevariable als $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ simuliert wird. Dabei werden für einen der drei Graphen mit $20$ Knoten alle Regressionskoeffizienten als $\beta_1^{N_V=20}=...=\beta_{20}^{N_V=20}=2$ und für einen der drei Graphen mit $30$ Knoten alle Regressionskoeffizienten als $\beta_{1}^{N_V=30}=...=\beta_{30}^{N_V=30}=1$ gesetzt. Für die restlichen $950$ Kovariablen gilt $\beta_j=0$, wodurch eine spärliche Besetzung von $\boldsymbol{\beta}$ gewährleistet ist. Entsprechend der Normalverteilungsannahme des klassischen linearen Modells wird der Fehlerterm als $\varepsilon_i \sim N(0, 8)$ simuliert. Durch die im Vergleich zu den einzelnen Regressionskoeffizienten relativ große Varianz des Fehlerterms soll ein Signal-Rauschen-Verhältnis simuliert werden, das einerseits die Detektion der einzelnen Regressionskoeffizienten erschwert und gleichzeitig an in der Genetik vorkommenden Signal-Rauschen-Verhältnissen angelehnt ist. Anschließend wird entsprechend der Voraussetzungen von Kapitel \ref{Kap_Penalisierte Regression} jede Kovariable standardisiert und die Responsevariable wird zentriert.
<<echo=FALSE, message=FALSE, Beispielgraph_Simu, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.9\\linewidth', fig.lp="fig:", fig.scap="Beispielgraph aus der Datensimulation", fig.cap="Ein für die Datensimulation nach dem  Erd{\\H{o}}s-R{\\'e}nyi-Modell pseudozufällig erstellter Graph mit $75$ Knoten und einer Wahrscheinlichkeit für das Aufreteten einer Kante zwischen zwei Knoten von $p=0.0566$. Die Knotengrade für den türkisen, gelben sowie roten Knoten entsprechen $0$, $1$ bzw. $8$ und spiegeln beispielhaft die Variabilität der Knotenzentralität wider, die durch das in dieser Arbeit verwendete Schema erzeugt wird.">>=
library(graph)
load("~/Studium/BIOMETRIE/Masterarbeit/R-Code/R-Objekte/Beispielgraph_Simu.RData")
globalattrs = list()
globalattrs$node <- list(fillcolor = "lightgreen", fontsize="20")
nodeattrs <- list()
nodeattrs$fillcolor <- c(x420 = "red", x395="yellow", x387="cyan")

plot(Beispielgraph, attrs = globalattrs, nodeAttrs = nodeattrs)
@

\section{Schätzung der Regressionskoeffizienten}\label{Kap_Schaetzung_beta}

Sowohl für die Enet-Methode als auch für die $\text{NBPR}_c$-Methoden werden bei jedem Datensatz von den $n=300$ simulierten Beobachtungen $n_{tune}=50$ für die Suche der optimalen Tuningparameter -- basierend auf dem kleinsten Vorhersagefehler -- verwendet. Mit den ausgewählten Tuningparametern werden anschließend basierend auf weiteren $n_{train}=50$ Beobachtungen die Regressionskoeffizienten geschätzt.\\

\subsection{Schätzung mittels Enet}\label{Schätzung mittel Enet}
Die Schätzung von $\boldsymbol{\beta}$ mittels der Enet-Methode wird mit Hilfe des R-Pakets \texttt{glmnet} \cite{friedmann2010glmnet} realisiert. Dabei wird für $0 \leq \alpha \leq 1$ und $\gamma=\gamma_1+\gamma_2$, $\gamma \ge 0$, eine alternative Lagrangeform der PRSS aus Gleichung \eqref{Eq_Enet_Lagrange} gewählt:  
\begin{align}\label{Eq_Enet_Alternative_Lagrange}
\boldsymbol{\hat{\beta}}^{Enet}(\gamma, \alpha)&=\arg \displaystyle\min_{\beta} \textnormal{PRSS}_{Enet}(\boldsymbol{\beta}, \gamma, \alpha) \notag \\
&=\arg \displaystyle\min_{\beta} \left\lbrace \sum_{i=1}^{n} (Y_i - \sum_{j=1}^{p} x_{ij} \beta_j)^2  + \gamma \alpha \sum_{j=1}^{p}|\beta_j| + \gamma (1-\alpha) \sum_{j=1}^{p}\beta_j^2. \right\rbrace
\end{align}
Die Wahl von $\alpha$ bestimmt dabei, inwieweit die Lasso-Penalisierung im Verhältnis zur Ridge-Penalisierung und damit die Variablenselektion zur Stabiliserung der Schätzung gewichtet wird. Auf dieser Grundlage wird für alle Kombinationen der Sequenz $\alpha={0.05, 0.1, \dots ,0.95}$ und einer Sequenz von $\gamma$, die von der Funktion \texttt{cv.glmnet} gewählt wird (vgl. dazu auch \citeNP{friedmann2010glmnet}), mit den $n_{tune}$ Beobachtungen für jeden Datensatz das optimale $\alpha$ und $\gamma$ durch eine zehnfache Kreuzvalidierung ermittelt. Mit den ausgewählten Tuningparametern werden unter Verwendung der $n_{train}$ Beobachtungen die Regressionskoeffizienten mit der Funktion \texttt{glmnet} geschätzt.\\


\subsection{Schätzung mittels $\text{NBPR}_c$}\label{Schätzung mittels NBPR}
Da es sich bei \eqref{Eq_NBPR_round_t} um eine konvexe Funktion handelt, kann die Matlab-Erweiterung \texttt{CVX} zur Lösung des Optimierungsproblems verwendet werden \cite{gb08cvx,cvx}. Während die Autoren der $\text{NBPR}_c$-Methode die Zielfunktion über die \texttt{CVX}-Funktion \texttt{square\_pos(norm(\dots,2))} minimieren, wird in dieser Arbeit als Alternative die \texttt{CVX}-Funktion \texttt{sum\_square(\dots)} gewählt. Die Änderung liefert eine um den Faktor $14$ beschleunigte Lösung von \eqref{Eq_NBPR_round_t} und weist nach Angaben der Autoren der \texttt{CVX}-Toolbox nur geringe numerische Abweichungen zu \texttt{square\_pos(norm(\dots,2))} auf. 
Für die Bestimmung der Werte der verschiedenen Zentralitätsmaße als Gewichtung wird auf das R-Paket \texttt{igraph} \cite{igraph} zurückgegriffen. Da die Betweenness- und die Eigenvektor-Zentralität für einen Knoten $u$ auch Werte von null annehmen können, obwohl $u$ mindestens zu einem anderen Knoten $v$ adjazent ist, sind diese Maße im Strafterm von \eqref{Eq_NBPR_round_t} wegen $\frac{1}{\mathscr{B}(u)}$ bzw. $\frac{1}{\mathscr{E}(u)}$ als Gewichtung nicht direkt verwendbar. Daher wird in dieser Arbeit die Verwendung von Rangplätzen basierend auf den unterschiedlichen Zentralitätswerten vorgeschlagen und verwendet. $\text{NBPR}_{\mathscr{B},R}$, $\text{NBPR}_{\mathscr{C},R}$,
$\text{NBPR}_{d,R}$ und $\text{NBPR}_{\mathscr{E},R}$ bezeichnen jeweils die $\text{NBPR}_{c}$-Methode mit Rangplätzen als Gewichtung basierend auf der Betweenness-, Closeness-, Knotengrad- bzw. der Eigenvektor-Zentralität. Zusätzlich wird eine Gewichtung direkt mit der Closeness-Zentralität ($\text{NBPR}_{\mathscr{C}}$) sowie der Wurzel aus dem Knotengrad ($\text{NBPR}_{\sqrt{d}}$) vorgenommen. Darüber hinaus wird die $\text{NBPR}_{c}$-Methode ohne Knotengewichtung durchgeführt ($\text{NBPR}_{1}$).\\
Dem Vorschlag von \citeA{kim_network-based_2013} folgend werden die Lagrange-Multiplikatoren als $\varsigma_1=\frac{\gamma_1}{\tau}$ und $ \varsigma_2=\frac{\gamma_2}{\tau}$ ausgedrückt. Mit den $n_{tune}$ Beobachtungen wird die optimale Tuningparameterkombination über $4\cdot4\cdot5=80$ mögliche Kombinationen von $\varsigma_1$, $\varsigma_2$ und $\tau$ hinsichtlich Vorhersagefehlers bestimmt. Als Werte für die Tuningparameter werden äquidistante Punkte aus den Intervallen $\left[ \max(|\boldsymbol{\hat{\beta}}^{Enet}|), \max(|\boldsymbol{\hat{\beta}}^{Enet}|) \cdot \frac{p}{4} \right]$, $\left[ N_E, \max(|\boldsymbol{\hat{\beta}}^{Enet}|) \cdot N_E \right]$ bzw. $\left[0.000001, \frac{N_E}{2}\right]$ für $\varsigma_1$, $\varsigma_2$ respektive $\tau$ gewählt.\\
Für den Startschätzer $\boldsymbol{\hat{\beta}}^{(0)}$ in Algorithmus \ref{Alg_NBPR} schlagen die Autoren den Lasso-Schätzer vor. Da dieser Schätzer allerdings unerwünschte Limitationen für hochdimensionale Probleme aufweist -- insbesondere im Hinblick auf die fehlende Gruppierungseigenschaft (siehe Kapitel \ref{Kap_Lasso-Regression}) --, wird in dieser Arbeit der überlegene Enet-Schätzer als Startschätzer verwendet und die Grenze für den Abbruch des Algorithmus bei $\epsilon = 0.0001$ gesetzt. Die anschließende Schätzung von $\boldsymbol{\beta}$ erfolgt wie bei der Enet-Regression mittels den $n_{train}$ Beobachtungen und unter erneuter Verwendung der \texttt{CVX}-Toolbox.


\section{Ergebnisse}\label{Kap_Ergebnisse_Schaetzung}
Während die Enet-Methode für alle $100$ simulierten Datensätze numerisch stabile Ergebnisse liefert, schwankt der Anteil an Simulationen mit stabilen Schätzern über die einzelnen $\text{NBPR}_c$-Versionen zwischen $0.74$ bei $\text{NBPR}_{\mathscr{C},R}$ und $0.38$ bei $\text{NBPR}_1$ (siehe dazu auch Tabelle \ref{Tab_Uebersicht_RP_FP_Raten}). Die Instabilität einzelner Simulationen äußert sich in mehreren hundert geschätzten Regressionskoeffizienten, die jeweils im Bereich $[-2000000,-1000]$ bzw. $[1000,8000000]$ liegen, während die restlichen Regressionskoeffizienten eines Datensatzes als  $\hat{\beta}_j=0$ geschätzt werden. Für die nachfolgende deskriptive Auswertung werden nur diejenigen Datensätze berücksichtigt, die eine numerisch stabile Schätzung liefern.\\
\begin{comment}, da eine abschließende Erklärung für die instabilen Ergebnisse den Rahmen dieser Arbeit übersteigen würde.\\
\end{comment}
 
Zur Bestimmung der RP- bzw- FP-Rate  werden sowohl für die Enet-Methode als auch bei den sieben unterschiedlichen Versionen der $\text{NBPR}_c$-Methode alle $|\hat{\beta_j}| = 0$ gesetzt, wenn $|\hat{\beta_j}| < 0.00001$ gilt. Anhand von Tabelle \ref{Tab_Uebersicht_RP_FP_Raten} ist ersichtlich, dass die Enet-Methode mit $0.436$ die niedrigste RP-Rate hat, wohingegen die Gewichtung mit $\sqrt{d(u)}$ mit einer RP-Rate von $0.494$ die besten Ergebnisse liefert. Bezogen auf das wahre Modell, in dem $50$ Regressionskoeffizienten ungleich null sind, setzt davon $\text{NBPR}_{\sqrt{d}}$  durchschnittlich ca. $3$ Regressionskoeffizienten mehr als die Enet-Methode ungleich null.\\
Im Hinblick auf die empirische Häufigkeitsverteilung der RP-Rate ähneln sich die Enet- und die $\text{NBPR}_c$-Versionen mit Rangplätzen als Gewichtung (siehe Abbildung \ref{fig:Histo_TP_Simu}). Diese Ähnlichkeit wird durch die Werte des arithmetischen Mittelwerts, des Medians, der empirischen Standardabweichung sowie dem $25\%$- bzw. $75\%$-Quantil in Tabelle \ref{Tab_Uebersicht_RP_FP_Raten} unterstrichen. Dagegen ist die empirische Häufigkeitsverteilung der $\text{NBPR}_c$-Versionen ohne Gewichtung bzw. mit Closeness- und $\sqrt{d(u)}$-Zentralitätsgewichtung deutlich flacher und die Streuung größer. Allerdings liegt den entsprechenden Histogrammen bzw. berechneten Streuungen auch eine geringere Anzahl an erfolgreichen Simulationen zu Grunde. Daher kann nicht ausgeschlossen werden kann, dass sich die Verteilungen bei größeren Fallzahlen den übrigen annähern würden.\\

Für die FP-Rate sind sich bis auf $\text{NBPR}_{\mathscr{C}}$ alle Schätzer ähnlich und im Schnitt schlechter als die Enet-Schätzer. Dadurch relativiert sich auch der weiter oben erwähnte Vorteil der $\text{NBPR}_{\sqrt{d}}$-Version gegenüber der Enet-Regression bezüglich der RP-Rate. Erstgenannte wählt mit einer FP-Rate von $0.094$ fast doppelt so viele falsche Koeffizienten als die Enet-Methode aus. Dies entspricht bei $950$ Regressionskoeffizienten, die im wahren Modell keinen Einfluss auf die Responsevariable haben, ca. $89$ fälschlich ausgewählten Kovariablen für $\text{NBPR}_{\sqrt{d}}$. Somit kommen in der Simulationsstudie für $\text{NBPR}_{\sqrt{d}}$ auf jedes korrekt gesetzte $\hat{\beta}_j \neq 0$ circa $3.6$ fälschlich gesetzte $\hat{\beta}_j \neq 0$. Für die Enet-Methode beträgt das Verhältnis circa $1:2.2$. Die Verhältnisse der übrigen Schätzungen liegen dazwischen.\\ 
Im Bezug auf die empirischen Häufigkeitsverteilungen der FP-Rate in Abbildung \ref{fig:Histo_FP_Simu} sind die Enet-Methode und die $\text{NBPR}_c$-Versionen mit Gewichtungen über die Ränge sowie mit der Closeness-Zentralität fast identisch und dabei rechtsschräg. Zwar weisen auch die $\text{NBPR}_{\sqrt{d}}$- und $\text{NBPR}_{1}$-Version eine rechtsschräge Häufigkeitsverteilung auf, diese sind jedoch deutlich weniger steil. Allerdings müssen auch hierbei die abweichenden Ergebnisse wegen der geringen Anzahl an numerisch stabilen und somit berücksichtigten Simulationen unter Vorbehalt betrachtet werden.\\

Zur Bestimmung des Modell- bzw. Vorhersagefehlers werden die nach der Bestimmung der Tuningparameter sowie Schätzung der Regressionskoeffizienten verbliebenen $n_{test}=200$ Beobachtungen pro simulierten Datensatz verwendet. Dabei liegt der Modellfehler der Enet-Methode mit $\text{ME}=155.56$ deutlich unter denen der übrigen Schätzmethoden, von denen wiederum die Gewichtung mit der Eigenvektor-Zentralität ($\text{ME}=182.71$) das beste Ergebnis liefert (siehe Tabelle \ref{Tab_Uebersicht_ME_PE}). Auffällig ist die relativ große empirische Standardabweichung der $\text{NBPR}_{\sqrt{d}}$-Version ($\text{s}=177.07$) im Verhältnis zu den übrigen Schätzungen. Grund dafür ist die im entsprechenden Histogramm in Abbildung \ref{fig:Histo_ME_Simu} abgebildete Simulation mit einem ME-Wert über $1200$, bei der es sich um einen Ausreißer handeln kann. Allgemein weisen die Histogramme für alle $\text{NBPR}_c$-Version einzelne ME-Werte über $400$ auf, die die hohen empirischen Standardabweichungen erklären. Werden die Werte als zufällige Ausreißer behandelt und nicht berücksichtigt, ist auf Grundlage der Histogramme zu erkennen, dass sich die durchschnittlichen ME-Werte weiter an den der Enet-Methode annähern. Ob es sich dabei wirklich um zufällige Ausreißer oder systematische Abweichungen handelt, kann an dieser Stelle nicht abschließend geklärt werden.



\begin{comment}
!!!!!! Bis hier lesen!
mit den ausgewählten Tuningparametern sowie einen Testdatensatz mit $n_{test}=200$ für die Überprüfung der Qualität der $\boldsymbol{\hat{\beta}}$ unterteilt. 


Die durchschnittliche RP-Rate liegt dabei bei $0.436$ (siehe dazu auch Tabelle \ref{Tab_Uebersicht_RP_FP_Raten}), was circa $22$ Regressionskoeffizienten im Setting entspricht. Somit werden in dieser Simulationsstudie von den wahren Regressionskoeffizienten, die ungleich null sind, weniger als die Hälfte mit der Enet-Methode aufgedeckt. Gleichzeitig liegt die durchschnittliche FP-Rate bei $0.052$, was circa $49$ Regressionskoeffizienten entspricht. Somit ist das Verhältnis der korrekt ungleich null gesetzten zu den fälschlich ungleich null gesetzten Regressionskoeffizienten ungefähr $1:2.2$.\\
Der mit den $n_{test}$ Beobachtungen bestimmten durchschnittliche Modelfehler (siehe Tabelle \ref{Tab_Uebersicht_ME_PE}) für die Enet-Methode liegt bei $155.56$ \\
\end{comment}
\begin{sidewaystable}[h]
\centering
\begin{tabular}{clclccccclccccc}
\multicolumn{1}{l}{}          &  & \multicolumn{1}{l}{} &  & \multicolumn{5}{c}{\textbf{RP-Rate}}                       &  & \multicolumn{5}{c}{\textbf{FP-Rate}}                        \\ \cline{3-3} \cline{5-9} \cline{11-15} 
\multicolumn{1}{l}{}          &  & $\pi$                &  & arith. Mittel & s       & Median & $Q_{0.25}$ & $Q_{0.75}$ &  & arith. Mittel & s       & Median  & $Q_{0.25}$ & $Q_{0.75}$ \\ \cline{3-3} \cline{5-9} \cline{11-15} 
$\text{Enet}$                 &  & 1.0                  &  & $0.436$       & $0.146$ & $0.42$ & $0.34$     & $0.54$     &  & $0.052$       & $0.035$ & $0.042$ & $0.027$    & $0.070$    \\
                              &  &                      &  &               &         &        &            &            &  &               &         &         &            &            \\
$\text{NBPR}_{\mathscr{B},R}$ &  & 0.71                 &  & $0.465$       & $0.142$ & $0.46$ & $0.36$     & $0.58$     &  & $0.059$       & $0.043$ & $0.048$ & $0.036$    & $0.074$    \\
$\text{NBPR}_{\mathscr{C},R}$ &  & 0.74                 &  & $0.461$       & $0.132$ & $0.46$ & $0.37$    & $0.54$     &  & $0.066$       & $0.076$ & $0.047$ & $0.033$    & $0.076$    \\
$\text{NBPR}_{d,R}$           &  & 0.66                 &  & $0.458$       & $0.152$ & $0.44$ & $0.36$     & $0.56$    &  & $0.073$       & $0.107$ & $0.048$ & $0.030$     & $0.074$    \\
$\text{NBPR}_{\mathscr{E},R}$ &  & 0.64                 &  & $0.460$        & $0.124$ & $0.45$ & $0.38$     & $0.54$     &  & $0.056$       & $0.043$ & $0.045$ & $0.032$    & $0.074$    \\
                              &  &                      &  &               &         &        &            &            &  &               &         &         &            &            \\
$\text{NBPR}_{\sqrt{d}}$      &  & 0.42                 &  & $0.494$       & $0.195$ & $0.5$  & $0.37$     & $0.64$    &  & $0.094$       & $0.150$ & $0.073$ & $0.033$    & $0.098$    \\
$\text{NBPR}_{\mathscr{C}}$   &  & 0.56                 &  & $0.440$       & $0.175$ & $0.44$ & $0.30$      & $0.55$     &  & $0.048$       & $0.047$ & $0.032$ & $0.024$    & $0.057$    \\
                              &  &                      &  &               &         &        &            &            &  &               &         &         &            &            \\
$\text{NBPR}_{1}$             &  & 0.38                 &  & $0.483$       & $0.164$ & $0.52$ & $0.37$    & $0.60$      &  & $0.067$       & $0.041$ & $0.067$ & $0.030$    & $0.091$   
\end{tabular}
\caption[Übersicht über RP- und FP-Raten der verschiedenen Schätzmethoden]{Übersicht für das arithmetische Mittel, die empirische Standardabweichung (s), den Median, das $25\%$- bzw. $75\%$-Quantil ($Q_{0.25}$; $Q_{0.75}$) für die richtig-positiv und falsch-positiv-Raten (RP-Rate; FP-Rate) der einzelnen Schätzmethoden sowie den Anteil an numerisch stabilen Simulationen ($\pi$). $\text{NBPR}_{\mathscr{B},R}$, $\text{NBPR}_{\mathscr{C},R}$,
$\text{NBPR}_{d,R}$ und $\text{NBPR}_{\mathscr{E},R}$ bezeichnen die $\text{NBPR}_{c}$-Methode, bei der jeweils die einzelnen Knotengewichte als Rangplätze über die Betweenness-, Closeness-, einfache Knotengrad- bzw. Eigenvektor-Zentralitäten gewählt wurden. Bei $\text{NBPR}_{\mathscr{C}}$ und $\text{NBPR}_{\sqrt{d}}$ wurden als Knotengewichte die Closeness-Zentralität bzw. die Wurzel aus dem Knotengrad verwendet. $\text{NBPR}_1$ bezeichnet die Netzwerk-basierten penalisierten Regression ohne Knotengewichtung.}\label{Tab_Uebersicht_RP_FP_Raten}
\end{sidewaystable}

\begin{sidewaystable}[h]
\centering
\begin{tabular}{clclccccclccccc}
\multicolumn{1}{l}{}          &  & \multicolumn{1}{l}{}                                  &  & \multicolumn{5}{c}{\textbf{ME}}                               &  & \multicolumn{5}{c}{\textbf{PE}}                                \\ \cline{3-3} \cline{5-9} \cline{11-15} 
\multicolumn{1}{l}{}          &  & \multicolumn{1}{c}{$\pi$} &  & arith. Mittel & s        & Median   & $Q_{0.25}$ & $Q_{0.75}$ &  & arith. Mittel & s        & Median    & $Q_{0.25}$ & $Q_{0.75}$ \\ \cline{3-3} \cline{5-9} \cline{11-15} 
$\text{Enet}$                 &  & 1.0                                                   &  & $155.56$      & $44.75$  & $147.12$ & $123.05$   & $187$      &  & $238.2$       & $52.92$  & $230.0$   & $198.1$    & $272.42$   \\
                              &  &                                                       &  &               &          &          &            &            &  &               &          &           &            &            \\
$\text{NBPR}_{\mathscr{B},R}$ &  & 0.71                                                  &  & $195.78$      & $85.44$  & $175.46$ & $146.72$   & $215.07$   &  & $1044.30$     & $285.73$ & $1024.21$ & $868.64$   & $1212.9$   \\
$\text{NBPR}_{\mathscr{C},R}$ &  & 0.74                                                  &  & $201.27$      & $101.75$ & $173.65$ & $148.63$   & $225.51$   &  & $973.02$      & $307.96$ & $954.30$  & $847.13$   & $1148.3$   \\
$\text{NBPR}_{d,R}$           &  & 0.66                                                  &  & $207.43$      & $138.74$ & $168.54$ & $148.66$   & $211.22$   &  & $972.36$      & $300.24$ & $971.32$  & $809.3$    & $1090.82$  \\
$\text{NBPR}_{\mathscr{E},R}$ &  & 0.64                                                  &  & $182.71$      & $73.85$  & $164.84$ & $149.39$   & $205.18$   &  & $1042.27$     & $247.08$ & $1021.01$ & $843.46$   & $1202.14$  \\
                              &  &                                                       &  &               &          &          &            &            &  &               &          &           &            &            \\
$\text{NBPR}_{\sqrt{d}}$      &  & 0.42                                                  &  & $236.30$      & $177.07$ & $209.66$ & $161.71$   & $241.56$   &  & $886.66$      & $280.03$ & $917.39$  & $749.35$   & $1053.0$   \\
$\text{NBPR}_{\mathscr{C}}$   &  & 0.56                                                  &  & $194.03$      & $73.82$  & $172.53$ & $153.8$    & $221.25$   &  & $998.38$      & $253.03$ & $1021.23$ & $826.73$   & $1205.05$  \\
                              &  &                                                       &  &               &          &          &            &            &  &               &          &           &            &            \\
$\text{NBPR}_{1}$             &  & 0.38                                                  &  & $204.95$      & $83.60$  & $197.76$ & $156.98$   & $223.98$   &  & $1004.30$     & $267.51$ & $1017.60$ & $818.13$   & $1148.13$ 
\end{tabular}
\caption[Übersicht über Model- und Vorhersagefehler der verschiedenen Schätzmethoden]{Übersicht für das arithmetische Mittel, die empirische Standardabweichung (s), den Median, das $25\%$- bzw. $75\%$-Quantil ($Q_{0.25}$; $Q_{0.75}$) für den Model- bzw Vorhersagefehler (ME; PE) der einzelnen Schätzmethoden sowie den Anteil an numerisch stabilen Simulationen ($\pi$). $\text{NBPR}_{\mathscr{B},R}$, $\text{NBPR}_{\mathscr{C},R}$,
$\text{NBPR}_{d,R}$ und $\text{NBPR}_{\mathscr{E},R}$ bezeichnen die $\text{NBPR}_{c}$-Methode, bei der jeweils die einzelnen Knotengewichte als Rangplätze über die Betweenness-, Closeness-, einfache Knotengrad- bzw. Eigenvektor-Zentralitäten gewählt wurden. Bei $\text{NBPR}_{\mathscr{C}}$ und
$\text{NBPR}_{\sqrt{d}}$ wurden als Knotengewichte die Closeness-Zentralität bzw. die Wurzel aus dem Knotengrad verwendet. $\text{NBPR}_1$ bezeichnet die Netzwerk-basierten penalisierten Regression ohne Knotengewichtung.}\label{Tab_Uebersicht_ME_PE}
\end{sidewaystable}

\begin{table}[h]
\centering
\begin{tabular}{ccccclclc}
                              &  & $\pi$ &  & $\mathcal{I}^{NBPR_c}=\mathcal{I}^{Enet}$ &  & $\mathcal{I}^{NBPR_c}_{\beta_j \neq 0}=\mathcal{I}^{Enet}_{\beta_j \neq 0}$ &  & $\mathcal{I}^{NBPR_c}_{\beta_j= 0}=\mathcal{I}^{Enet}_{\beta_j= 0}$ \\ \cline{3-3} \cline{5-5} \cline{7-7} \cline{9-9} 
$\text{NBPR}_{\mathscr{B},R}$ &  & 0.71  &  & $0.38$                                  &  & $0.69$                                                                    &  & $0.44$                                                            \\
$\text{NBPR}_{\mathscr{C},R}$ &  & 0.74  &  & $0.39$                                  &  & $0.77$                                                                    &  & $0.47$                                                            \\
$\text{NBPR}_{d,R}$           &  & 0.66  &  & $0.46$                                  &  & $0.83$                                                                    &  & $0.48$                                                            \\
$\text{NBPR}_{\mathscr{E},R}$ &  & 0.64  &  & $0.36$                                  &  & $0.81$                                                                    &  & $0.38$                                                            \\
                              &  &       &  &                                         &  &                                                                           &  &                                                                   \\
$\text{NBPR}_{\sqrt{d}}$      &  & 0.42  &  & $0.38$                                  &  & $0.81$                                                                    &  & $0.38$                                                            \\
$\text{NBPR}_{\mathscr{C}}$   &  & 0.56  &  & $0.45$                                  &  & $0.73$                                                                    &  & $0.45$                                                            \\
                              &  &       &  &                                         &  &                                                                           &  &                                                                   \\
$\text{NBPR}_{1}$             &  & 0.38  &  & $0.29$                                  &  & $0.79$                                                                    &  & $0.29$                                                           
\end{tabular}
\caption[Anteil an Simulationen mit identischer Besetzung Regressionskoeffizientenauswahl für Enet und $\text{NBPR}_c$-Versionen]{Der Anteil an Simulationen, bei denen die Besetzung von $\boldsymbol{\hat{\beta}}$ im Hinblick auf $\beta_j\neq 0$ bzw. $\beta_j = 0$ vom Enet-Schätzer identisch mit den $\text{NBPR}_c$-Versionen ist. $\mathcal{I}^{NBPR_c}$ bzw. $\mathcal{I}^{Enet}$  entsprechen dabei Spaltenvektoren, die für $\hat{\beta}_j \neq 0$ in der Zeile $j$ eine eins und ansonsten eine null stehen haben. $\mathcal{I}^{NBPR_c}_{\beta_j \neq 0}$ bzw. $\mathcal{I}^{Enet}_{\beta_j= 0}$ sind entsprechend aufgebaute Vektoren, wobei diese nur über diejenigen $j$ laufen, für die im wahren Modell $\beta_j \neq 0$ respektive $\beta_j = 0$ gilt. Zusätzlich wird der Anteil an numerisch stabilen Schätzungen für die einzelnen Schätzmethoden angegeben ($\pi$).}\label{Tab_Diff_betas_Enet_Nets}
\end{table}

%%%%%%%%%%%%
% Übersicht über Zentralitätsmaße (Median Closeness etc) über die Graphen



\chapter{Diskussion und Ausblick}\label{Diskussion und Ausblick}
Da auch unter Verwendung der \texttt{CVX}-Funktion 
\texttt{square\_pos(norm(\dots,2))} numerische Instabilitäten auftreten, kann die Verwendung der Funktion \texttt{sum\_square(\dots)} als Ursache ausgeschlossen werden. 
\begin{comment}
#######
## 1 ##
#######
# sqrt(d) nehmen viele autoren als standard. hier schneidet das nicht so gut ab
Zu Sig-Test bei adaptiven Methoden zur Variablenselektion:
"Classical tools for analyzing regression results, such as t statistics for judging
the significance of individual predictors, are based on the assumption that the
set of predictors is fixed in advance. When instead the set is chosen adaptively,
incorporating those variables that give the best fit for a particular set of data,
the classical tools are biased. For example, if there are 10 candidate predictors,
and we select the single one that gives the best fit, there is about a 40% chance
that that variable will be judged significant at the 5% level, when in fact all
predictors are independent of the response and each other. Similar bias holds
for the F test for comparing two models; it is based on the assumption that the
two models are fixed in advance, rather than chosen adaptively.
This bias affects the variable selection process itself. Formal selection proce-
dures such as stepwise regression and all-subsets regression are ultimately based
on statistics related to the F statistics for comparing models. Informal selection
procedures, in which an analyst picks variables that give a good fit, are similarly
affected."
aus Hesterberg "Least angle and l1 penalized regression: A review"
\end{comment}


\bibliography{Masterarbeit}


\begin{appendix}
\chapter{Beweise}
\section{Beweis für den erwarteten quadrierten Abstand von $\boldsymbol{\hat{\beta}}^{OLS}$ zu $\boldsymbol{\beta}$}\label{App_Beweis_Erwarteter_quadrierter_Abstand}
Bezeichne $\mathbf{B}$ eine $n \times p$-Matrix und $\mathbf{C}$ eine $p \times n$-Matrix, so gilt
\begin{align}\label{Eq_Spurgleichheit}
\text{sp}(\mathbf{BC})=\text{sp}(\mathbf{CB})
\end{align}
und
\begin{align}\label{Eq_Spurmultiplikation}
\text{sp}(\mathbf{M}\mathbf{D})=d \cdot \text{sp}(\mathbf{M})
\end{align}
für eine $p \times p$-Diagonalmatrix $\mathbf{D}=diag(d)$ mit den Werten $d$ und eine $p \times p$-Matrix $\mathbf{M}$ (siehe \citeNP{fahrmeir2009regression}).\\

Sei $\mathbf{b}$ ein zufälliger $n \times 1$-Spaltenvektor und $\mathbf{A}$ eine $n \times n$-Matrix, so lässt sich zeigen (siehe \citeNP{mathai1992quadratic}), dass für den Erwartungswert der quadratischen Funktion $Q(\mathbf{b})=\mathbf{b}'\mathbf{A}\mathbf{b}$
\begin{align}\label{Eq_Erwartung_Quadrat}
\mathbb{E}(\mathbf{b}'\mathbf{A}\mathbf{b})=\text{sp}(\mathbf{A}\boldsymbol{\Sigma})+\mathbb{E}(\mathbf{b})'\mathbf{A}\mathbb{E}(\mathbf{b})
\end{align}
gilt, wobei $\boldsymbol{\Sigma}$ die Kovarianzmatrix von $\mathbf{b}$ ist.\\

\begin{lemma}
Unter Berücksichtigung der Sätze \eqref{Eq_Spurgleichheit} bis \eqref{Eq_Erwartung_Quadrat} gilt
\begin{align*}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace = \sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}].
\end{align*}
\end{lemma}
\begin{proof}
\begin{align}\label{Eq_Beweis_Erwarteter_Quadrierter_Abstand}
\mathbb{E} \left\lbrace \mathcal{D}(\boldsymbol{\hat{\beta}}^{OLS},\boldsymbol{\beta}) \right\rbrace
&=\mathbb{E} \left\lbrace (\boldsymbol{\hat{\beta}}^{OLS} - \boldsymbol{\beta} )'(\boldsymbol{\hat{\beta}}^{OLS}-\boldsymbol{\beta}) \right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} - \boldsymbol{\beta}]' [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} - \boldsymbol{\beta}] \right\rbrace \notag \\
&\stackrel{\eqref{Eq_klass_lin_matrix}}{=}\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} +\boldsymbol{\varepsilon} ) -\boldsymbol{\beta}]' [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'(\mathbf{X}\boldsymbol{\beta} +\boldsymbol{\varepsilon} ) -\boldsymbol{\beta}] \right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [\mathbf{I}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon} - \boldsymbol{\beta}]'[\mathbf{I}\boldsymbol{\beta} + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon} - \boldsymbol{\beta}]
\right\rbrace \notag \\
&=\mathbb{E}\left\lbrace [(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}]'[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol{\varepsilon}] 
\right\rbrace \notag \\
&=\mathbb{E}\{
\boldsymbol{\varepsilon}'\underbrace{\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'}_{\substack{n \times n \text{-Matrix}}}\boldsymbol{\varepsilon}
\} \notag \\
&\stackrel{\eqref{Eq_Erwartung_Quadrat}}{=}\text{sp}[\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\underbrace{\mathbb{C}\text{ov}(\boldsymbol{\varepsilon})}_{\substack{\sigma^2\mathbf{I}}}] + \underbrace{\mathbb{E}(\boldsymbol{\varepsilon})'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbb{E}(\boldsymbol{\varepsilon})}_{\substack{0}} \notag \\
&\stackrel{\eqref{Eq_Spurmultiplikation}}{=}\sigma^2\text{sp}[\underbrace{\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}}_{\substack{n \times p \text{-Matrix}}}\underbrace{(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'}_{\substack{p \times n \text{-Matrix}}}] \notag \\
&\stackrel{\eqref{Eq_Spurgleichheit}}{=}\sigma^2\text{sp}[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}] 
\notag \\
&=\sigma^2 \text{sp}[(\mathbf{X}'\mathbf{X})^{-1}]. 
\end{align}
\end{proof}

\section{Beweis der Lasso-Schätzerdarstellung für ein orthogonales Design}\label{App_Lasso_Orthogonal}
\begin{satz}\label{Satz_Lasso_Ortho}
Es seien die Kovariablen in $\mathbf{X}$ standardisiert, sodass $\frac{1}{n}\sum_{i=1}^{n}x_{ij}=0$ und $\frac{1}{n}\sum_{i=1}^{n}x_{ij}^2=1$ für $1,\dots, p$ gilt, und die Responsevariable $\mathbf{Y}$ zentriert,  d.h. es gilt $\frac{1}{n}\sum_{i}^{n}y_i =0$ für die Realisierungen von $\mathbf{Y}$. Liege darüber hinaus ein orthogonales Design
\begin{align}\label{Eq_Kovarianz_Ortho}
\mathbf{X}'\mathbf{X}=\mathbf{I}
\end{align}
vor und bezeichne $\hat{\beta}_j^{OLS}$ den Kleinste-Quadrat-Schätzer, so gilt für den Lasso-Schätzer
\begin{align*}
\hat{\beta}_j^{Lasso}&=\text{sign}(\hat{\beta}_j^{OLS})(|\hat{\beta}_j^{OLS}|-\frac{\gamma}{2})^+ .
\end{align*}
\end{satz}

\begin{proof}
Die $\textnormal{PRSS}_{Lasso}(\boldsymbol{\beta}, \gamma)$ lässt sich schreiben als
\begin{align*}
\text{PRSS}_{Lasso}(\boldsymbol{\beta}, \gamma)
&=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+ \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&=\mathbf{Y}'\mathbf{Y}-2\mathbf{Y}'\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&\stackrel{\eqref{Eq_Kovarianz_Ortho}}{=}\mathbf{Y}'\mathbf{Y}-2\boldsymbol{\hat{\beta}}^{OLS}{'}\boldsymbol{\beta} + \boldsymbol{\beta}'\boldsymbol{\beta}+ \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&=\mathbf{Y}'\mathbf{Y}-2 \sum_{j=1}^{p}\hat{\beta}_j^{OLS} \beta_j + \sum_{j=1}^{p} \beta_j^2 + \gamma \sum_{j=1}^{p} |\beta_j| \notag \\
&=\mathbf{Y}'\mathbf{Y}+\sum_{j=1}^{p}\left( -2 \hat{\beta}_j^{OLS}  \beta_j + \beta_j^2 + \gamma |\beta_j| \right).
\end{align*}
Somit ergeben sich $j=1,\dots,p$ Gleichungssysteme, die nach $\beta_j$ abgeleitet und null gesetzt werden müssen:
\begin{align}\label{Eq_Lasso_Einzelbeta}
\hat{\beta}_j^{Lasso}=\frac{\partial}{\partial \beta_j} \left( \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS} \beta_j + \beta_j^2 + \gamma |\beta_j| \right) =0 \qquad \text{für} \ j=1,\dots,p .
\end{align}
Anhand der Darstellung in \eqref{Eq_Lasso_Einzelbeta} wird deutlich, dass für $\hat{\beta}_j^{OLS}>0$  auch $\beta_j > 0$ bzw. für $\hat{\beta}_j^{OLS}<0$ auch $\beta_j < 0$ gelten muss. Somit ergibt sich der Lasso-Schätzer für $\hat{\beta}_j^{OLS}>0$ als
\begin{align}\label{Eq_Lasso_Ortho_Beta_greater0}
&& 0&=\frac{\partial}{\partial \beta_j} \left( \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS}  \beta_j + \beta_j^2 + \gamma \beta_j \right) \notag \\
&\Rightarrow& 0&=-2\hat{\beta}_j^{OLS} + 2\beta_j + \gamma \notag \\
&\Rightarrow& \hat{\beta}_j^{Lasso}&=\hat{\beta}_j^{OLS}-\frac{\gamma}{2} 
\end{align}
und für $\hat{\beta}_j^{OLS}<0$ als
\begin{align}\label{Eq_Lasso_Ortho_Beta_smaller0}
&& 0&=\frac{\partial}{\partial \beta_j} \left( \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS}  \beta_j + \beta_j^2 - \gamma \beta_j \right) \notag \\
&\Rightarrow& 0&=-2\hat{\beta}_j^{OLS} + 2\beta_j - \gamma \notag \\
&\Rightarrow& \hat{\beta}_j^{Lasso}&=\hat{\beta}_j^{OLS}+\frac{\gamma}{2} .
\end{align}
Da in \eqref{Eq_Lasso_Ortho_Beta_greater0} die rechte Seite für den Lasso-Schätzer größer und in \eqref{Eq_Lasso_Ortho_Beta_smaller0} kleiner null sein muss, kann in beiden Fällen der Lasso-Schätzer dargestellt werden als
\begin{align}\label{Eq_Lasso_Orthogonaldarstellung}
\hat{\beta}_j^{Lasso}&=\text{sign}(\hat{\beta}_j^{OLS})(|\hat{\beta}_j^{OLS}|-\frac{\gamma}{2})^+ . 
\end{align}
Für den Fall, dass $\frac{\gamma}{2} \geq \hat{\beta}_j^{OLS}$ vorliegt, gilt $\hat{\beta}_j^{Lasso}=0$.
\end{proof}

\citeA{tibshirani96regression} sowie \citeA{friedman2007pathwise} verweisen darauf, dass \eqref{Eq_Lasso_Orthogonaldarstellung} einem \textit{weichen Schwellenwert} entspricht:
\begin{align*}
\hat{\beta}_j^{Lasso} =
\begin{cases}
\hat{\beta}_j^{OLS}-\frac{\gamma}{2}, \qquad &\text{wenn} \ \hat{\beta}_j^{OLS} > 0 \ \text{und} \ \frac{\gamma}{2} < |\hat{\beta}_j^{OLS}| ,\\
\hat{\beta}_j^{OLS}+\frac{\gamma}{2}, \qquad &\text{wenn} \ \hat{\beta}_j^{OLS} < 0 \ \text{und} \ \frac{\gamma}{2} < |\hat{\beta}_j^{OLS}| ,\\
0, \qquad &\text{wenn} \ \frac{\gamma}{2} \geq |\hat{\beta}_j^{OLS}|. 
\end{cases}
\end{align*}





\section{Beweis der der alternativen nEnet-Darstellung}\label{App_Beweis_Naiv_Zou_Darstellung}
\begin{satz}\label{Satz_Naiv_Enet}
Gegeben den Datensatz $(\mathbf{X}, \mathbf{Y}$) sowie die Lagrange-Multiplikatoren $\gamma_1$ und $\gamma_2$, sei $(\mathbf{X^*}, \mathbf{Y^*})$ ein erweiterter Datensatz mit
\begin{align}\label{Eq_Erweiterter_Datensatz_Enet}
\mathbf{X^*}_{(n+p)\times p} = \frac{1}{\sqrt{1+\gamma_2}}
\begin{pmatrix}
\mathbf{X}\\
\sqrt{\gamma_2}\mathbf{I}
\end{pmatrix}, 
\qquad 
\mathbf{Y^*}_{(n+p) \times 1}=\begin{pmatrix}
\mathbf{Y}\\
\mathbf{0}
\end{pmatrix}.
\end{align}
Mit $\omega = \gamma_1 /\sqrt{1+\gamma_2}$, $\boldsymbol{\beta^*}=\sqrt{1+\gamma_2}\boldsymbol{\beta}$ und $||\cdot||^2_2$ für die quadrierte $L_2$-Norm bzw. $||\cdot||_1$ für die $L_1$-Norm, lässt sich die penalisierte Residuenquadratsumme schreiben als 
\begin{align}\label{Eq_Naiv_Enet_Lassoform}
\textnormal{PRSS}_{nEnet}(\omega, \boldsymbol{\beta})=\textnormal{PRSS}_{nEnet}(\omega, \boldsymbol{\beta^*})=||\mathbf{Y^*} -\mathbf{X^*}\boldsymbol{\beta^*}||^2_2+\omega||\boldsymbol{\beta^*}||_1.
\end{align}
Sei zudem
\begin{align*}
\boldsymbol{\hat{\beta}^*}=\arg \displaystyle\min_{\boldsymbol{\beta^*}} \textnormal{PRSS}_{nEnet}(\omega, \boldsymbol{\beta^*}),
\end{align*}
so gilt
\begin{align}\label{Eq_Naiv_Enet_Zou}
\boldsymbol{\hat{\beta}}^{nEnet}=\frac{1}{\sqrt{1+\gamma_2}}\boldsymbol{\hat{\beta}^*}.
\end{align}
\end{satz}

\noindent \textit{Beweis.} Da es sich bei \eqref{Eq_Naiv_Enet_Lassoform} um eine Lagrange-Form handelt, die äquivalent zur Lasso-Form ist, und wegen $\gamma_2 \geq 0$ (vgl. dazu Anhang \ref{App_Lagrange-Form}) auch $\sqrt{1+\gamma_2} \geq 1$ gelten muss, liefert die Ableitung nach $\boldsymbol{\beta^*}$ und anschließendes null setzen den nEnet-Schätzer
\begin{align*}
&& \frac{\textnormal{PRSS}_{nEnet}(\omega, \boldsymbol{\beta^*})}{\partial \boldsymbol{\beta^*}}
&=-2\mathbf{X^*}'\mathbf{Y^*}+2\mathbf{X^*}'\mathbf{X^*}\boldsymbol{\beta^*} + \omega \frac{\boldsymbol{\beta^*}}{|\boldsymbol{\beta^*}|} \\
&\Rightarrow & 0 &= -2\mathbf{X^*}'\mathbf{Y^*}+2\mathbf{X^*}'\mathbf{X^*}\boldsymbol{\beta^*} + \omega \frac{\sqrt{1+\gamma_2}\boldsymbol{\beta}}{\sqrt{1+\gamma_2}|\boldsymbol{\beta}|} \\
&\Leftrightarrow &\boldsymbol{\hat{\beta}^*} &= (\mathbf{X^*}'\mathbf{X^*})^{-1}\mathbf{X^*}'\mathbf{Y^*} - (\mathbf{X^*}'\mathbf{X^*})^{-1} \frac{\omega}{2} \textnormal{sign}(\boldsymbol{\beta}) \\
&\Leftrightarrow &\sqrt{1+\gamma_2}\boldsymbol{\hat{\beta}} &= (\mathbf{X^*}'\mathbf{X^*})^{-1}\mathbf{X^*}'\mathbf{Y^*} - (\mathbf{X^*}'\mathbf{X^*})^{-1} \frac{\omega}{2} \textnormal{sign}(\boldsymbol{\beta}) \\
&\Rightarrow & \boldsymbol{\hat{\beta}}^{nEnet} &= \frac{1}{\sqrt{1+\gamma_2}}\boldsymbol{\hat{\beta}^*}. \\ 
\tag*{$\square$} 
\end{align*}

\begin{lemma}
Unter Berücksichtigung der Annahmen aus Satz \ref{Satz_Naiv_Enet} und da wegen $\gamma_2 \geq 0$ (vgl. dazu Anhang \ref{App_Lagrange-Form}) auch $\sqrt{1+\gamma_2} \geq 1$ gelten muss, gilt für die Darstellung des naiven Elastic Net-Schätzers aus \eqref{Eq_Naiv_Enet_Zou}:
\begin{align*}
\frac{1}{\sqrt{1+\gamma_2}}\boldsymbol{\hat{\beta}^*}=(\mathbf{X}'\mathbf{X}+\gamma_2 \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}-(\mathbf{X}'\mathbf{X}+\gamma_2 \mathbf{I})^{-1} \frac{\gamma_1}{2} \textnormal{sign}(\boldsymbol{\beta}).
\end{align*}
\end{lemma}
\begin{proof}
\begin{align*}
\frac{1}{\sqrt{1+\gamma_2}}\boldsymbol{\hat{\beta}^*}
&=\frac{1}{\sqrt{1+\gamma_2}} \left\lbrace(\mathbf{X^*}'\mathbf{X^*})^{-1}\mathbf{X^*}'\mathbf{Y^*} - (\mathbf{X^*}'\mathbf{X^*})^{-1} \frac{\omega}{2} \frac{\boldsymbol{\beta^*}}{|\boldsymbol{\beta^*}|} \right\rbrace\\
%
&\stackrel{\eqref{Eq_Erweiterter_Datensatz_Enet}}{=}
	\frac{1}{\sqrt{1+\gamma_2}} 
		\left\lbrace
			\left[
			 \frac{1}{\sqrt{1+\gamma_2}}
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}'
			 \frac{1}{\sqrt{1+\gamma_2}}
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}
			\right]^{-1}
			\frac{1}{\sqrt{1+\gamma_2}}
			\begin{pmatrix}
			 \mathbf{X}\\
			 \sqrt{\gamma_2}\mathbf{I}
			\end{pmatrix}'
			\begin{pmatrix}
			 \mathbf{Y}\\
			 \mathbf{0}
			\end{pmatrix}
			\right.\\
			& \phantom{{}=1}- \left.
			\left[
			 \frac{1}{\sqrt{1+\gamma_2}}
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}'
			 \frac{1}{\sqrt{1+\gamma_2}}
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}
			\right]^{-1}
			\frac{\gamma_1/ \sqrt{1+\gamma_2}}{2}
			\frac{\sqrt{1+\gamma_2} \boldsymbol{\beta}}{|\sqrt{1+\gamma_2} \boldsymbol{\beta}|}
		\right\rbrace\\
%
&=
	\frac{1}{\sqrt{1+\gamma_2}} 
		\left\lbrace
			\left[
			 \frac{1}{1+\gamma_2}
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}'\begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}
			\right]^{-1}
			\frac{1}{\sqrt{1+\gamma_2}}
			\begin{pmatrix}
			 \mathbf{X}\\
			 \sqrt{\gamma_2}\mathbf{I}
			\end{pmatrix}'
			\begin{pmatrix}
			 \mathbf{Y}\\
			 \mathbf{0}
			\end{pmatrix}
			\right.\\
			& \phantom{{}=1}- \left.
			\left[
			 \frac{1}{1+\gamma_2}
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}'
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}
			\right]^{-1}
			\frac{\gamma_1}{2 \sqrt{1+\gamma_2}}
			\frac{\boldsymbol{\beta}}{| \boldsymbol{\beta}|}
		\right\rbrace\\
%
&=
\left.
			(1+\gamma_2)
			\left[
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}'\begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}
			\right]^{-1}
			\frac{1}{1+\gamma_2}
			\begin{pmatrix}
			 \mathbf{X}\\
			 \sqrt{\gamma_2}\mathbf{I}
			\end{pmatrix}'
			\begin{pmatrix}
			 \mathbf{Y}\\
			 \mathbf{0}
			\end{pmatrix}
			\right.\\
			& \phantom{{}=1}- \left.
			(1+\gamma_2)
			\left[
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}'
			 \begin{pmatrix}
			  \mathbf{X}\\
			  \sqrt{\gamma_2}\mathbf{I}
			 \end{pmatrix}
			\right]^{-1}
			\frac{\gamma_1}{2 (1+\gamma_2)}
			\frac{ \boldsymbol{\beta}}
			{|\boldsymbol{\beta}|}
\right. \\
%
&=(\mathbf{X}'\mathbf{X}+\gamma_2 \mathbf{I})^{-1}\mathbf{X}'\mathbf{Y}-(\mathbf{X}'\mathbf{X}+\gamma_2 \mathbf{I})^{-1} \frac{\gamma_1}{2} \textnormal{sign}(\boldsymbol{\beta}).
\end{align*}
\end{proof}


\section{Beweis der nEnet-Schätzerdarstellung für ein orthogonales Design}\label{App_nEnet_Orthogonal}
\begin{satz}
Bezeichne $\hat{\beta}_j^{OLS}$ den Kleinste-Quadrat-Schätzer für die $j$-te Kovariable. Unter Berücksichtigung der Annahmen für $\mathbf{Y}$ und $\mathbf{X}$ aus Satz \ref{Satz_Lasso_Ortho} gilt für den nEnet-Schätzer
\begin{align*}
\hat{\beta}_j^{nEnet}=\frac{(|\hat{\beta}_j^{OLS}|- \gamma_1 /2)^+}{1+\gamma_2}\textnormal{sign}(\hat{\beta}_j^{OLS}),
\end{align*}
wobei $(\cdot)^+$ angibt, dass nur der positive Teil der Klammer ausgewertet wird.
\end{satz}
\begin{proof}
Die $\textnormal{PRSS}_{nEnet}(\boldsymbol{\beta}, \gamma_1, \gamma_2)$ lässt sich schreiben als
\begin{align*}
\textnormal{PRSS}_{nEnet}(\boldsymbol{\beta}, \gamma_1, \gamma_2)
&=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})+ \gamma_1 \sum_{j=1}^{p} |\beta_j| + \gamma_2 \sum_{j=1}^{p} \beta_j^2 \notag \\
&=\mathbf{Y}'\mathbf{Y}-2\mathbf{Y}'\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta} + \gamma_1 \sum_{j=1}^{p} |\beta_j| + \gamma_2 \sum_{j=1}^{p} \beta_j^2 \notag \\
&\stackrel{\eqref{Eq_Kovarianz_Ortho}}{=}\mathbf{Y}'\mathbf{Y}-2\boldsymbol{\hat{\beta}}^{OLS}{'}\boldsymbol{\beta} + \boldsymbol{\beta}'\boldsymbol{\beta} + \gamma_1 \sum_{j=1}^{p} |\beta_j| + \gamma_2 \sum_{j=1}^{p} \beta_j^2 \notag \\
&=\mathbf{Y}'\mathbf{Y}-2 \sum_{j=1}^{p}\hat{\beta}_j^{OLS} \beta_j + \sum_{j=1}^{p} \beta_j^2 + \gamma_1 \sum_{j=1}^{p} |\beta_j| + \gamma_2 \sum_{j=1}^{p} \beta_j^2 \notag \\
&=\mathbf{Y}'\mathbf{Y}+\sum_{j=1}^{p}\left( -2 \hat{\beta}_j^{OLS}  \beta_j + \beta_j^2 + \gamma_1 |\beta_j| + \gamma_2 \beta_j^2 \right).
\end{align*}
Somit ergeben sich $j=1,\dots,p$ Gleichungssysteme, die nach $\beta_j$ abgeleitet und null gesetzt werden müssen:
\begin{align}\label{Eq_nEnet_Einzelbeta}
\hat{\beta}_j^{nEnet}=\frac{\partial}{\partial \beta_j} \left( \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS} \beta_j + \beta_j^2 + \gamma_1 |\beta_j| +\gamma_2 \beta_j^2 \right) =0 \qquad \text{für} \ j=1,\dots,p .
\end{align}
Anhand der Darstellung in \eqref{Eq_Lasso_Einzelbeta} wird deutlich, dass für $\hat{\beta}_j^{OLS}>0$  auch $\beta_j > 0$ bzw. für $\hat{\beta}_j^{OLS}<0$ auch $\beta_j < 0$ gelten muss. Somit ergibt sich der nEnet-Schätzer für $\hat{\beta}_j^{OLS}>0$ als
\begin{align}\label{Eq_nEnet_Ortho_Beta_greater0}
&& 0&=\frac{\partial}{\partial \beta_j} \left( \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS}  \beta_j + \beta_j^2 + \gamma_1 \beta_j + \gamma_2 \beta_j^2 \right) \notag \\
&\Rightarrow& 0&=-2\hat{\beta}_j^{OLS} + 2\beta_j + \gamma_1 + 2\gamma_2 \beta_j \notag \\
&\Leftrightarrow& \beta_j + \gamma_2 \beta_j &=\hat{\beta}_j^{OLS} - \frac{\gamma_1}{2} \notag \\
&\Leftrightarrow& (1 + \gamma_2) \beta_j &=\hat{\beta}_j^{OLS} - \frac{\gamma_1}{2} \notag \\
&\Rightarrow& \hat{\beta}_j^{nEnet}&=\frac{\hat{\beta}_j^{OLS}-\gamma_1 /2}{1 + \gamma_2} 
\end{align}
und für $\hat{\beta}_j^{OLS}<0$ als
\begin{align}\label{Eq_nEnet_Ortho_Beta_smaller0}
&& 0&=\frac{\partial}{\partial \beta_j} \left( \mathbf{Y}'\mathbf{Y} -2\hat{\beta}_j^{OLS}  \beta_j + \beta_j^2 - \gamma_1 \beta_j + \gamma_2 \beta_j^2 \right) \notag \\
&\Rightarrow& 0&=-2\hat{\beta}_j^{OLS} + 2\beta_j - \gamma_1 + 2\gamma_2 \beta_j \notag \\
&\Leftrightarrow& \beta_j + \gamma_2 \beta_j &=\hat{\beta}_j^{OLS} + \frac{\gamma_1}{2} \notag \\
&\Leftrightarrow& (1 + \gamma_2) \beta_j &=\hat{\beta}_j^{OLS} + \frac{\gamma_1}{2} \notag \\
&\Rightarrow& \hat{\beta}_j^{nEnet}&=\frac{\hat{\beta}_j^{OLS}+\gamma_1 /2}{1 + \gamma_2} .
\end{align}
Da in \eqref{Eq_nEnet_Ortho_Beta_greater0} die rechte Seite für den nEnet-Schätzer größer und in \eqref{Eq_nEnet_Ortho_Beta_smaller0} kleiner null sein muss, kann in beiden Fällen der nEnet-Schätzer dargestellt werden als
\begin{align}\label{Eq_nEnet_Orthogonaldarstellung}
\hat{\beta}_j^{nEnet}=\frac{(|\hat{\beta}_j^{OLS}|- \gamma_1 /2)^+}{1+\gamma_2}\textnormal{sign}(\hat{\beta}_j^{OLS}). 
\end{align}
Für den Fall, dass $\gamma_1 /2 \geq \hat{\beta}_j^{OLS}$ vorliegt, gilt $\hat{\beta}_j^{nEnet}=0$, und für $\gamma_2 = 0$ gilt $\hat{\beta}_j^{nEnet}=\hat{\beta}_j^{Lasso}$ aus dem orthogonalen Design von \ref{App_Lasso_Orthogonal}.
\end{proof}

\section{Beweis für den Gruppierungseffekt bei strikt konvexen Straftermen}\label{App_GroupEffect_Strict_Conv}
\begin{satz}
Es sei $\mathbf{x}_k = \mathbf{x_l}$ für $k,l \in \{1,\dots,p \}$ und für die Schätzung der Regressionskoeffizienten gelte allgemein $\boldsymbol{\hat{\beta}}=\arg \displaystyle\min_{\beta} ||\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}||^2_2 + \gamma P(\boldsymbol{\beta})$. Bezeichnet $||\cdot||^2_2$ die quadrierte $L_2$-Norm und sei $P(\cdot)$ ein Strafterm, der die Summe der \textit{strikt konvexen} Funktionen $J(\beta_j)$ für $j=1,\dots,p$ ist. Dann muss $\hat{\beta}_k = \hat{\beta}_l$ für $\forall \gamma > 0$ gelten.
\end{satz}
Der nachfolgende Beweis ist eine ausführlichere Ausarbeitung des Beweises von \citeA{zou_regularization_2005}.
\begin{proof}
Es sei ein $\gamma > 0$ festgelegt. Für den Fall, dass $\hat{\beta}_k \neq \hat{\beta}_l$ gilt, sei $\boldsymbol{\hat{\beta}}^*$ definiert als
\begin{align*}
\hat{\beta}_j^*=
\begin{cases}
\hat{\beta}_j \qquad &\text{wenn} \ j \neq k \ \text{und} \ j \neq l,\\
\frac{1}{2}\hat{\beta}_k+\frac{1}{2}\hat{\beta}_l \qquad &\text{wenn} \ j = k \ \text{oder} \ j = l,\\
\end{cases}
\end{align*}
d.h. $\hat{\beta}_k^* = \hat{\beta}_l^* \neq \hat{\beta}_k,\hat{\beta}_l$. Wegen $\hat{\beta}_k^* + \hat{\beta}_l^*=(\frac{1}{2}\hat{\beta}_k+\frac{1}{2}\hat{\beta}_l) +  (\frac{1}{2}\hat{\beta}_k+\frac{1}{2}\hat{\beta}_l) = \hat{\beta}_k + \hat{\beta}_l$ und $\mathbf{x}_k = \mathbf{x}_l$ ist $||\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}||^2_2=||\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^*||^2_2$.  
Unter Berücksichtigung der \textit{Jensenschen Ungleichheit} für strikt konvexe Funktionen (siehe Anhang \ref{App_Jensensche_Ungleichung}), 
muss wegen $\hat{\beta}_k \neq \hat{\beta}_l$ und $\frac{1}{2} J(\beta_k)+\frac{1}{2} J(\beta_k)= J(\beta_k)$ bzw. $\frac{1}{2} J(\beta_l)+\frac{1}{2} J(\beta_l)= J(\beta_l)$ gelten
\begin{align*}
&& J \left( \frac{1}{2}\hat{\beta}_k+\frac{1}{2}\hat{\beta}_l \right) + J \left( \frac{1}{2}\hat{\beta}_k+\frac{1}{2}\hat{\beta}_l \right) &< \frac{1}{2}J(\hat{\beta}_k) + \frac{1}{2}J(\hat{\beta}_l) +\frac{1}{2}J(\hat{\beta}_k)+\frac{1}{2}J(\hat{\beta}_l) \\
&\Leftrightarrow& J (\hat{\beta}_k^*) + J (\hat{\beta}_l^*) &< J(\hat{\beta}_k) + J(\hat{\beta}_l).
\end{align*}
Damit gilt $P(\boldsymbol{\hat{\beta}^*}) <  P(\boldsymbol{\hat{\beta}})$, was ein Widerspruch zur Annahme ist, dass $\boldsymbol{\hat{\beta}}$ ein Minimum ist. Folglich muss $\hat{\beta}_k =\hat{\beta}_l$ sein.
\end{proof}



\section{Beweis der oberen Grenze der Differenz zwischen zwei nGrace-Schätzern}\label{App_obere_Grenze_Diff_nGrace}
\begin{satz}
Es seien die Kovariablen in $\mathbf{X}$ standardisiert, $\mathbf{Y}$ zentriert und $\gamma_1$ bzw. $\gamma_2$ fest. Darüber hinaus gelte $\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) \cdot \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2) >0$ für die nGrace-Schätzer der Kovariablen $k$ und $l$ mit $k,l \in \{1,\dots,p\}$. Zusätzlich gelte für die Knotengrade $d(k)=d(l)=w(\{k,l\})$ und die Knoten seien nur miteinander im Graphen verbunden. Ist die einheitenlose Differenz zwischen $\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2)$ und $\hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2)$ definiert als
\begin{align*}
D_{\gamma_1,\gamma_2}(k,l)=\frac{1}{||\mathbf{Y}||_1}||\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2)-\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)||_2,
\end{align*}
so gilt
\begin{align*}
D_{\gamma_1,\gamma_2}(k,l) \leq \frac{1}{2\gamma_2}\sqrt{2(1-\rho_{k,l})}.
\end{align*}
wobei $\rho_{k,l}$ der Stichprobenkorrelation $\rho_{k,l}=\mathbf{x}_k{'}\mathbf{x}_l$ entspricht.
\end{satz}
Der nachfolgende Beweis ist eine ausführliche Ausarbeitung des Beweises von \citeA{Li2007Net1_Workingpaper}.
\begin{proof}
Da $\boldsymbol{\hat{\beta}}^{nEnet}$ die Gleichung
\begin{align}\label{Eq_nGrace_Ableitung_beta_j}
\frac{\partial\textnormal{PRSS}_{nGrace}(\boldsymbol{\beta},\gamma_1,\gamma_2)}{\partial \beta_j}=0 \qquad \textnormal{für} \ \hat{\beta}^{nGrace}_j \neq 0 \qquad \textnormal{mit} \ j=1,\dots,p
\end{align}
erfüllt, ergibt sich wegen der Voraussetzung $d(k)=d(l)=w(\{k,l\})$ für die Kovariable $k$
\begin{align}\label{Eq_Diff_nGrace_Kovariable_K}
0&=-2\mathbf{x}_k{'}\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) + \gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&\phantom{{}=1} + 2\gamma_2\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2) - 2\gamma_2\sum_{\{u,k\}}w(\{u,k\})\frac{\beta_u^{nGrace}}{\sqrt{d(u)d(k)}} \notag \\
&=-2\mathbf{x}_k{'}\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) + \gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&\phantom{{}=1} + 2\gamma_2\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2) - 2\gamma_2 w(\{u,k\})\frac{\beta_u^{nGrace}}{\sqrt{d(u)d(k)}} \notag \\
&=-\mathbf{x}_k{'}\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) + \frac{\gamma_1}{2} \textnormal{sign}\left\lbrace\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_2\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2) - \gamma_2 \hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)
\end{align}
und für Kovariable $l$
\begin{align}\label{Eq_Diff_nGrace_Kovariable_L}
0&=-2\mathbf{x}_l{'}\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) + \gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&\phantom{{}=1} + 2\gamma_2\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2) - 2\gamma_2\sum_{\{v,l\}}w(\{v,l\})\frac{\beta_v^{nGrace}}{\sqrt{d(v)d(l)}} \notag \\
&=-2\mathbf{x}_l{'}\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) + \gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&\phantom{{}=1} + 2\gamma_2\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2) - 2\gamma_2 w(\{v,l\})\frac{\beta_v^{nGrace}}{\sqrt{d(v)d(l)}} \notag \\
&=-\mathbf{x}_l{'}\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) + \frac{\gamma_1}{2} \textnormal{sign}\left\lbrace\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_2\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2) - \gamma_2 \hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2).
\end{align}
Durch die Voraussetzung, dass die einzige Kante der Knoten $k$ und $l$ diejenige ist, die sie miteinander verbindet, fällt in \eqref{Eq_Diff_nGrace_Kovariable_K} und \eqref{Eq_Diff_nGrace_Kovariable_L} das jeweilige Summenzeichen weg und es muss $u=l$ respektive $v=k$ gelten.\\
Wegen der Annahme $\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) \cdot \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2) >0$ gilt auch $\text{sign}\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2)=$\\ $\text{sign}\hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2)$ womit sich als Differenz aus \eqref{Eq_Diff_nGrace_Kovariable_K} und \eqref{Eq_Diff_nGrace_Kovariable_L} ergibt:
\begin{align}
&& 0&=-(\mathbf{x}_k{'}- \mathbf{x}_l{'})\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) \notag \\ 
&& &\phantom{{}=1} + \frac{1}{2}\gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_k^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&& &\phantom{{}=1} - \frac{1}{2}\gamma_1 \textnormal{sign}\left\lbrace\hat{\beta}_l^{nGrace}(\gamma_1,\gamma_2)\right\rbrace \notag \\
&& &\phantom{{}=1} + \gamma_2\left( \hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2) \right) \notag \\ 
&& &\phantom{{}=1} - \gamma_2 \left( \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) \right) \notag \\
&\Leftrightarrow& 0&=-(\mathbf{x}_k{'}- \mathbf{x}_l{'})\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right) \notag \\ 
&& &\phantom{{}=1} + 2 \gamma_2 \left( \hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2) \right) \notag \\
&\Leftrightarrow& \hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2)&=\frac{(\mathbf{x}_k{'}- \mathbf{x}_l{'})\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right)}{2\gamma_2} \notag \\
&\Rightarrow& \frac{\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2)}{||\mathbf{Y}||_1} &= \frac{(\mathbf{x}_k{'}- \mathbf{x}_l{'})\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right)}{||\mathbf{Y}||_1 \cdot 2\gamma_2} \notag \\
&\Rightarrow& \frac{||\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2)||_2}{||\mathbf{Y}||_1} &= \frac{||(\mathbf{x}_k{'}- \mathbf{x}_l{'})\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right)||_2}{||\mathbf{Y}||_1 \cdot 2\gamma_2} \notag \\
&\Rightarrow& \frac{||\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2)||_2}{||\mathbf{Y}||_1} &\stackrel{\eqref{Eq_Cauchy-Schwarz-Ungleichung}}{\leq} \frac{||(\mathbf{x}_k{'}- \mathbf{x}_l{'})||_2 \cdot ||\left(\mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}^{nGrace}(\gamma_1,\gamma_2)\right)||_2}{||\mathbf{Y}||_1 \cdot 2\gamma_2}. \notag \\
\end{align}
Unter Berücksichtigung der Gleichungen \eqref{Eq_Verhaeltnis_Enet_Resid_vs_Y2}, \eqref{Eq_Betrag_Diff_xl_xk} und \eqref{Eq_L1_Norm_greater_L2_Norm} aus dem Beweis für Satz \ref{Satz_obere_Grenze_Diff_nEnet} gilt für die obere Grenze für zwei nur miteinander verbundene nGrace-Schätzer:
\begin{align*}
&& \frac{||\hat{\beta}^{nGrace}_k(\gamma_1,\gamma_2) - \hat{\beta}^{nGrace}_l(\gamma_1,\gamma_2)||_2}{||\mathbf{Y}||_1} &\leq \frac{1}{2\gamma_2} \frac{||\boldsymbol{\hat{\varepsilon}}^{nGrace}||_2}{||\mathbf{Y}||_1} \cdot ||(\mathbf{x}_k{'}- \mathbf{x}_l{'})||_2 \notag \\
&\Rightarrow& &\leq \frac{1}{2\gamma_2} \frac{||\boldsymbol{\hat{\varepsilon}}^{nGrace}||_2}{||\mathbf{Y}||_2} \cdot ||(\mathbf{x}_k{'}- \mathbf{x}_l{'})||_2 \notag \\
&\Rightarrow& &\leq \frac{1}{2\gamma_2} ||(\mathbf{x}_k{'}- \mathbf{x}_l{'})||_2 \notag \\
&\Rightarrow& &\leq \frac{1}{2\gamma_2}\sqrt{2(1-\rho_{k,l})}.
\end{align*}
\end{proof}


\section{Beweis der asymptotischen Eigenschaft des Grace-Schätzers}\label{App_Asymp_Grace_Proof}
\begin{satz}
Gelte $\frac{\gamma_{1,n}}{\sqrt{n}} \rightarrow \gamma_{1,0} \geq 0$ sowie $\frac{\gamma_{2,n}}{\sqrt{n}} \rightarrow \gamma_{2,0} \geq 0$ für $n \rightarrow \infty$. Sei darüber hinaus
\begin{align}\label{Eq_Asymptotic_C-matrix}
\mathbf{C}=\lim\limits_{n \rightarrow \infty} \left( \frac{1}{n} \sum_{i=1}^{n} \mathbf{x}_i \mathbf{x}_i^{'} \right) \qquad \text{mit} \ \mathbf{x}_i=(x_{i,1}, \dots , x_{i,p})'
\end{align}
nicht singulär, dann gilt
\begin{align*}
\sqrt{n}(\boldsymbol{\hat{\beta}}_{(n)}^{Grace} - \boldsymbol{\beta}) \xrightarrow{d} \arg \min (L)
\end{align*}
mit
\begin{align*}
L(\mathbf{u})&=-2\mathbf{u}'\mathbf{W} + \mathbf{u}'\mathbf{C}\mathbf{u} \notag \\
&\phantom{{}=1} + \gamma_{1,0}\sum_{j=1}^{p}\left\lbrace u_j \textnormal{sign}(\beta_j)I(\beta_j \neq 0) + |u_j|I(\beta_j=0) \right\rbrace \notag \\
&\phantom{{}=1} + 2\gamma_{2,0} \sum_{\{k,l\}} \left\lbrace \left( \frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}\right)
\left( \frac{u_k}{\sqrt{d(k)}} - \frac{u_l}{\sqrt{d(l)}}\right)w(\{k,l\})
\right\rbrace
\end{align*}
und
\begin{align*}
\mathbf{W}\sim N(0, \sigma^2 \mathbf{C})
\end{align*}
sowie 
\begin{align*}
\mathbf{u}=(u_1, \dots, u_p)'.
\end{align*}
\end{satz}
Der nachfolgende Beweis ist eine ausführliche und korrigierte Ausarbeitung des Beweises für das asymptotische Verhalten der von \citeA{Li2007Net1_Workingpaper} definierten Funktion $L_n(\mathbf{u})$.
\begin{proof}
Es sei $L_n(\mathbf{u})$ definiert als
\begin{align}\label{Eq_Vn_Grace}
L_n(\mathbf{u})&=\sum_{i=1}^{n} 
\left\lbrace 
\left( 
\varepsilon_i - \frac{\mathbf{u}'\mathbf{x}_i}{\sqrt{n}}
\right)^2 - \varepsilon_i^2
\right\rbrace
+ \gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
|\beta_j + \frac{u_j}{\sqrt{n}}|-|\beta_j|
\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_{2,n}\sum_{\{k,l\}} \left\lbrace
\left[
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)
+
\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)}{\sqrt{n}}
\right]^2 w(\{k,l\}) \right. \notag \\
&\phantom{{}=2} \left.
- \left(
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)^2 w(\{k,l\})
\right\rbrace
\end{align}
mit einem Minimum für $\sqrt{n}(\boldsymbol{\hat{\beta}}_{(n)}^{Grace}-\boldsymbol{\beta})$. Um das zu zeigen, kann \eqref{Eq_Vn_Grace} wegen $\varepsilon_i=Y_i - x_i{'}\boldsymbol{\beta}$ zunächst geschrieben werden als 
\begin{align}\label{Eq_Vn_Grace_Alternative_Dars}
L_n(\mathbf{u})&=\sum_{i=1}^{n} 
\left\lbrace 
\left( 
Y_i - \mathbf{x_i}'\boldsymbol{\beta} - \frac{\mathbf{u}'\mathbf{x}_i}{\sqrt{n}}
\right)^2 - (Y_i - \mathbf{x_i}'\boldsymbol{\beta})^2
\right\rbrace
+ \gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
|\beta_j + \frac{u_j}{\sqrt{n}}|-|\beta_j|
\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_{2,n}\sum_{\{k,l\}} \left\lbrace
\left[
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)
+
\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)}{\sqrt{n}}
\right]^2 w(\{k,l\}) \right. \notag \\
&\phantom{{}=1} \left.
- \left(
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)^2 w(\{k,l\})
\right\rbrace \notag \\
%%%%%%%%%%%%
&=\sum_{i=1}^{n}  
\left( 
Y_i - \mathbf{x_i}'\boldsymbol{\beta} - \frac{\mathbf{u}'\mathbf{x}_i}{\sqrt{n}}
\right)^2
+\gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
|\beta_j + \frac{u_j}{\sqrt{n}}|\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_{2,n}\sum_{\{k,l\}}
\left[
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)
+
\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)}{\sqrt{n}}
\right]^2 w(\{k,l\}) \notag \\
&\phantom{{}=1} - \left\lbrace \sum_{i=1}^{n}(Y_i - \mathbf{x_i}'\boldsymbol{\beta})^2 + \gamma_{1,n}\sum_{j=1}^p|\beta_j|
+ \gamma_{2,n}\sum_{\{k,l\}}\left(
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)^2 w(\{k,l\})
\right\rbrace \notag \\
%%%%%%%%%%%%%%%%%%%%%%%
&=\sum_{i=1}^{n}  
\left( 
Y_i - \mathbf{x_i}'\boldsymbol{\beta} - \frac{\mathbf{u}'\mathbf{x}_i}{\sqrt{n}}
\right)^2
+\gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
|\beta_j + \frac{u_j}{\sqrt{n}}|\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_{2,n}\sum_{\{k,l\}}
\left\lbrace
2
\frac{
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)
\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)
}
{\sqrt{n}} 
+ 
\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)^2}{n}
\right\rbrace w(\{k,l\})
\notag \\
&\phantom{{}=1} - \left\lbrace \sum_{i=1}^{n}(Y_i - \mathbf{x_i}'\boldsymbol{\beta})^2 + \sum_{j=1}^p \gamma_{1,n}|\beta_j|
\right\rbrace.
\end{align}
Zur Vereinfachung der Notation sei im folgenden Schritt $\boldsymbol{\hat{\beta}}=\boldsymbol{\hat{\beta}}_{(n)}^{Grace}$ bzw. $\hat{\beta}_j=\hat{\beta}_{(n)j}^{Grace}$. Da der letzte Term in \eqref{Eq_Vn_Grace_Alternative_Dars} unabhängig von $u$ ist und bei der Minimierung wegfällt, muss nur für den restlichen Teil gezeigt werden, dass $L_n(u)$ durch $\sqrt{n}(\boldsymbol{\hat{\beta}}-\boldsymbol{\beta})$ minimiert wird:
\begin{align}\label{Eq_Ln_Grace_Minimi}
L_n \left( \sqrt{n}(\boldsymbol{\hat{\beta}}-\boldsymbol{\beta}) \right) &=
\sum_{i=1}^{n}  
\left( 
Y_i - (\boldsymbol{\beta}+\boldsymbol{\hat{\beta}}-\boldsymbol{\beta})'\mathbf{x_i}
\right)^2
+\gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
|\beta_j + \hat{\beta}_j -\beta_j|\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_{2,n}\sum_{\{k,l\}}
\left\lbrace
2
\frac{
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right) \sqrt{n}
\left( 
\frac{\hat{\beta}_k}{\sqrt{d(k)}} - \frac{\beta_k}{\sqrt{d(k)}} -
\frac{\hat{\beta}_l}{\sqrt{d(l)}} +
\frac{\beta_l}{\sqrt{d(l)}}
\right)
}
{\sqrt{n}} \right. \notag \\
&\phantom{{}=1} \left. +
\frac{n
\left( 
\frac{\hat{\beta}_k}{\sqrt{d(k)}} - \frac{\beta_k}{\sqrt{d(k)}} -
\frac{\hat{\beta}_l}{\sqrt{d(l)}} +
\frac{\beta_l}{\sqrt{d(l)}}
\right)^2}{n}
\right\rbrace w(\{k,l\}) \notag \\
&=
\sum_{i=1}^{n}  
\left( 
Y_i - \boldsymbol{\hat{\beta}}'\mathbf{x_i}
\right)^2
+\gamma_{1,n}\sum_{j=1}^{p}
|\hat{\beta}j| \notag \\
&\phantom{{}=1} + \gamma_{2,n}\sum_{\{k,l\}}
\left\lbrace
-\frac{\beta_k^2}{d(k)} -\frac{\beta_l^2}{d(l)} + \frac{\hat{\beta}_k^2}{d(k)} +\frac{\hat{\beta}_l^2}{d(l)}-2\frac{\hat{\beta}_k\hat{\beta}_l}{\sqrt{d(k)d(l)}}
\right\rbrace w(\{k,l\}) \notag \\
&\leq \sum_{i=1}^{n}  
\left( 
Y_i - (\boldsymbol{\beta} + \frac{1}{\sqrt{n}} \mathbf{u})'\mathbf{x}_i
\right)^2
+\gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
|\beta_j + \frac{u_j}{\sqrt{n}}|\right\rbrace \notag \\
&\phantom{{}=1} + \gamma_{2,n}\sum_{\{k,l\}}
\left\lbrace
2
\frac{
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)
\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)
}
{\sqrt{n}} \right. \notag \\
&\phantom{{}=2} \left. 
+ 
\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)^2}{n}
\right\rbrace w(\{k,l\}), \qquad \text{für alle} \ u.
\end{align}
Die Ungleichung in \eqref{Eq_Ln_Grace_Minimi} gilt, weil $\boldsymbol{\hat{\beta}}$ als derjenige Schätzer definiert ist, der die PRSS der Grace-Regression minimiert.\\
Da $\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(x_i\varepsilon_i) \xrightarrow{d} N(0, \sigma^2 \mathbf{C})$ \cite{Anderson1992AsymptoticDistriRegression} und \eqref{Eq_Asymptotic_C-matrix} für $n \rightarrow \infty$ gilt, folgt für den ersten Term aus \eqref{Eq_Vn_Grace}
\begin{align*}
\sum_{i=1}^{n} 
\left\lbrace 
\left( 
\varepsilon_i - \frac{\mathbf{u}'\mathbf{x}_i}{\sqrt{n}}
\right)^2 - \varepsilon_i^2
\right\rbrace = \sum_{i=1}^{n} 
\left\lbrace  
\varepsilon_i^2 - 2\frac{\mathbf{u}'\mathbf{x}_i \varepsilon_i}{\sqrt{n}} + \frac{\mathbf{u}'\mathbf{x}_i \mathbf{x}_i'\mathbf{u}}{n} - \varepsilon_i^2
\right\rbrace \rightarrow -2 \mathbf{u}'\mathbf{W} + \mathbf{u}'\mathbf{C}\mathbf{u}
\end{align*}
und für den zweiten Term kann für $j$ unterschieden werden
\begin{align*}
\gamma_{1,n}\left(
|\beta_j + \frac{u_j}{\sqrt{n}}|-|\beta_j|
\right)=\begin{cases}
\gamma_{1,n}\left(\frac{u_j}{\sqrt{n}}\right) \qquad &\text{für} \ \beta_j,u_j > 0 \\
\gamma_{1,n}\left(\frac{u_j}{\sqrt{n}}\right) \qquad &\text{für} \ \beta_j,u_j < 0 \\
\gamma_{1,n}\left(-\frac{u_j}{\sqrt{n}}\right) \qquad &\text{für} \ \beta_j >0 \ \text{und} \ u_j < 0 \\
\gamma_{1,n}\left(-\frac{u_j}{\sqrt{n}}\right) \qquad &\text{für} \ \beta_j <0 \ \text{und} \ u_j > 0 \\
\gamma_{1,n}\left(\frac{|u_j|}{\sqrt{n}}\right) \qquad &\text{für} \ \beta_j =0 \ \text{und} \ u_j \neq 0.
\end{cases}
\end{align*}
Wegen $\left(\frac{\gamma_{1,n}}{\sqrt{n}}, \frac{\gamma_{2,n}}{\sqrt{n}} \right) \rightarrow (\gamma_{1,0}, \gamma_{2,0}) \geq (0,0)$ ergibt sich damit für den zweiten Term
\begin{align*}
&& \gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
|\beta_j + \frac{u_j}{\sqrt{n}}|-|\beta_j|
\right\rbrace &= \gamma_{1,n}\sum_{j=1}^{p}\left\lbrace
\frac{u_j}{\sqrt{n}} \cdot \textnormal{sign}(\beta_j) \cdot I(\beta_j \neq 0) + \frac{|u_j|}{\sqrt{n}} \cdot I(\beta_j=0)
\right\rbrace \\
&\Leftrightarrow& &= \frac{\gamma_{1,n}}{\sqrt{n}}\sum_{j=1}^{p}\left\lbrace
u_j \cdot \textnormal{sign}(\beta_j) \cdot I(\beta_j \neq 0) + |u_j| \cdot I(\beta_j=0)
\right\rbrace \\
&\Rightarrow& &\rightarrow \gamma_{1,0}\sum_{j=1}^{p}\left\lbrace
u_j \cdot \textnormal{sign}(\beta_j) \cdot I(\beta_j \neq 0) + |u_j| \cdot I(\beta_j=0)
\right\rbrace.
\end{align*}
Der dritte Term aus \eqref{Eq_Vn_Grace} lässt sich darstellen als
\begin{align*}
&& &\gamma_{2,n}\sum_{\{k,l\}} \left\lbrace
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)^2 w(\{k,l\})\right. \notag \\
&& &\phantom{{}=2} \left.
+
2\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)}{\sqrt{n}} w(\{k,l\}) \right. \notag \\
&& &\phantom{{}=2} \left.
+
\left[
\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)}{\sqrt{n}} \right]^2 w(\{k,l\}) 
- \left(
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)^2 w(\{k,l\})
\right\rbrace \notag \\
&&= &\gamma_{2,n}\sum_{\{k,l\}} \left\lbrace
2\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)}{\sqrt{n}} w(\{k,l\}) \right. \notag \\
&& &\phantom{{}=2} \left.
+
\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)^2}{n} w(\{k,l\})
\right\rbrace
\end{align*}
und da für $n \rightarrow \infty$ für den Term $\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)^2}{n} w(\{k,l\}) \rightarrow 0$ gilt, folgt
\begin{align*}
&\gamma_{2,n}\sum_{\{k,l\}} \left\lbrace
2\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)\frac{\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right)}{\sqrt{n}} w(\{k,l\}) \right\rbrace \notag \\
\rightarrow& 2\gamma_{2,0}\sum_{\{k,l\}} \left\lbrace
\left( 
\frac{\beta_k}{\sqrt{d(k)}} - \frac{\beta_l}{\sqrt{d(l)}}
\right)\left( 
\frac{u_k}{\sqrt{d(k)}} - 
\frac{u_l}{\sqrt{d(l)}}
\right) w(\{k,l\}) \right\rbrace. \notag \\
\end{align*}
Da $L$ eine strikt konvexe Funktion mit einem globalen Minimum ist, gilt
\begin{align*}
\arg \min (L_n)=\sqrt{n}(\boldsymbol{\hat{\beta}}_{(n)}^{Grace} - \boldsymbol{\beta}) \xrightarrow{d} \arg \min (L).
\end{align*}
\end{proof}

\chapter{Erläuterungen}
\section{Verlust- und Risikofunktionen}\label{App_Risikofunktion}
Wird die Schätzung $\boldsymbol{\hat{\theta}}$ für den wahren Parametervektor $\boldsymbol{\theta}$ als eine \textit{Entscheidung} $d$ aufgefasst, stellt sich die Frage, wie gut diese Entscheidung war. Die Grundlage der Beantwortung ist der \textit{Parameterraum} $\Theta$ aller möglichen $\boldsymbol{\theta}$, der \textit{Entscheidungsraum} $\text{D}$ aller möglichen $d$ sowie eine \textit{Verlustfunktion} $\mathscr{L}(d,\boldsymbol{\theta})$ \cite{judge1985theory}. Formal ist die Verlustfunktion eine Abbildung
\begin{align*}
\mathscr{L}:\Theta \times \text{D} \rightarrow \mathbb{R},
\end{align*}
die den Verlust als einen Wert aus $\mathbb{R}$ angibt, wenn sich beim wahren Parametervektor $\boldsymbol{\theta}$ für $d$ entschieden wird \cite{gross2003linear}. Dabei wird die Entscheidung $d$ über eine Entscheidungsfunktion $\nu$ für die Beobachtungen $\mathbf{y}$ getroffen. In der Regel wird $\mathbf{y}$ als Realisierung des Zufallsvektors $\mathbf{Y}$ angesehen, sodass $\nu(\mathbf{Y})=d$ auch als Zufallsvektor aufgefasst werden kann. Im Hinblick auf die Eingangsfrage, wie gut eine Entscheidung $d$ ist, ist daher nicht der Verlust durch die Funktion $\nu(\mathbf{y})$ für die konkrete Beobachtung, sondern für die Zufallsvariable $\mathbf{Y}$ von Interesse. Dafür bietet sich der \textit{erwartete Verlust} 
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
an, welcher der Erwartungswert der Verlustfunktion ist und als \textit{Risko} oder \textit{Riskofunktion} bezeichnet wird \cite{gross2003linear,judge1985theory}.
Beispielsweise kann bei der klassischen linearen Regression $\boldsymbol{\hat{\beta}}^{OLS}$ als Entscheidung $d$ durch die Methode der kleinsten Quadrate aufgefasst und als Verlustfunktion $\mathscr{L}(\boldsymbol{\hat{\beta}}^{OLS})$ der quadrierte Abstand zu $\boldsymbol{\beta}$ gewählt werden. Die zugehörige Risikofunktion ist der erwartete Quadratwert von $\boldsymbol{\hat{\beta}}^{OLS}$.\\
Für den Vergleich von zwei verschiedenen Entscheidungsfunktionen $\nu_1(\mathbf{Y})$ und $\nu_2(\mathbf{Y})$, im Fall dieser Arbeit von Schätzmethoden für $\boldsymbol{\beta}$, können nach \citeA{gross2003linear} drei Fälle unterschieden werden: 
\begin{enumerate}
\item Die Funktion $\nu_1(\mathbf{Y})$ ist \textit{gleichmäßig nicht schlechter} als eine Funktion $\nu_1(\mathbf{Y})$, wenn
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu_1(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace \leq \mathbb{E} \left\lbrace \mathscr{L}(\nu_2(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
für alle $\boldsymbol{\theta} \in \Theta$ gilt.

\item Die Funktion $\nu_1(\mathbf{Y})$ ist \textit{gleichmäßig besser} als eine Funktion $\nu_1(\mathbf{Y})$, wenn
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu_1(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace < \mathbb{E} \left\lbrace \mathscr{L}(\nu_2(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
für mindestens ein $\boldsymbol{\theta} \in \Theta$ und ansonsten Fall $1$ gilt.

\item Die Funktion $\nu_1(\mathbf{Y})$ ist \textit{strikt besser} als eine Funktion $\nu_1(\mathbf{Y})$, wenn
\begin{align*}
\mathbb{E} \left\lbrace \mathscr{L}(\nu_1(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace < \mathbb{E} \left\lbrace \mathscr{L}(\nu_2(\mathbf{Y}),\boldsymbol{\theta}) \right\rbrace
\end{align*}
für alle $\boldsymbol{\theta} \in \Theta$ gilt.
\end{enumerate}

\section{Lagrange-Form und -Multiplikatoren}\label{App_Lagrange-Form}
Optimierungsprobleme können um Nebenbedingungen für die Variablen, die variiert werden sollen, ergänzt werden. Sei $\mathbf{x} \in \mathbb{R}^p$, so kann beispielsweise
\begin{align}\label{Eq_Optimierung_Equal}
&&\arg \displaystyle\min_{\mathbf{x}} \ & f(\mathbf{x}) \notag \\ 
&&\text{s.d.} \ & g_i(\mathbf{x})=t_i \qquad \text{für} \  i=1,\dots,p^* \qquad \text{mit} \ p^* \le p
\end{align}
ein Minimierungsproblem sein, wobei $f(\mathbf{x})$ die Zielfunktion und $g_j(\mathbf{x})=t_i$ für $i=1,\dots,p^*$ die zu erfüllenden Nebenbedingungen sind. 
\begin{comment}Es muss stets $p^* < p$ gelten, da ansonsten die Lösung für das Optimierungsproblem durch die Bedingungen nicht eingeschränkt, sondern bestimmt wäre.
\end{comment}
Werden die Nebenbedingungen als $g_i(\mathbf{x})-t_i=0$ formuliert, kann \eqref{Eq_Optimierung_Equal} alternativ in der \textit{Lagrange-Form} dargestellt werden \cite{haftka1992elements}, die definiert ist als
\begin{align}\label{Eq_lagrange_Allgemein}
\mathcal{L}(\mathbf{x},\boldsymbol{\gamma})=f(\mathbf{x})+ \sum_{i=1}^{p^*}\gamma_i (g_i(\mathbf{x})-t_i).
\end{align}
Die Lagrange-Form hat keine zusätzlichen Nebenbedingungen, ist dafür allerdings um $p^*$ Argumente, welche die Nebenbedingung aus \eqref{Eq_Optimierung_Equal} widerspiegeln, und den zugehörigen \textit{Lagrange-Multiplikatoren} $\gamma_i$ erweitert. Um eine Extremstelle zu finden, die die Nebenbedingungen erfüllt, muss $\mathcal{L}(\mathbf{x},\boldsymbol{\gamma})$ sowohl nach $\mathbf{x}$ als auch nach $\boldsymbol{\gamma}$ abgeleitet und Null gesetzt werden, d.h.
\begin{align*}
\frac{\partial \mathcal{L}}{\partial x_j}=\frac{\partial f}{\partial x_j} + \sum_{i=1}^{p^*}\gamma_i \frac{\partial g_i}{\partial x_j}&=0 \qquad \text{für} \ j=1,\dots,p , \\ 
\frac{\partial \mathcal{L}}{\partial \gamma_i}=g_i(\mathbf{x})-t_i&=0 \qquad \text{für} \ i=1,\dots,p^* ,
\end{align*}
womit sich $p+p^*$ zu lösende Gleichungen ergeben \cite{strang1991calculus}.\\
Geometrisch lassen sich die Nebenbedingungen als spezielle \textit{Konturlinien} der Nebenbedingungsfunktionen $g_i(\mathbf{x})$ betrachten, die sich mit den Konturlinien der Zielfunktion $f(\mathbf{x})$ überschneiden und an bestimmten Punkten diese \textit{tangieren}. Gesucht werden diese Tangentenpunkte, da nur dort ein stationärer Punkt für $f(\mathbf{x})$ und damit eine Extremstelle unter Einhaltung der Nebenbedingungen erreicht werden kann. Wird beispielsweise die Funktion $f(x,y)=(x-1)^2 + (y-1)^2$ mit der Nebenbedingung $g(x,y)=(x-1)^2+(y-1)^2=0.5$ betrachtet, überschneiden sich die Konturlinien der Zielfunktion mit der Konturlinie der Nebenbedingungen an allen Punkten bis auf an zwei Tangentenpunkten (siehe Abbildung \ref{Fig:Lagrange_Geo_Interpret}). Diese beiden Punkte sind das Minimum $(x=0.5,y=0.5)$ und Maximum $(x=-0.5,y=-0.5)$, da es unter Berücksichtigung der Nebenbedingung keine extremeren Punkte für $f(x,y)$ gibt.\\

\begin{figure}[ht]
 \begin{minipage}{0.5\linewidth}
  \centering
  \setkeys{Gin}{width=0.99\textwidth}
  \includegraphics{Konturplot.png}
 \end{minipage}%
 \begin{minipage}{0.5\linewidth}
  \centering
  \setkeys{Gin}{width=0.99\textwidth}
  \includegraphics{Funktionsplot.png}
 \end{minipage}
 \caption[Graphische Veranschaulichung einer Nebenbedingung]{Das linke Bild zeigt die Konturlinien (rote Kreise) der Zielfunktion $f(x,y)=(x-1)^2+(y-1)^2$ und als blauen Kreis die spezielle Konturlinie $g(x,y)=0.5$ der Nebenbedingung $g(x,y)=x^2+y^2$. Zusätzlich sind die Gradienten am Punkt $(0.5,0.5)$ für beide Funktionen eingezeichnet, die dort parallel zueinander sind. Im rechten Bild ist die Zielfunktion als elliptischer Paraboloid mit verschiedenen Konturlinien in rot eingezeichnet. Die blaue Ellipse kennzeichnet alle Punkte auf dem elliptischen Paraboloid, die die Nebenbedingung erfüllen. Am untersten Punkt dieser Ellipse ist ein Minimum und am obersten Punkt ein Maximum für die Zielfunktion unter Einhaltung der Nebenbedingung erreicht. Aus dem rechten Bild wird auch deutlich, dass Extremstellen unter Einhaltung der Nebenbedingung keine Extremstelle für die Zielfunktion ohne Nebenbedingung sein müssen.}
 \label{Fig:Lagrange_Geo_Interpret}
\end{figure} 
Bezeichnet $\nabla$ den \textit{Gradienten} einer Funktion, so befinden sich die gesuchten Tangentenpunkte an allen Punkten, für die die Gleichung
\begin{align}\label{Eq_Gradientengleichung}
\nabla f(\mathbf{x}) &= \sum_{i=1}^{p^*} \gamma_i \nabla  (g_i(\mathbf{x})-t_i)
\end{align}
erfüllt ist. Mit \eqref{Eq_Gradientengleichung} wird gefordert, dass der Gradient von $f(\mathbf{x})$ parallel zur der Linearkombination der Gradienten $\sum_{i=1}^{p^*} \nabla  (g_i(\mathbf{x})-t_i)$ ist, die jeweils um $\gamma_i$ gestreckt bzw. gestaucht sind. Da Gradienten immer senkrecht zu den Konturlinien stehen \cite{strang1991calculus}, kann die Parallelität aus \eqref{Eq_Gradientengleichung} nur in dem \textit{Tangentenpunkt} der Konturlinien erfüllt sein. Durch Umstellen von \eqref{Eq_Gradientengleichung} ist ersichtlich, dass für die Extremstellen ein Vektor $\mathbf{x}$ gesucht wird, der
\begin{align*}
\nabla \mathcal{L}(\mathbf{x},\boldsymbol{\gamma})&= \nabla f(\mathbf{x}) - \sum_{i=1}^{p^*} \gamma_i \nabla  (g_i(\mathbf{x})-t_i)=\mathbf{0}
\end{align*}
erfüllt.\\

Für den Fall, dass die Bedingungen für die Zielfunktion aus \ref{Eq_Optimierung_Equal} als \textit{Ungleichungen} formuliert sind, d.h.
\begin{align}\label{Eq_Optimierung_Unequal}
&&\arg \displaystyle\min_{\mathbf{x}} \ & f(\mathbf{x}) \notag \\ 
&&\text{s.d.} \ & h_i(\mathbf{x}) \le t_i \qquad \text{für} \  i=1,\dots,p^* \qquad \text{mit} \ p^* \le p ,
\end{align}
müssen sogenannte \textit{Slackvariablen} eingeführt werden, durch die die Ungleichungen zu Gleichungen umgewandelt werden. Dieses Vorgehen beruht auf der Tatsache, dass es für $h_i(\mathbf{x})-t_i \le 0$ eine Variable $s_i^2$ geben muss, durch die $h_i(\mathbf{x})-t_i + s_i^2 = 0$ gilt, was wiederum einer Nebenbedingung als Gleichung entspricht \cite{haftka1992elements}. Dadurch ergibt sich als Lagrange-Form
\begin{align}
\mathcal{L}(\mathbf{x}, \boldsymbol{\gamma}, \mathbf{s})=f(\mathbf{x})+ \sum_{i=1}^{p^*}\gamma_i (h_i(\mathbf{x})-t_i + s_i^2),
\end{align}
die nach $\mathbf{x}$, $\boldsymbol{\gamma}$ und $\mathbf{s}$ abgeleitet und Null gesetzt werden muss:
\begin{align}
\frac{\partial \mathcal{L}}{\partial x_j}=\frac{\partial f}{\partial x_j} + \sum_{i=1}^{p^*} \gamma_i \frac{\partial h_i}{\partial x_j} &= 0 \qquad \text{für} \ j=1,\dots,p  , \label{Eq_Lag_Part_1} \\
\frac{\partial \mathcal{L}}{\partial \gamma_i}=h_i - t_i + s_i^2 &=0 \qquad \text{für} \ i=1,\dots,p^*  , \label{Eq_Lag_Part_2} \\
\frac{\partial \mathcal{L}}{\partial s_i}=2\gamma_i s_i &=0
\qquad \text{für} \ i=1,\dots,p^*  . \label{Eq_Lag_Part_3}
\end{align}
Anhand von \eqref{Eq_Lag_Part_2} und \eqref{Eq_Lag_Part_3} ist erkennbar, für welche Nebenbedingungen später ein Lagrange-Multiplikator existiert, da für den Fall, dass $g(\mathbf{x})_i-t_i < 0$ ist, die Slackvariable $s^2_i > 0$ sein muss und damit wegen \eqref{Eq_Lag_Part_3} für den Lagrange-Multiplikator $\gamma_i=0$ gilt. Die Nebenbedingung ist dann \textit{deaktiviert}, wohingegen für $s_i^2=0$ die Nebenbedingung \textit{aktiviert} ist und es einen Lagrange-Multiplikator $\gamma_i \ge 0$ gibt \cite{arora2011introductionOptimum}. Geometrisch bedeutet eine Deaktivierung, dass sich eine Extremstelle der Zielfunktion innerhalb der Fläche befindet, die durch die Konturlinie einer Nebenbedingung entsteht, und deswegen die Lösung unabhängig von dieser Nebenbedingung ist. Um eindeutige Lagrange-Multiplikatoren $\gamma_i$ zu erhalten, müssen die Gradienten der aktiven Nebenbedingungen linear unabhängig sein.\\
Neben den Ableitungen \eqref{Eq_Lag_Part_1} bis \eqref{Eq_Lag_Part_3} muss für die Nebenbedingungen $h_i(\mathbf{x}) - t_i \le 0$ bei Minimierungsproblemen gefordert werden, dass alle $\gamma_i \ge 0$ sind. Für die Begründung muss zunächst eine Sensitivitätsanalyse für die Zielfunktionen mit Nebenbedingungen eingeführt werden. Sei $\mathbf{x}^*$ ein Punkt, der eine Extremstelle unter Einhaltung der Nebenbedingungen aus \eqref{Eq_Optimierung_Unequal} beschreibt und abhängig von der Wahl von $t_i$ für $i=1,\dots,p^*$ ist, d.h. $\mathbf{x}^*=\mathbf{x}^*|\mathbf{t}$. Somit hängt auch der Zielfunktionswert $f^*=f^*(\mathbf{t})$, der die Nebenbedingung \textit{erfüllt}, von $\mathbf{t}$ ab. Beschreibt $\boldsymbol{\gamma}^* > \mathbf{0}$ die für die Lösung $\mathbf{x}^*$ zugehörigen Lagrange-Multiplikatoren, d.h. alle Nebenbedingungen sind aktiviert, gilt im Fall von $\mathbf{t}=\mathbf{0}$, dass
\begin{align}\label{Eq_Ableitung_t_0}
\frac{\partial f^*}{\partial t_i}=\frac{\partial f^*(\mathbf{x}^*|\mathbf{0})}{\partial t_i}=-\gamma^*_i \qquad \text{für} \ i=1,\dots,p^* .
\end{align}
Mittels der \textit{abgebrochenen Taylorreihe} kann dann für einen beliebigen Vektor $\mathbf{t}$ der Zielfunktionswert $f(\mathbf{t})$ als 
\begin{align}\label{Eq_Taylor_Sensi_Lag-Multipli}
f(\mathbf{t})=f^*(\mathbf{x}^*|\mathbf{0}) + \sum_{i=1}^{p^*} \frac{\partial f^*(\mathbf{x}^*|\mathbf{0})}{\partial t_i} t_i \stackrel{\eqref{Eq_Ableitung_t_0}}{=} f^*(\mathbf{x}^*|\mathbf{0}) -\sum_{i=1}^{p^*} \gamma^*_i \cdot t_i
\end{align}
approximiert werden  \cite{arora2011introductionOptimum}. Durch Umstellen von \eqref{Eq_Taylor_Sensi_Lag-Multipli} gibt
\begin{align}\label{Eq_Differenz_Sensi_t_0}
\delta f^*=f(\mathbf{t})-f^*(\mathbf{x}^*|\mathbf{0})=- \sum_{i=1}^{p^*} \gamma^* \cdot t_i
\end{align}
an, wie sich die optimale Lösung unter Einhaltung der Nebenbedingung verändert, wenn $\mathbf{t}$ variiert wird. Werden beispielsweise einzelne $t_i$ erhöht, kann dies wegen $h_i(\mathbf{x}) \le t_i$ als eine \textit{Abmilderung} der Nebenbedingungen aufgefasst werden, da sich die Menge der möglichen Lösungen für das Optimierungsproblem vergrößert. Umgekehrt hat eine Verkleinerung von einzelnen $t_i$ eine \textit{Verstärkung der Einschränkung} durch die Nebenbedingungen zur Folge, weil die Menge der möglichen Lösungen verkleinert wird.\\
Für das Minimierungsproblem aus \eqref{Eq_Optimierung_Unequal} bedeutet das, dass alle $\gamma_i$ positiv sein müssen, da \eqref{Eq_Differenz_Sensi_t_0} den Anstieg bzw. den Abfall in $f(\mathbf{t})$ beschreibt, wenn einzelne $t_i$ verändert werden. Für ein negatives $\gamma_i$ würde eine Vergrößerung eines $t_i$ einen Anstieg in $f(t_i)$ für das Minimum bedeuten, was ein Widerspruch ist, da durch eine Abmilderung der Nebenbedingung die Menge der möglichen Lösungen vergrößert wird und das neue Minimum kleiner oder gleich dem alten sein muss.

\begin{comment}

%%%
Dazu wird die 
Eine intuitive Erklärung hierfür ist der Umstand, dass für eine Nebenbedingung $h_i(\mathbf{x})-t_i \le 0$ \textit{gelockert} werden kann, indem sie zu $h_i(\mathbf{x})-t_i \le u_i$ mit $u_i > 0$ erweitert wird. Dadurch erweitern sich das Set der Möglichen Punkte für eine Extremstelle

\end{comment}

\section{Konvexe Funktionen}\label{App_Konvexe_Funktionen}
Eine Funktion $f:X \rightarrow \mathbb{R}$ heißt \textit{konvex}, wenn
\begin{align*}\label{Eq_Convex_Funktion}
f(\pi a + (1-\pi)b) \leq \pi f(a) + (1-\pi) f(b) \qquad \text{mit} \quad 0 < \pi < 1
\end{align*}
für alle $a,b \in X$ gilt \cite{matousek2006understanding}. Es handelt sich bei $f$ um eine \textit{streng konvexe} Funktion, wenn für alle $a \neq b$ gilt
\begin{align*}
f(\pi a + (1-\pi)b) < \pi f(a) + (1-\pi) f(b) \qquad \text{mit} \quad 0 < \pi < 1.
\end{align*}
Sind $g$ und $h$ zwei konvexe Funktionen und $p,q \ge 0$, so lässt sich zeigen, dass
\begin{align*}
f=p \cdot g + q \cdot h
\end{align*}
ebenfalls eine konvexe Funktion ist \cite{boyd2004convex}.

\section{Irrepresentable Condition}\label{App_Irrepresentable Condition}
Sei der Regressionskoeffizientenvektor $\boldsymbol{\beta}=(\beta_1, \dots, \beta_k, \beta_{l}, \dots, \beta_p)'$ mit $\beta_j \neq 0$ für $j=1,\dots,k$ und $\beta_j = 0$ für $j=l, \dots, p$. Sei darüber hinaus $\boldsymbol{\beta}_{(1)}=(\beta_1, \dots, \beta_k)'$ bzw. $\boldsymbol{\beta}_{(2)}=(\beta_{l}, \dots, \beta_p)'$ und bezeichne $\mathbf{X}_{(1)}$ die ersten $k$ Spalten und $\mathbf{X}_{(2)}$ die letzten $(p-k)$ Spalten von $\mathbf{X}$. Damit ergeben sich die vier Kovarianzmatrizen $\boldsymbol{\Sigma}_{(11)}=\mathbf{X}_{(1)}'\mathbf{X}_{(1)}$, $\boldsymbol{\Sigma}_{(12)}=\mathbf{X}_{(1)}'\mathbf{X}_{(2)}$, $\boldsymbol{\Sigma}_{(21)}=\mathbf{X}_{(2)}'\mathbf{X}_{(1)}$ und $\boldsymbol{\Sigma}_{(22)}=\mathbf{X}_{(2)}'\mathbf{X}_{(2)}$, mit denen sich die Kovarianzmatrix $\boldsymbol{\Sigma}$ von $\mathbf{X}$ darstellen lässt:
\begin{align*}
\boldsymbol{\Sigma} = 
\begin{pmatrix}
\boldsymbol{\Sigma}_{(11)} & \boldsymbol{\Sigma}_{(12)} \\
\boldsymbol{\Sigma}_{(21)} & \boldsymbol{\Sigma}_{(22)}.
\end{pmatrix}
\end{align*}
Sei $||\mathbf{x}||_{\infty}=\max_j|x_i|$ für einen Vektor $\mathbf{x}$ und $\boldsymbol{\Sigma}_{(11)}$ invertierbar, dann ist die \textit{Irrepresentable Condition} definiert als
\begin{align*}
||\boldsymbol{\Sigma}_{(21)}\boldsymbol{\Sigma}_{(11)}^{-1} \text{sign}(\boldsymbol{\beta}_{(1)})||_{\infty}\leq 1-\eta \qquad \text{mit} \ \eta \in (0,1].
\end{align*}
Somit fordert die Irrepresentable Condition, dass die Kovariablen, die nicht im wahren Modell sind, sich nicht durch die Kovariablen aus dem wahren Modell darstellen lassen \cite{Zhao2006Lasso_Consistency}.

\section{Jensensche Ungleichung}\label{App_Jensensche_Ungleichung}
Für eine \textit{konvexe} Funktion $J$ und $z_1,z_2, \dots,z_p \in (0,1)$ mit $\sum_{j=1}^{p}z_j =1$ gilt 
\begin{align*}
J \left(\sum_{j=1}^{p} z_j x_j \right) \leq \sum_{j=1}^{p} \left( z_j J(x_j) \right).
\end{align*}
Handelt es sich um eine \textit{strikt konvexe} Funktion $J$ und $\exists x_k \colon x_k \neq x_l$ für $x_k,x_l \in \{x_1, \dots, x_p\}$  \cite{Mitroi2010Precision_Jensen}, gilt
\begin{align*}
J \left(\sum_{j=1}^{p} z_j x_j \right) < \sum_{j=1}^{p} \left( z_j J(x_j) \right).
\end{align*}



\chapter{Abbildungen und Tabellen}\label{App_Abbildungen_Tabellen}
\section{KEGG Cancer-Pathway}\label{KEGG-Cancer-Pathway}
\begin{figure}[htbp]
  \centering
  \setkeys{Gin}{width=0.99\textwidth}
  \includegraphics{CancerPathway.png}
  \caption[Übersicht über den KEGG Cancer-Pathway]{Übersicht über den Cancer-Pathway aus der KEGG-Datenbank (\citeNP{Kanehisa2000KEGG}; Stand 03. November, 2014).}
   \label{Fig:KEGG_Cancer_PW}
\end{figure} 

\newpage
\section{KEGG Cancer-Pathway als ungerichteter Graph}

<<echo=FALSE, message=FALSE, Cancer_PW_Graph, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.9\\linewidth', fig.lp="fig:", fig.scap="KEGG Cancer-Pathway als ungerichteter Graph", fig.cap="Darstellung des KEGG Cancer-Pathways aus Abbildung \\ref{Fig:KEGG_Cancer_PW} als ungerichteter Graph.">>=
#setwd("~/Studium/BIOMETRIE/Masterarbeit/R-Code/")
#g <- parseKGML2Graph("./Pathways/hsa05200")
#library(igraph)
#g <- #igraph.to.graphNEL(as.undirected(igraph.from.graphNEL(g)))
#save(g, file="../R-Code/R-Objekte/CancerPathway.RData")

library(KEGGgraph)
load("~/Studium/BIOMETRIE/Masterarbeit/R-Code/R-Objekte/CancerPathway.RData")
plot(g, attrs=list(node=list(label=NULL,fillcolor="lightgreen", fontsize="30", 
                              fixedsize=FALSE, size="60"),
                    graph=list(fontcolor="transparent")))
@

\newpage
\section{Histogramme für die RP- und FP-Raten sowie Modell- und Vorhersagefehler der einzelnen Schätzmethoden}

<<echo=FALSE, message=FALSE, Histo_TP_Simu, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.99\\linewidth', fig.lp="fig:", fig.scap="Histogramme für die RP-Raten", fig.cap="Histogramme für die RP-Raten der einzelnen Schätzmethoden in der Simulationsstudie.">>=
load("/home/momo/Studium/BIOMETRIE/Masterarbeit/Daten_BackUp/Ergebnisse/Auswertung_beta_hats.RData")
par(mfrow=c(1,1))
par(mfrow=c(3,3))
hist(TP.Enet, main="Enet",
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

hist(TP.Betweenness.Rank, main="Betweenness-Zentralität (Ränge)",
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

hist(TP.closeness.Rank, main="Closeness-Zentralität (Ränge)",
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

hist(TP.DegreeRank, main="Knotengrad-Zentralität (Ränge)", 
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

hist(TP.evcent.Rank, main="Eigenvektor-Zentralität (Ränge)", 
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

hist(TP.closenessNORank, main="Closeness-Zentralität", 
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

hist(TP.sqrtdegree.NORank, main=expression(sqrt(d(u))-Zentralität),
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

hist(TP.noweight, main="ohne Gewichtung",
     breaks=seq(0,1,0.05),xlab="RP-Rate",ylab="absolute Häufigkeit", ylim=c(0,20))

@

\newpage


<<echo=FALSE, message=FALSE, Histo_FP_Simu, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.99\\linewidth', fig.lp="fig:", fig.scap="Histogramme für die FP-Raten", fig.cap="Histogramme für die FP-Raten der einzelnen Schätzmethoden in der Simulationsstudie.">>=
load("/home/momo/Studium/BIOMETRIE/Masterarbeit/Daten_BackUp/Ergebnisse/Auswertung_beta_hats.RData")
par(mfrow=c(1,1))
par(mfrow=c(3,3))

hist(FP.Enet, main="Enet", 
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(FP.Betweenness.Rank, main="Betweenness-Zentralität (Ränge)",
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(FP.closeness.Rank, main="Closeness-Zentralität (Ränge)",
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(FP.DegreeRank, main="Knotengrad-Zentralität (Ränge)", 
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(FP.evcent.Rank, main="Eigenvektor-Zentralität (Ränge)", 
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(FP.closenessNORank, main="Closeness-Zentralität", 
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(FP.sqrtdegree.NORank, main=expression(sqrt(d(u))-Zentralität),
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(FP.noweight, main="ohne Gewichtung",
     breaks=seq(0,1,0.05),xlab="FP-Rate",ylab="absolute Häufigkeit", ylim=c(0,70))

@


\newpage

<<echo=FALSE, message=FALSE, Histo_ME_Simu, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.99\\linewidth', fig.lp="fig:", fig.scap="Histogramme für die Modellfehler", fig.cap="Histogramme für die Modellfehler der einzelnen Schätzmethoden in der Simulationsstudie.">>=
load("/home/momo/Studium/BIOMETRIE/Masterarbeit/Daten_BackUp/Ergebnisse/Auswertung_beta_hats.RData")
par(mfrow=c(1,1))
par(mfrow=c(3,3))

hist(ME.Enet, main="Enet", 
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

hist(ME.Betweenness.Rank, main="Betweenness-Zentralität (Ränge)",
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

hist(ME.closeness.Rank, main="Closeness-Zentralität (Ränge)",
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

hist(ME.DegreeRank, main="Knotengrad-Zentralität (Ränge)", 
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

hist(ME.evcent.Rank, main="Eigenvektor-Zentralität (Ränge)", 
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

hist(ME.closenessNORank, main="Closeness-Zentralität", 
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

hist(ME.sqrtdegree.NORank, main=expression(sqrt(d(u))-Zentralität),
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

hist(ME.noweight, main="ohne Gewichtung",
     breaks=seq(0,1400,50),xlab="ME",ylab="absolute Häufigkeit", ylim=c(0,50))

@


\newpage

<<echo=FALSE, message=FALSE, Histo_PE_Simu, fig.pos="H", fig.show='hold', fig.align='center', out.width='0.99\\linewidth', fig.lp="fig:", fig.scap="Histogramme für die Vorhersagefehler", fig.cap="Histogramme für die Vorhersagefehler der einzelnen Schätzmethoden in der Simulationsstudie.">>=
load("/home/momo/Studium/BIOMETRIE/Masterarbeit/Daten_BackUp/Ergebnisse/Auswertung_beta_hats.RData")
par(mfrow=c(1,1))
par(mfrow=c(3,3))

hist(PE.Enet, main="Enet", 
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70) )

hist(PE.Betweenness.Rank, main="Betweenness-Zentralität (Ränge)",
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(PE.closeness.Rank, main="Closeness-Zentralität (Ränge)",
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(PE.DegreeRank, main="Knotengrad-Zentralität (Ränge)", 
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(PE.evcent.Rank, main="Eigenvektor-Zentralität (Ränge)", 
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(PE.closenessNORank, main="Closeness-Zentralität", 
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(PE.sqrtdegree.NORank, main=expression(sqrt(d(u))-Zentralität),
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70))

hist(PE.noweight, main="ohne Gewichtung",
     breaks=seq(0,2000,100),xlab="PE",ylab="absolute Häufigkeit", ylim=c(0,70))
@




% Die eidesstattliche Erklärung auf einer neuen Seite
\newpage
% Keine Nummerierung für diesen Teil
\section*{Eigenständigkeitserklärung}
% Keine Kopf- und Fußzeilen ausgeben
\thispagestyle{empty}
% Aber trotzdem ins Inhaltsverzeichnis aufnehmen
\addcontentsline{toc}{chapter}{Eigenständigkeitserklärung}
% Hier der offizielle Text der eidesstattlichen Erklärung
Hiermit versichere ich, dass ich die vorliegende Arbeit "`XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXX"' selbstständig und ohne Benutzung anderer als der angegebenen 
Hilfsmittel angefertigt habe; die aus fremden Quellen direkt oder indirekt übernommenen Gedanken sind als solche kenntlich gemacht. 
Die Arbeit wurde bisher in gleicher oder ähnlicher Form keiner anderen Prüfungskommission vorgelegt und auch nicht veröffentlicht.
% Etwas Abstand für die Unterschrift
\vspace{3cm}
% Hier kommt die Unterschrift drüber
\begin{tabbing}
\hspace{6cm}  \= \kill
\textit{Bremen, \today} \> \textit{Moritz Hanke}
\end{tabbing}

\end{appendix}

\end{document}


